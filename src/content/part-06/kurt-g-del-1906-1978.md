# Kurt Gödel (1906–1978)

VI.93. André Weil

Further Reading

Aspray, W. 1990.Modern Computing John von Neumann and the Origins of. Cambridge, MA: MIT Press.

Wolfgang Coy

VI.92 Kurt Gödel

b. Brno, Moravia (now Czech Republic), 1906; d. Princeton, New Jersey, 1978 Logic; relativity theory Born in Brno, Moravia, Gödel did his most important work at the University of Vienna. In 1940 he emigrated to the United States, where he accepted an appointment at the Institute for Advanced Study in Princeton. Considered the greatest mathematical logician of the twentieth century, Gödel is renowned for his proofs of three fundamental results: the semanticof first-order logic [IV.23 §2](/part-04/logic-and-model-theory);
the syntactic completeness incompleteness of formal number theory the consistency, relative to the axioms of zermelo–[V.15](/part-05/gdels-theorem); and fraenkel choice [III.1](/part-03/axiom-of-choice) and the generalized[IV.22 §3.1](/part-04/set-theory) set theory, of the continuum hypoth-axiom of esis [IV.22 §5](/part-04/set-theory). with the following kind of question: how do we know that a statement in group theory, for example, that is Gödel’s completeness theorem (1930) is concerned true in every group is actually provable from the axioms of group theory?
Gödel showed that in any first-order theory (one in which quantifiers are allowed over ele-ments but not over subsets), any statement true in all models is indeed provable. In an equivalent form, this completeness theorem states that any set of statements that is consistent (that is, from which no contradiction may be derived) has a model—a structure in which all those statements hold.
waves through logic and the philosophy of mathemat-ics. Gödel’s incompleteness theorem (1931) sent shock hilbert [VI.63](/part-06/david-hilbert-18621943) had set out a program in which all statements (in number theory, for example) should be derivable from a fixed set of axioms. It was generally believed that such a program was in principle possible, until the incompleteness theorem destroyed that hope. effect, asserts “shows that such a statement must be both true and Gödel’s idea was to construct a statement Sis not provable.” A moment’s thought S that, in unprovable.
Gödel’s remarkable achievement was to manage to encode such a statement in the languageof number theory. His proof applies to such axioms as the peano axioms [III.67](/part-03/the-peano-axioms) for number theory, and more

819

generally, to any reasonable extension of them (such asthe Zermelo–Fraenkel axioms for set theory). another blow to the Hilbert program. Suppose thatwe have a set of axioms Gödel’s second incompleteness theorem represented T (for example, the Peano axioms) that is consistent. Can we prove that it is consistent? Gödel showed that, ifthe statement “$T$ is consistent” (when encoded as a T is consistent, then statement of number theory) cannot be proved from So “$T$ is consistent” is an explicit example of a true T. but unprovable statement.
Again, this applies whenis the set of Peano axioms, or any reasonable exten-$T$ sion there of (roughly, any extension that allows oneto encode into arithmetic statements about provability and the like). As a slogan: “a theory cannot prove its own consistency.”The axiom of choice became highly controversial when Ernst Zermelo used it to prove that every set canbe well-ordered, a task that, together with the proof of the continuum hypothesis, Hilbert had listed first among the problems he posed in 1900 to the International Congress of Mathematicians.
In 1938 Gödelshowed that both the axiom of choice and the generalized continuum hypothesis are consequences of another principle (the axiom of construct i bility) that holds in a submodel of any model of Zermelo–Fraenkel set theory. Both are consequently consistent with (not disprovable from) the Zermelo–Fraenkel axioms.
Much later (1963) Paul Cohen showed that both statements are also independent of (not provable from) those axioms. Apart from logic, Gödel also worked in relativity theory, where he established the existence of models of einstein’s field equations [IV.13](/part-04/general-relativity-and-the-einstein-equations) that permit time travel into the past.

Further Reading

Dawson Jr., J. W. 1997.of Kurt Gödel. Natick, MA: A. K. Peters. Logical Dilemmas: The Life and Work John W. Dawson Jr. VI.93 André Weil b. Paris, 1906; d. Princeton, New Jersey, 1998 Algebraic geometry; number theory André Weil was one of the most influential mathemat i-cians of the twentieth century. His influence is due both to his original contributions to a remarkably broad spectrum of mathematical theories, and to the mark

820

he left on mathematical practice and style, through some of his own works as well as through the bourbaki [VI.96](/part-06/nicolas-bourbaki-1935) group, of which he was one of the principal founders. Weil, as well as his sister, the philosopher, political activist, and religious thinker Simone Weil, received anexcellent education. Both were brilliant students, very widely read, with a keen interest in languages (including Sanskrit). André Weil soon specialized in mathematics, his sister in philosophy.
He graduated (and was first in his year in theÉcole Normale Supérieure (ENS) when he was not evenagrégation for mathematics) from the nineteen years old, and traveled in Italy and Germany. He obtained his doctorate in Paris at the age of twenty-two, and then went to Aligarh, India, as a professor for two years. After a brief spell in Marseilles, he was Maître de Conférences at Strasbourg University (along with Henri Cartan) from 1933 to 1939.
The idea of the Bourbaki project arose there from discussions about teaching with Cartan, and grew in Paris in meetings that included other friends from the ENS.His research achievements began with his 1928 Paris thesis. In it, he generalized mordell’s theorem [V.29](/part-05/rational-points-on-curves-and-vi40-ernst-eduard-kummer-18101893) of 1922, that the group of rational points on antic curve [III.21](/part-03/elliptic-curves) is a finitely generated Abelian group, ellipto the group of field [III.63](/part-03/number-fields)) of a Jacobi an variety.
During the follow-K-rational points (where K is a number ing twelve years, Weil branched out in various directions, all related to important research topics of the1930 s: the approximation of holomorphic functions of several variables by polynomials; the conjugation of maximal tori in compact lie groups [III.48 §1](/part-03/lie-theory); the theory of integration on compact and Abelian topological groups; and the definition of uniformcal spaces [III.90](/part-03/topological-spaces). But problems of arithmetic origin topologist ood out among his interests:
further thoughts on his thesis and on Siegel’s finiteness theorem for integral points; a bold “vector bundle” version of the riemannroch theorem with similar work by E. Witt);[V.31](/part-05/the-riemannroch-theorem) on a Riemann surface (in parallelp-adic analogs of elliptic functions Starting in 1940, Weil became active on what was[V.31](/part-05/the-riemannroch-theorem) (with his student Elisabeth Lutz). probably the biggest challenge in arithmetic algebraic geometry at the time.
Helmut Hasse had proved in 1932 the analogue of the riemann hypothesis [IV.2 §3](/part-04/number-theory) for curves of genus 1 (elliptic curves) defined over a field with finitely many elements. The problem was to generalize this to algebraic curves of genus higher than 1. In 1936, Max Deuring had proposed algebraic corre-spondences as a crucial new ingredient for attacking

VI. Mathematicians

this problem; but the problem remained open until World War II. Weil’s initial attempt, written while in jail in Rouen, was very modest, and contained little more than Deuring’s observations of 1936. But, after several years of searching in various directions while in resi-dence in the United States, Weil finally became the first person to prove the analogue of the Riemann hypo the-sis for all nonsingular curves. This proof relied on his complete rewriting of algebraic geometry (over an arbitrary ground field), which he had published before inhis Foundations of Algebraic Geometry (1946).
Further more, Weil generalized the analogue of the riemann hypothesis from curves to algebraic varieties of arbitrary dimensions, defined over a finite field, and added a new topological interpretation of the main invariants of the relevant zeta functions. Taken together, all these conjectures became known as[V.35](/part-05/the-weil-conjectures);
they represented the most important stimulus the weil conjectures for the further development of algebraic geometry right through to the 1970 s, and to some extent later as well. Several mathematicians were at work in the 1930 s and 1940 s trying to rewrite algebraic geometry.
Weil’s Foundations, even though it does contain striking new insights (e.g., a novel definition of intersection multiplicity), owes its basic notions (generic points, spe-cial iz at i ons) to van der Waerden, and it exerted its influence on the mathematical community in conju nc-tion with the (different) rewriting of algebraic geometry developed so successfully by Oscar Zariski from 1938 onward.
It was therefore to a large extent the charac-teristic style, rather than just the “mathematical content,” of theof doing algebraic geometry for the next twenty years Foundations that would create a new way or so, until it began to be replaced by Grothendieck’s language of schemes. Later works include, among other seminal papers and books, Weil’s “adelic” rewriting of Siegel’s workon quadratic forms, and a crucial contribution to the philosophy, due to Taniyama and Shimura, that elliptic curves over the rational numbers should be modular—the proof of this fact is the basis of Wiles’s 1995
proof of In 1947, Weil—whose evasion of the French draft in fermat’s last theorem [V.10](/part-05/fermats-last-theorem). 1939 was considered very critically by many American colleagues—finally obtained a professorship at a distin-guished university, namely Chicago. In 1958, he moved to Princeton as a permanent member of the institute for Advanced Study. many fronts of mathematical research, contributing The postwar years saw Weil continuously active on

VI.94. Alan Turing

insightfully to many subjects that were in the air atthe time. To mention just a few: the Weil groups of class field theory [V.28](/part-05/from-quadratic-reciprocity-to-vi38-augustus-de-morgan-18061871); the explicit formulas of analytic number theory; various aspects of differential geometry, in particular kähler manifolds [III.88 §3](/part-03/symplectic-manifolds); the determination of Dirichlet series by their functional equations.
All of these topics point to seminal works with out which today’s mathematics would not be what it is. In his later years, Weil put his erudition and historical sense to work writing articles and a book on the history of mathematics: Number Theory, an Approach through History. He also published a partial autobiography ending in 1945, literary quality. Souvenirs d’Apprentissage, of considerable

Further Reading

Weil, A. 1976.Kronecker. Ergebnisse der Mathematik und ihrer Grenzge-Elliptic Functions According to Eisenstein and biete, volume 88. Berlin: Springer.. 1980. Oeuvres Scientifiques/Collected Papers, second edn. Berlin: Springer.. 1984. Number Theory. An Approach through History. From Hammurapi to Legendre. 1991. Souvenirs d’Apprentissage. Boston, MA: Birkhäuser.. Basel: Birkhäuser

1991. (English translation: 1992, Mathematician. Basel: Birkhäuser.)The Apprenticeship of a Norbert Schappacher and Birgit Petri VI.94 Alan Turing b. London, 1912; d. Wilmslow, England, 1954 Logic; computing; cryptography; mathematical biology In 1936, as a young Fellow of King’s College, Cambridge, Alan Turing made a crucial contribution to mathematical logic: he defined “computability” with what isnow called the turing machines [IV.20 §1.1](/part-04/computational-complexity).
Although mathematically equivalent to a definition of effective calculability earlier given by concept was compelling because of his entirely orig-church [VI.89](/part-06/alonzo-church-19031995), Turing’s inal philosophical analysis. It won the endorsement of Church, and indeed also of gödel [VI.92](/part-06/kurt-gdel-19061978), whose 1931 incompleteness theorem [V.15](/part-05/gdels-theorem) under lay Turing’s investigation. Using his definition, Turing showed that first-order logic was undecidable, and thus dealt the final death blow togram.
(See logic and model theory hilbert’s [VI.63](/part-06/david-hilbert-18621943) form a list pro-[IV.23 §2](/part-04/logic-and-model-theory) for more details.) in that it gives an exact meaning to the question of Computability is now fundamental in mathematics,

821

whether a method exists to solve a problem. As an illustration, hilbert’s tenth problem [V.20](/part-05/the-insolubility-of-the-halting-problem), on the general solubility of Diophantine equations, was com-pletely resolved in 1970 by methods connected with Turing’s ideas. Turing himself pioneered extensions ofhis definition in mathematical logic, and applications of it in algebra.
However, he was unusual as a mathe-matician in that he explored not only the mathematical uses of his ideas (in questions of decidability in algebra)but also the wider implications for philosophy, science, and engineering. One factor in Turing’s breakthrough was his fascination with the problem of mind and matter. Tur-ing’s analysis of mental states and operations has since become a point of departure for the cognitive sciences. Turing himself blazed this trail later by his advocacy of the possibility of artificial intelligence.
His famous 1950 “Turing test” was part of an extensive range ofresearch proposals in this field. A more immediately applicable aspect of his 1936 work lay in his observation that a single “universal”machine could do the work of any Turing machine, by reading the description of that machine as a table of instructions. This is the essential principle of the modern digital computer, whose programs are themselves data structures. In 1945 Turing used this insight to plana first electronic computer and its programming.
He was preempted by argued that von Neumann had used Turing’s insight von neumann [VI.91](/part-06/john-von-neumann-19031957), but it can be that computing must be primarily an application of logic. Thus, Turing laid the foundations of modern computer science. Turing was able to bridge theory and practice because between 1938 and 1945 he was the chief sci-entific figure in British cryptography, with particular responsibility for decrypting German naval signals.
His main contributions lay in a brilliant logical solution of the Enigma cipher, and in Bayesian information theory. The advanced electronics employed in British code breaking gave him the experience to become a pioneer of practical computing as well. neering, and increasingly with drew from attempts to Turing had less success in postwar computer engiinfluence the course of computer development. In-stead, at Manchester University after 1949 he concentrated on a theory of nonlinear partial differ en-tial equations applied to biological development.
Like his 1936 work, this opened an entirely new field. It also illustrated his broad mathematical scope, which

822

included important work on the tion [IV.2 §3](/part-04/number-theory). He was busy working on biological riemann zeta func theory and new ideas in physics at the time of his sudden death. Turing’s short life combined the purest mathematics and the most practical applications. It was also marked by other contrasts. Although he promoted the theme of computer-based artificial intelligence, there was nothing mechanical about his thought or life. The wit and drama of the “Turing test” have made him a lasting figure in the popularization of mathematical ideas.
the dramatization of his life, drawing on the extraordinary secrecy of his war work, and his subsequent persecu-tion as a homosexual, have also attracted great public interest.

Further Reading

Hodges, A. 1983.Simon & Schuster. Alan Turing: The Enigma. New York: Turing, A. M. 1992–2001.Turing. Amsterdam: Elsevier. The Collected Works of A. M.

Andrew Hodges

VI.95 Abraham Robinson

b. Waldenburg (Lower Silesia; now Walbrzych, Poland), 1918; d. New Haven, Connecticut, 1974 Applied mathematics; logic; model theory; nonstandard analysis Robinson was educated at a private Rabbinical school and then at the Jewish High School in Breslau until 1933, when he emigrated with his family to Palestine. There Robinson finished high school, going on to study mathematics at the Hebrew University under Abraham Fraenkel. He spent the spring of 1940 at the Sorbonne, but when the Germans invaded France Robinson made his way to England.
There he spent the war as a refugee, in the service of the Free French Forces. Robinson’s mathematical talents were soon recognized, and he was assigned to the Royal Aircraft Establishment in Farnborough, where he was part of a team designing supersonic delta wings and reconstructing German V-2 rockets to determine how they worked. After the war, Robinson received his M.Sc. degree in mathematics from the Hebrew University, with minors in physics and philosophy. Several years later, he completed his Ph. D. in mathematics at Birkbeck College, London.
His thesis,“On the metamathematics of algebra,” was published in

1951.

College of Aeronautics since its founding in Cranfield in Meanwhile, Robinson had been teaching at the Royal

VI. Mathematicians

October of 1946. Although promoted to Deputy Head ofthe Department of Aeronautics in 1950, in the following year Robinson accepted a position, at the rank of associate professor, at the University of Toronto in the Department of Applied Mathematics. While at Toronto, most of his publications were devoted to applied math-ematics, including papers on supersonic airfoil design and a book he coauthored with his former student from Cranfield, J. A. Laurmann, on His years at Toronto (1951–57) proved to be a tran-Wing Theory.
sitional period in Robinson’s career, as his interests turned increasingly toward mathematical logic, beginning with studies of algebraically closed fields of characteristic zero. In 1955 he published a book in french summarizing much of his early work in mathematical logic and matique des Ideaux model theory. Robinson was a pioneering con-[IV.23](/part-04/logic-and-model-theory), Théorie Métamathétributor to model theory, which at its simplest uses mathematical logic to analyze mathematical structures(like groups, fields, or even set theory itself).
Given an axiomatic system, aisfies the axioms. One of his early impressive results model is a structure that satwas a model-theoretic proof, which he published in 1955 inteenth problem, namely that a positive-definite rational Mathematische Annalen, of Hilbert’s seven function over the reals can be expressed as a sum of squares of rational functions. This was soon followed by another book, Complete Theories (1956), which further extended ideas he had explored earlier in his thesis on model-theoretic algebra.
Here Robinson introduced such important concepts as model completeness, model completion, and the “prime model test,” along with proofs of the completeness of real-closed fields [IV.23 §5](/part-04/logic-and-model-theory) and the uniqueness of the model completion of a model-complete theory. University, where he assumed the chair formerly heldby his teacher Abraham Fraenkel in the Einstein Insti-In the fall of 1957 Robinson returned to the Hebrew tute of Mathematics.
While at the Hebrew University, Robinson worked on aspects of local differential algebra, differentially closed fields, and in logic on[VI.81](/part-06/thoralf-skolem-18871963) results dealing with nonstandard models of skolem’s arithmetic. These provide models of ordinary peano arithmetic gers (0, 1, 2, 3[III.67](/part-03/the-peano-axioms), the usual arithmetic of the inte-, . . .), but ones that include “nonstandard” elements, “numbers” that extend the scope of the stan-dard model to models that are larger but nevertheless satisfy the axioms of the standard structure.
A nonstandard model of arithmetic may include, for example, infinite integers. As Haim Gaifman puts it succinctly, “A

VI.96. Nicolas Bourbaki

nonstandard model is one that constitutes an inter pre-tation of a formal system that is admittedly different from the intended one.” States, at Princeton, replacing was on sabbatical leave. It was there that Robinson Robinson spent the year 1960–61 in the united church [VI.89](/part-06/alonzo-church-19031995), who was inspired to make his most revolutionary contri-bution to mathematics, nonstandard analysis, using model theory to allow the rigorous introduction of infinitesimals.
In fact, this extended the usual, standard model of the real numbers to a nonstandard model that included both infinite and infinitesimal elements. He first published on this topic in 1961 in the Proceedings of the Netherlands Royal Academy of Sci-ences.
This paper was soon followed by a book, Introduction to Model Theory and to the metamathematics of Algebra (1963), a thorough revision of his earlier book of 1951, including a new section on nonstandard analysis. Meanwhile, Robinson had left Jerusalem for Los Angeles, where he was appointed as Carnap’s chair at UCLA in mathematics and philosophy. In addition to writing an introductory text, Numbers and Ideals:
An Introduction to Some Basic Concepts of Algebra and Number Theory (1965), he also published his definitive introduction tothe important results he obtained while at UCLA (1962–Nonstandard Analysis (1966). Among 67) was his proof of the invariant subspace theorem in Hilbert space for the case of polynomially compact operators, published with his graduate student Allen Bernstein. (The case for compact operators had been established by Aronszajn and Smith in 1954;
what Bernstein and Robinson did was extend this to the case of an operator T is compact.)T such that some nonzero polynomial of 74), where he was eventually given a Sterling Profes-sorship in 1971. Among Robinson’s most important In 1967 Robinson moved to Yale University (1967 mathematical achievements during this period were his extension of Paul Cohen’s method of forcing [IV.22 §5.2](/part-04/set-theory) in set theory to model theory, and applica-tions of nonstandard analysis in economics and quantum physics.
He also applied nonstandard analysis to achieve an outstanding result in number theory, namely a simplification of Carl Ludwig Siegel’s theorem regarding integer points on curves (1929), as generalized by Kurt Mahler for rational as well as integer solutions (1934). This was work that Robinson did jointly with Peter Roquette; together they extended the Siegel–Mahler theorem by considering nonstandard integer

823

points and nonstandard prime divisors. After Robin-son’s death from pancreatic cancer in 1974, Roquette published this work in the Journal of Number Theory in 1975.

Further Reading

Dauben, J. W. 1995.of Nonstandard Analysis. A Personal and Mathematical Abraham Robinson. The Creation Odyssey. 2002. Abraham Robinson. 1918–1974.. Princeton, NJ: Princeton University Press. Biographical Davis, M., and R. Hersh. 1972. Nonstandard analysis. Memoirs of the National Academy of Science st if ic American 226:78–86. 82:1–44.Scien Gaifman, H. 2003. Non-standard models in a broader per-spec tive. In Nonstandard Models of Arithmetic and Set Theory Providence, RI: American Mathematical Society., edited by A. Enayat and R. Kossak, pp. 1–22. Joseph W. Dauben VI.96 Nicolas Bourbaki b.
Paris, 1935; d. —Set theory; algebra; topology; foundations of mathematics; analysis; differential and algebraic geometry; integration theory; spectral theory; Lie algebras; commutative algebras; history of mathematics Bourbaki is a pseudonym chosen in 1935 by a group of French mathematicians, including Henri Cartan, Jean Dieudonné, and andré weil [VI.93](/part-06/andr-weil-19061998). Under this nom de plume, several generations of mostly french mathematicians conceived, wrote, and published a series of treatises under the general title de Mathématique.
The uncommon use of the singu-Éléments lar “mathématique” underscored a strong commitment to the unity of mathematics that is one of the chief characteristics of the group. Together with the “Bourbaki Seminar,” this monumental work promoted a uni-fied, axiomatic, and structural view of pure mathematics that has exerted a strong influence on teaching and research since World War II, especially in France. who fought in the Franco-Prussian war in 1870–71.
Ahoax lecture given by students at the École Normale Charles Denis Sauter Bourbaki was a French general Supérieure to the entering class in 1923 culminated with a “Bourbaki theorem.” In 1935, a group of mathe-maticians, many of whom had taken part in that lecture, as either audience or pranksters, decided to adopt that name for the fictive author of the modern treatise ofanalysis they were planning to write. December 10, 1934. In addition to Cartan, Dieudonné, Their first meeting had taken place in Paris on

824

and Weil, other young university professors of mathe-matics were present: Claude Chevalley, Jean Delsarte, and René de Possel. Agreeing that analysis textbooks available in French (such as Édouard Goursat’sd’Analyse) were outdated, they decided to write a book, Cours collectively, to replace them.
Having been in touch with modern German mathematics, especially at hilbert’s [VI.63](/part-06/david-hilbert-18621943) Göttingen, and influenced in particular by Barteel van der Waerden’sthat their large treatise should begin with an “abstract Moderne Algebra, they thought packet” summarizing in axiomatic form basic general notions such as sets, groups, and fields. Soon after this, Szolem Mandelbrojt joined the group.
Paul Dubreil and Jean Leray took part in just a few of the original meet-ings, and were replaced by Charles Ehresmann and the physicist Jean Coulomb. In July 1935, the group had its first “congress” (as its annual summer meetings would later be called) in Besse-en-Chandesse, Auvergne, where the pen name“N. Bourbaki” was definitively adopted (the first name, Nicolas, was chosen later). Settling on working proce-dures, they drew up the general out line of the planned treatise. The members of the group worked collectively following certain ritual rules.
They co-opted new col-laborators, kept membership secret, and refused to acknowledge individual contributions. During the three or four working sessions they held every year, each contribution prepared in advance by one of them was read line by line, discussed, and severely criticized by the others. Up to ten successive drafts and several years of work by various authors were often needed before afinal version was unanimously adopted. The first book let—a digest of results in set theory— was dated 1939 but issued in 1940.
Despite the dif-ficult working conditions during World War II, this was soon followed in the 1940 s by several book-lets dealing mostly with general topology and algebra. Today, the Elements of Mathematics consists of several books: Variable Functions Theory of Sets, Topological Vector Spaces, Algebra, General Topology, Integra-, Real tion Manifolds, Commutative Algebra, Lie Groups and Lie Algebra, Differential and Analytic, Spectral Theories, and Elements of the History of Mathematics.
Many of them have been extensively revised over the years and translated into several languages, including English and Russian. The first six books formed a tight linear exposition entitled “The fundamental structures of analysis.” When they first appeared, they were striking for the log-ical organization of the topics covered. The axiomatic

VI. Mathematicians

method was used systematically, and great effort was made to ensure a global unity of style, notation, and terminology. The avowed ambition was to take math-ematics from its very start and, proceeding from the general toward the particular, write a unified survey ofmost of modern mathematics. into the “Association of Bourbaki’s Collaborators,” asthe group is now officially known.
After World War II, Several generations of mathematicians were co-opted Samuel Eilenberg, Laurent Schwartz, Roger Godement, Jean-Louis Koszul, and Jean-Pierre Serre, among others, took part in the writing of the treatise. Later, Armand Borel, John Tate, François Bruhat, Serge Lang, and Alexander Grothendieck also joined.
Although its frequency of publication has now slowed to a trickle, the group is still functioning in the first decade of the twenty-first century. Notwithstanding the number of collaborators involved and the extensiveness of the work they published, Bourbaki’s vision of mathematics was, andhas remained, surprisingly coherent. Most of the crucial mathematical choices, which would come to havea huge impact on the structural image of mathematics that the group would later vigorously promote, were made in the late 1930 s.
In the follow-ing decades, many mathematicians shared a conviction that a tight axiomatic refoundation of their research domains would help over come current blockages. This was felt, for example, in probability theory, model theory, algebraic geometry and topology, commutative algebra, Lie groups, and Lie algebras. group and its individual members steadily grew, Bour-baki’s public image soon encompassed more than just After World War II, as the notoriety of both the the treatise.
At the level of mathematical research, the Bourbaki Seminar was a prestigious outlet established in Paris in 1948, and it has met three times ayear ever since. Members of Bourbaki selected speakers who usually summarized some one else’s work, and supervised the publication of their talks. The topics selected emphasized specific domains of mathematics, such as algebraic and differential geometry, at the expense of others, such as probability theory or applied mathematics.
were always clear, especially after two articles pub-lished in the late 1940 s under that name argued for Bourbaki’s views on the philosophy of mathematics a complete reorganization of mathematics, eschewing older classification schemes in favor of fundamental structures (some times called “mother-structures” and

VI.96. Nicolas Bourbaki

supposedly closer to the deep mental structures of humans) meant to underscore the organic unity of mathematics. Bourbaki’s public image was echoed by structuralists in the human sciences as well as artists and philosophers, and it was invoked by radical reformers of mathematical education from kindergarten to university—although actual members of Bourbaki were rarely involved directly. From the late 1960 s, Bourbaki’s critics became louder on two counts:
they took issue with the bourbaki approach to the logical foundations of mathematics and they found gaps in the group’s encyclopedic objec-tives. category theory [III.8](/part-03/categories) developed by Saunders Mac Lane and Samuel Eilenberg was found to offer a more fruitful foundational framework than Bourbaki’sstr uct ures. It also became clear that whole branches of mathematics—probability theory, geometry, and, to a lesser extent, analysis and logic—were to remain absent from the treatise, their very place in the grand architec-ture of Bourbakist mathematics left unclear. For a new

825

generation of mathematicians, it was Bourbaki’s eli-tist contempt for applications that was especially damaging. Bourbaki’s impact on mathematics was profound: despite its excesses, Bourbaki’s unified, structural, rig-orous image of mathematics is still with us. But it was those very characteristics that led to a feeling that Bour-baki was corseting mathematical research. The backlash seems to be abating some what nowadays, but no new Bourbaki is in view.

Further Reading

Beaulieu, L. 1994. Questions and answers about Bourbaki’searly work (1934–1944). In The Intersection of History and Mathematics Birkhäuser. , edited by S. Chikara et al., pp. 241–52. Basel: Corry, L. 1996.Structures. Basel: Birkhäuser. Modern Algebra and the Rise of Mathematical Mac Lane, S. 1996. Structures in mathematics. Mathematica 4:174–86. Philosophia

David Aubin

This page intentionally left blank

The Influence of Mathematics

VII.1 Mathematics and Chemistry

Jacek Klinowski and Alan L. Mackay

1 Introduction

Since gation (described by Vitruvius) of the proportions of archimedes [VI.3](/part-06/archimedes-ca), and his experimental investigold and silver in an alloy, the solution of chemical problems has employed mathematics. Carl Schorlemmer studied the paraffinic series of hydrocarbons (then important because of the discovery of oil in Pennsylvania) and showed how their properties changed with the addition of successive carbon atoms.
His close friend in Manchester, Friedrich Engels, was inspired by this tointroduce the transformation of “quantity into quality” into his philosophical out look, which then became a mantra of dialectical materialism. From a similar chemical observation, cayley [VI.46](/part-06/arthur-cayley-18211895) in 1857 developed “rooted trees” and the mathematics of the enum era-tion of branched molecules, the first articulation of graph theory his fundamental enumeration theorem, facilitating fur-[III.34]. Later, George Pólya developed ther advances in the counting of these molecules.
Still more recently, chemical problems such as the mechanics and kinematics of DNA have had a significant influence on knot theory [III.44](/part-03/knot-polynomials). science for no more than 150 years. Before this, it was However, chemistry has been a quantitative modern a distant dream: when ing the calculus in around 1700, much of his time was newton [VI.14](/part-06/isaac-newton-16421727) was develop spent working on alchemy.
He explained why, having established “the motions of the planets, the comets, the Moon and the sea,” he was unable to determine the remaining structure of the world from the same propositions: I suspect that they may all depend upon certain forces by which the particles of the bodies, by some causes hitherto unknown, are either mutually impelled toward one another, and cohere in regular figures, or are

Part VII

repelled and recede from one another. These forces being unknown, philosophers have hitherto attempted the search of Nature in vain; but I hope the principles laid down will afford some light either to this or some truer method of philosophy. The nature of such forces came to be understood only two hundred years later, and indeed the electron, the particle responsible for chemical bonding, was not discovered until 1897.
This is why the main flow of ideas has been from mathematical theory to applications in chemistry. Some of the fundamental equations of chemistry, though based on experiment rather than strict mathe-mat ical reasoning, convey a wealth of information with great simplicity and elegance (Thomas 2003). For example, consider Boltzmann’s fundamental equation of sta-tistical thermodynamics, which links entropy, S, to Ω, the number of possible ways of arranging the particles: S = k . og  Ω, where k is known as the Boltzmann con- stant.
There is also the expression derived by Balmer for the wavelength,λ, of spectral lines from hydrogen in the visible portion of the spectrum: 1λ = R n12 - n12, where as the Rydberg constant. A third example, the Braggn1 and n2 are integers,1 (n1)2< n2, and R is known equation, links the wavelength, X-rays, the distance, d, between planes in a crystal lat-λ, of monochromatic tice, and the angle, the direction of the X-rays. It says thatθ, between the crystal planes andnλ = 2 d. in  θ, where rule,”Pn+is a small integer.
Finally, there is the “phase F = C +2, which links the number of phases, Pber of components,, the number of degrees of freedom, C, in a chemical system. This is F, and the num- the same relationship as that between the number ofvertices, faces, and edges in a convex polyhedron, and emerges from the geometrical representation of the system. In recent years computers have become the dominant tool in theoretical chemistry. Not only can computers

828

solve differential equations numerically, they can often provide exact algebraic expressions, some times even ones that are too elaborate to write out. Computing has required the development of algorithms in the fields of structure has been revolutionized by the advent of computers:, process, modeling, and search. Mathematics in particular in the facility for dealing with nonlinear problems and for displaying results graphically. This has led to fundamental advances, some of them bearingon chemistry.
problems can be divided into discrete and continuous treatments, reflecting on the one hand the fund a men-In general, mathematical approaches to chemical tal discrete atomic nature of matter and on the other the continuous statistical behavior of large numbers of atoms. For example, enumerating molecules is a discrete problem, while a problem involving global mea-sures such as temperature and other thermodynamic parameters will be continuous.
These treatments have required different branches of mathematics, with integers more important for discrete problems and real numbers more important for continuous ones. We shall now out line some chemical problems to which, in our view, mathematics has made the most significant contributions. 2 Structure

2.1 Description of Crystal Structure

Crystal structure themselves to form macroscopic materials. Early ideas is the study of how atoms arrange in the subject were based purely on the symmetry ofcrystals and their morphology (that is, the shapes they tended to form), and were developed in the nineteenth century in the absence of definite information about the atomic structure of matter. The 230 which codify different ways of arranging objects peri-space groups, od ically in three-dimensional (3 D) space, were found independently by Fedorov, Schoenflies, and Barlow between 1885 and 1891.
They result from the systematic combination of a certain collection of fourteen lattices, named Bravais lattices after their discovery in 1848 by Auguste Bravais, with the thirty-two so-called tal lo graphic point groups, which were developed from cr ys morphological considerations. Since the diffraction of X-rays was demonstrated in 1912 by Max von Laue and practical X-ray analysis was developed by W. H. Bragg and his son W. L. Bragg, the crystal structures of several hundred thousand inor-ganic and organic substances have been determined.

VII. The Influence of Mathematics

However, such analysis was for a long time held back bythe time required for the calculation of fourier transforms owing to the discovery of[III.27](/part-03/the-fourier-transform). This difficulty is now a thing of the past, the fast fourier transforms ally applied algorithm and one of those most often[III.26](/part - 03/the - fast - fourier - transform) by Cooley and Tukey in 1965—a uni ver cited in mathematics and computer science. The fundamental geometry of two - dimensional (2 D) and 3 D spatial structures led mathematicians to seek analogous problems inwork has found application in the
description of N dimensions. Some of this quasi- crystals crystals, exhibit a high degree of organization, but, which are arrangements of atoms that, like which lack the periodic behavior of crystals. (That is, they do not have translational symmetry.) The most not able example is the following, which uses six-dimensional geometry. Take a regular cubic lattice dimensions and let V be a 3 D subspace of R6 that con - L in six tains no point ofto V all points from L apart from the origin.
Now project on L that are closer to V than a certain distance exhibits a great deal of local regularity but not glo bald. The result is a 3 D structure of points that regularity. This structure gives a very good model for quasicrystals. Until recently, crystals in three dimensions had always been thought of as periodic, and therefore capable of showing only twofold, threefold, fourfold, or sixfold axes of symmetry. Fivefold axes were excluded, because a plane cannot be tiled with regular pentagons.
How - ever, in 1982, X-ray and electron diffraction demonstrated the presence of fivefold diffraction symmetryin certain rapidly cooled alloys. Careful electron microscopy was necessary to distinguish the observed structures from the twinning (symmetrical intergrowth) of“normal” crystals. This discovery, of a quasicrystal line alloy phase “with long-range orientational order and no translational symmetry,” has brought about an ideological shift in crystallography. one possible mathematical formalism for the descrip-tion of quasicrystals.
Quasilattices have two incom-The earlier concept of a “quasilattice” appeared to be men sur able periods in the same direction, and the ratio of these periods was given by so-called Pisot and Salem numbers. Apolynomial with integer coefficients of degree Pisot numberθ is a root of am such that if$i = 2$, . . . , mθ2, . . . , θ. A real quadraticm are the other roots, then algebraic integer|θi| < 1, [IV.1 §11](/part - 04/number - theory) greater than 1 and of degree 2 or 3 is a Pisot number if its norm is equal to±1.
The golden ratio is an example of a Pisot number since it has degree 2 and VII.1. Mathematics and Chemistry norm - 1. A Salem number is defined in a similar way to a Pisot number, but with the inequalities replaced by equalities. used to describe quasicrystals. This has stimulateda great deal of theoretical lie algebra [III.48 §2](/part - 03/lie - theory) arguments have also been N-dimensional geometry.
Before the discovery of quasicrystals, Roger Penrose had shown how to cover a plane nonperiodically using two different types of rhombic tiles, and correspond-ing rules were developed for 3 D space with two kinds of rhombohedral tiles. The Fourier transform of sucha 3 D structure with atoms placed in the rhombohedral cells explains the observed diffraction patterns of 3 D quasicrystals, while Penrose’s 2 D pattern cor-responds to decagonal quasicrystals, which consist of stacked layers of the 2 D pattern and which have been experimentally observed.
The broadening of classical crystallography to encompass quasicrystals has been given further impe-tus by recent advances in electron microscopy. It is now possible to observe atomic arrangements directly, including those of the decagonal quasicrystals just mentioned, rather than having to deduce them from diffraction patterns, where the phases of the various diffracted beams are lost in the experimental system and have to be recovered mathematically. The whole field of computational and experimental image processing has become coherent as a result.
of a single repeating unit, but the unit is a compos-ite object, a pattern made out of identical decagons. Another model describes 2 D quasicrystals in terms Unlike the unit cells in periodic crystals, these quasi-unit cells are allowed to overlap, but where they do their constituent decagons must match up. This conceptual device is an alternative to the use of two kinds of unit cell. It emphasizes the dominating physical presenceof locally ordered atomic clusters, with no long-range order, and it can be extended to three dimensions.
The predictions of this model agree with the observed com-position of a 2 D decagonal quasicrystal, as well as with the results obtained by electron microscopy and X-ray diffraction. Nevertheless, although a huge amount of interesting mathematics has been generated by the discovery of quasicrystals, most of it is not physically relevant: the structures emerge from the competition between local and global ordering forces rather than from the mathematics of the Penrose tiling.
The acceptance of quasicrystals demonstrates the need to accommodate more general concepts ofinto classical crystallography. It has explicitly intro - order 829 duced concepts of ordered clusters of atoms but ordered clusters of clus - hierarchy, by involving not just ters, where local order has predominated over the regular lattice repetition. Quasicrystals represent the first step from absolute regularity toward more gen-eral structures that are intimately bound up with the notion of Information can be stored in a device which has two information.
or more clearly identifiable states that are metastable. This means that each state is a local equilibrium, and topass from one to another, one must supply and remove enough energy to take the device over the local energy watershed. A switch, for example, can be on or off; it is stable in either state and to change the state takes a certain amount of energy. To take a more general example, any information, encoded as a sequence of binary digits, can be read in, read out, and stored as a sequenceof magnetic domains, where each one is magnetized either north or south.
so cannot be used to store information, but a piece of silicon carbide, for example, exists as a sequence of Perfect crystals have no alternative metastable states, close-packed layers, each of which may be in one or other of two almost equivalent positions. To describe the structure of a piece of silicon carbide therefore demands a knowledge of the sequence of positions in which the layers are stacked. This can be represented by a string of binary digits.
Now that it is possible to arrange atoms in a structure almost at will, at least if they are on a surface, the processing of information has become important to chemistry. mathematics has been essential for the solution of the phase problem In determining the arrangement of atoms in crystals,, which had held up progress in structural chemistry and molecular biology for decades. Apattern of diffracted X - rays, recorded as an array of spots on a photographic plate, depends on the arrange-ment of atoms in the molecule causing the diffraction.
The problem is that the diffraction pattern registers only the intensity of the light waves, but to work back to the molecular structure it is necessary to know their phase as well (that is, the positions of the crests and troughs of the waves relative to each other). This results in a classic inverse problem, which was solved by Jerome and Isabella Karle and Herbert A. Hauptman. A Voronoi diagram consists of points, representing atom sites, with each point contained in a region (see also mathematical biology [VII.2 §5](/part - 07/mathematical - biology)).
The region surrounding a given site consists of all points that are closer to that site than to any of the other sites 830 Figure 1 Voronoi dissection of 2 D space. (figure 1). The geometric dual of the Voronoi diagram, a system of triangles with the sites as vertices, is called theof the Delaunay triangulation is that it is a triangulation Delaunay triangulation.
(An alternative definition of the sites with the additional property that, for each triangle, the circumcircle of that triangle contains no other sites.) These dissections give a well-defined wayof representing many N-dimensional chemical struc- tures as arrangements of polytopes. Crystals, which have periodic boundaries, are easier to deal with than extended structures that terminate in a boundary. The Voronoi dissection of crystal structures enables one todescribe them as networks.
Nevertheless, despite much progress in understanding structure, it is not yet pos-sible to guess a crystal structure in advance just from the composition of elements in its molecules. 2.2 Computational Chemistry Attempts to solve which gives the quantum mechanical description ofthe schrödinger equation [III.83](/part - 03/the - schrdinger - equation), matter, began soon after it was proposed in 1926.For very simple systems, calculations performed on mechanical calculators agreed with the experimental results of spectroscopy.
In the 1950 s, electronic computers became available for general scientific use, andthe new field of computational chemistry developed, the aim of which was to obtain quantitative infor-mation on atomic positions, bond lengths, electronic configurations of atoms, etc., by means of numerical solutions of the Schrödinger equation. Advances dur-ing the 1960 s included deriving suitable functions for representing electronic orbitals, obtaining approximate VII.
The Influence of Mathematics solutions to the problem of how the motions of differ-ent electrons correlate with each other, and providing formulas for the derivative of the energy of a molecule with respect to the positions of the atomic nuclei. Powerful software packages became available in the early1970 s. Much current research is aimed at developing methods that can handle larger and larger molecules. is a major recent field of activity in quantum mechan-ical computation, and concerns macroscopic features Density functional theory (DFT) (Parr and Yang 1989) of materials.
It has been successful in the description of the properties of metals, semiconductors, and insulators, and even of complex materials such as pro-teins and carbon nanotubes. Traditional methods in the study of electronic structure—such as one called the Hartree–Fock theory molecular orbital method, which assigns the electrons two at a time to a set of molecular orbitals—involve very complicated many-electron wave functions.
The main objective of DFT is to replace the many-body electronic wave function, which depends on 3 tronic density Nvariables, with a different basic quantity, the, which depends on just 3 variables, and elec therefore greatly speeds up calculations. The partial differential equations of quantum mechanics, physics, fields, surfaces, potentials, and waves can some times be solved analytically, but even if they cannot, they are now almost always soluble by numer-ical methods. All this relies on the corresponding pure mathematics.
(For a discussion of how to solve par-tial differential equations numerically, see numerical analysis [IV.21 §5](/part - 04/numerical - analysis).) 2.3 Chemical Topology Isomers of the same elements but have different physical andare chemical compounds that are made out chemical properties. This can happen for various reasons. In groups are linked together in different ways. this structural isomers, the atoms and functional class includes have variable amounts of branching, and chain isomers, where hydrocarbon chains position isomers chain is different (figure 2(a)).
In, where the position of a functional group in a stereoisomers the bond structure is the same, but the geometrical posi-tion ing of atoms and functional groups in space differs (figure 2(b)). This class includes where different isomers are mirror images of each other optical isomers, (figure 2(c)). While structural isomers have different chemical properties, stereoisomers behave identically in most chemical reactions. There are also isomers such as catenanes and DNA. topological VII.1.
Mathematics and Chemistry (a) CH3 CH3 H3 C CH CHCH CH2 CH H3 C CH CH2 CH CH CH 2 2 3 2 2 3 (b) H H H Cl C C C C Cl Cl Cl H H H (c) HOOC C NH2 H2 N C COOH CH3 H3 C Figure 2 (a) Position isomerism. (b) Stereoisomerism.(c) Optical isomerism. mining, for any given molecule, how many isomers ithas. To do this, one first associates with any molecule An important theme in chemical topology is detera molecular graph, the vertices representing atoms and the edges representing chemical bonds.
To enumer-ate stereoisomers, one counts the symmetries of this graph, but first one must consider symmetries of the molecule (Cotton 1990) in order to decide which symmetries of the graph correspond to spatial trans for-ma tions that make chemical sense. Cayley addressed the problem of enumerating is, combinatorially possible branched molecules. to structural isomers, that do this, one must count how many different molecular graphs there are with a given set of elements, where two graphs are regarded as the same if they are isomorphic.
The enumeration of isomorphism types uses group theory to count the intrinsic graph symmetries. After Pólya published his remarkable tion theorem [IV.18 §6](/part - 04/enumerative - and - algebraic - combinatorics) in 1937, his work us in ge num era-generating functions groups [III.68](/part - 03/permutation - groups) became central to the enumeration of [[IV.18 §§2.4, 3]](/part - 04/enumerative - and - algebraic - combinatorics) and permutation isomers in organic chemistry. The theorem solves the general problem of how many configurations there are with certain properties.
It has applications such as the enumeration of chemical compounds and the enumeration of rooted trees in graph theory. A new branch of graph theory, called enumerative graph theory, is based on Pólya’s ideas (see combinatorics [IV.18](/part - 04/enumerative - and - algebraic - combinatorics)).algebraic and enumerative molecules with remarkable topologies have been syn-Although not all the possible isomers occur in nature, 831 the sized artificially. Among them are which contains eight carbon atoms arranged at the cubane, C8 H8, corners of a cube, each linked to a single hydrogen atom;
dodecahedrane, C H , which, as its name suggests, has a dodecahedral shape; the knot; and the self-assembling compound20 20 molecular trefoil olympiadane composed of five interlocked rings. Catenanes (from Latin more interlocked rings that are inseparable with out catena, chain) are molecules containing two or breaking a covalent bond.wheel, and axis, axle) are dumbbell shaped, having a Rotaxanes (from Latin rota, rod and two bulky stopper groups, around which there are encircling macrocyclic components. The stoppers of the dumbbell prevent the macrocycles from slipping off the rod.
Even a molecular möbius strip [IV.7 §2.3](/part - 04/dierential - topology) has recently been synthesized. Macromolecules, such as synthetic polymers and biopolymers (e.g., DNA and proteins), are very large and highly flexible. The degree to which a polymer molecule coils and knots and links with other molecules is crucial to its physical and chemical properties, such as reactiv - ity, viscosity, and crystallization behavior.
The topological entanglement of short chains can be modeled using Monte Carlo simulation, and the results can now be experimentally verified with fluorescence microscopy. DNA, the central substance of life, has a complex and fascinating topology, which is closely related to its biological function.
The major geometric descriptions of supercoiled DNA (that is, DNA wrapped around a series of proteins) involve the concepts of linking, twisting, and writhing numbers that come from knot theory. DNA knots, which are created spontaneously within cells, interfere with replication, reduce transcription, and may decrease the stability of the DNA. “Resolvase enzymes” detect and remove these knots, but the mech-anism of this process is not understood. However, using topological concepts of knots and tangles, onecan gain insight into the reaction site and there by try to infer the mechanism.
(See also biology [VII.2 §5](/part - 07/mathematical - biology).) mathematical 2.4 Fullerenes Graphite and diamond, the two crystalline forms of the element carbon, have been known since time immemorial, but exist naturally in soot and geological deposits, were dis - fullerenes, which were subsequently found to covered only in the mid 1980 s. The most common is the almost-spherical carbon cage C molecule (figure 3), also known as “buckminsterfull erene” after the archi-tect who designed enormous domes, but fullerenes C60 24,

832

Figure 3 The structure of the fullerene C60 .

Cprovides insights into the possible types of such struc-28, C32, C36, C50, C70, C76, C84, etc., also exist. Topology tures, while group theory and graph theory describe the symmetry of the molecules, allowing one to interpret their vibrational modes. In all fullerenes, each carbon atom is connected to exactly three neighboring ones, and the resulting molecule is a “cage” made of rings of either five or six carbon atoms.
From tion ship$(6 - n)f = euler12$, where’s [VI.19](/part-06/leonhard-euler-17071783) topological rela-f is the number ofnthe polyhedron, we conclude first that-hedral faces and the summation is over all faces of$(nn)n f = 12$, since n is found to take only the values 5 or 6, and second thatf can take any value greater than 1.5 tence of ordered structures of a new kind, derived from graphite and related to fullerenes, with topolo-In 1994, Terrones and Mackay predicted the exisgies of triply periodic These new structures, which are of great practical inter-minimal surfaces [III.94
§3.1](/part-03/variational-methods). est, are produced by introducing eight-membered rings of carbon atoms into a sheet of six-membered rings. This gives rise to saddle-shaped surfaces of negative gaussian curvature [III.78](/part-03/ricci-flow), unlike the fullerenes, which have positive curvature. Thus, to model them mathematically one must consider embeddings of non-Euclidean 2 D spaces into R3. This has contributed to a renewed interest in certain aspects of non-euclidean geometry.

2.5 Spectroscopy

Spectroscopy is the study of the interaction of elec-tro magnetic radiation (light, radio waves, X-rays, etc.)

VII. The Influence of Mathematics

with matter. The central portion of the electromagnetic spectrum—spanning the infrared, visible, and ultraviolet wavelengths and the radio frequency region—isof particular interest to chemistry. A molecule, which consists of electrically charged nuclei and electrons, may interact with the oscillating electric and magnetic fields of light and absorb enough energy to be promoted from one discrete vibrational energy level to another. Such a transition is registered in the infrared spectrum of the molecule.
The Raman spectrum monitors in elastic scattering of light by molecules (that is, when some of the light is scattered at a different fre-quency from the frequency of the in coming photons). Visible and ultraviolet light can redistribute the electrons in the molecule: this is electronic spectroscopy. spectra of chemical compounds (Cotton 1990; Hollas2003). For any given molecule, the symmetry opera-Group theory is essential in the interpretation of the tions that can be applied to it form aand can be represented by matrices.
This allows one to group [I.3 §2.1](/part-01/fundamental-definitions), identify “spectroscopical ly active” events in a molecule. For example, just three bands are observed in the infrared spectrum and eight bands in the Raman spectrum of dodecahedrane. This is a consequence of the icosahedral symmetry of the molecule and is what one expects from group-theoretic considerations. Also, there are no coincidences between the infrared- and Raman-active modes.
Similarly, group theory correctly predicts that, because of the high symmetry of a Cmolecule, it has only four lines in its infrared spectrum60 and ten in its Raman spectrum, even though it has 174 vibrational modes.

2.6 Curved Surfaces

Structural chemistry has greatly changed in the last twenty years. First, as we have seen, the rigid concept of a “perfect crystal” has been relaxed to embrace structures such as quasicrystals and textures. Second, an advance has been made from classical geometry to 3 D differential geometry. The main reason for this has been the use of curved surfaces for describing a great variety of structures (Hyde et al. 1997). film is formed.
Surface tension minimizes the energy When a wire frame is dipped into soapy water, a thin of the film, which is proportional to its surface area. As a result, the film has the smallest area consistent with the shape of the frame and with the requirement that the point. If the symmetries of a minimal surface are given mean curvature of the film be zero at every

VII.1. Mathematics and Chemistry

y

z

x

Figure 4 face. The surface divides space into two inter penetrating One unit cell of the P triply periodic minimal sur labyrinths. by one of the 230 space groups mentioned earlier, then the surface is periodic in three independent directions. Such triply periodic minimal surfaces (TPMSs) are of special interest because they appear in a variety of real structures such as silicates, bicontinuous mixtures, lyotropic colloids, detergent films, lipid bilayers, polymer interfaces, and biological formations (an example of a TPMS is illustrated in figure 4).
Thus, TPMSs provide a concise description of many seemingly unre-lated structures. Extensions of TPMSs may even have applications in cosmology as “branes.” complex analysis suitable for general investigation of minimal surfaces. Consider a transformation of a min - In 1866 weierstrass [VI.44](/part - 06/karl - weierstrass - 18151897) discovered a method of imal surface into the complex plane by combination oftwo simple maps.
The first is the Gauss map\nu, under which the image of a point P of the surface is the point Pof the intersection of the surface normal vector at P with the unit sphere centered at P. The second map is a stereographic projectionσ of the point P^ on the sphere into the complex plane C, resulting in the point P. The composite map, hood of any nonumbilic point on the surface to a simplyσ \nu , conformally maps the neighbor- connected region of C. (An umbilic point is one where the two principal curvatures are the same.) The inverse of this composite map is called the representation.
Enneper–Weierstrass 833 sian coordinates (surface are determined by a set of three integrals: In a system with the origin at (x, y, z) of anyx0 nontrivial minimal$, y0$, z0), the Carte - x = x0 + R(eω)ω0 (1 - τ2)R(τ)dτ, y = y0 + R(eω)ω0 i(1 + τ2)R(τ) dτ, z = z0 + R(eω)ω0 2τR(τ) dτ. Here tion of a complex variable R(τ) is the Weierstrass functionτ, and it is .
It is a func - holomorphic [I.3 §5.6](/part - 01/fundamental - definitions) in a simply connected region of C, except at isolated points. The Cartesian coordinates of any (nonumbilic) point on a minimal surface are thus expressed as the real parts of certain contour integrals, evaluated in the complex plane from some fixed pointω0 to a variable pointωthe integrands are.
Integration is carried out within the domain where holomorphic[I.3 §5.6](/part - 01/fundamental - definitions), and thus by Cauchy’s theorem the values of the integrals are inde-pendent of the path of integration from$ω^{0} to ω$. In this$way$, a specific minimal surface is completely defined byits Weierstrass function.
unknown, the coordinates of points lying on minimal surfaces involve functions of the form While the Weierstrass functions for many TPMSs are some  R(τ) = τ8 + 2\muτ6 + 1λτ4 + 2\muτ2 + 1, where A method has been developed for deriving this function\mu and . ambda are sufficient to parametrize the surface. for a given type of surface, and it generates different families of minimal surfaces from the above equation. For example, taking\mu = 0 and λ = −14 gives a surface known as the The application of minimal surfaces to the physical D surface (for “diamond”).
world has so far been descriptive, rather than quantitative. Although explicit analytical equations for the parameters of some TPMSs have recently been derived, problems such as stability and mechanical strength are unresolved. While describing structure using the concept of curvature is mathematically attractive, it hasyet to make its full impact on chemistry.

2.7 Enumeration of Crystalline Structures

It is a matter of considerable scientific and practical importance to enumerate all possible networks of atoms in a systematic way. For example, 4-connected networks (that is, networks in which each atom is con-nected to exactly four neighbors) occur in crystalline

834

elements, hydrates, covalently bonded crystals, sili-cates, and many synthetic compounds. Of particular interest is the possibility of using systematic enumeration to discover and generate new architectures. nanoporous in them that allow some substances to pass through and not others. Many are naturally occurring, such as Nanoporous materials are materials with tiny holes cell membranes and “molecular sieves” called but many others have been synthesized. There are now zeolites, 152 recognized structure types of zeolites, with several new types being added to the list every year.
Zeolites find many important applications in science and technology, in areas as diverse as catalysis, chemical sepa-ration, water softening, agriculture, refrigeration, and optoelectronics. Unfortunately, the problem of enu-meration is fraught with difficulties, and since the number of 4-connected 3 D networks is infinite and there is no systematic procedure for their derivation, the results reported so far have been obtained by empirical methods. Enumeration originated with the work of Wells (1984) on 3 D nets and polyhedra.
Many possible new structures were found by model building or computer search algorithms. New research in this field is based on recent advances in combinatorial tiling theory, devel-oped by the first generation of pure mathematicians familiar with computing.
The tiling approach identified over nine hundred networks with one, two, and three kinds of in equivalent vertices, which we call uninodal, binodal, and trinodal. However, only a fraction of the mathematically generated networks are chemically feasible (many would be “strained” frameworks requiring unrealistic bond lengths and bond angles), so for the mathematics to be useful an effective filtering process is needed toidentify the most plausible frameworks.
Methods of computational chemistry were therefore used to minimize the framework energy of the various hypotheti-cal structures, which were treated as though they were made from silicon dioxide. The unit cell parameters, framework energies and densities, volumes available to adsorption, and X-ray diffraction patterns were all calculated. A total of 887 structures were successfully optimized and ranked according to their framework energies and available volumes to give a subset of chemically feasible hypothetical structures. A number of them have since been synthesized.
structures of zeolites and other silicates, aluminophos-The results of these calculations are relevant to the

VII. The Influence of Mathematics

phates (Al POs), oxides, nitrides, chalcogenides, halides, carbon networks, and even to polyhedral bubbles in foams.

2.8 Global Optimization Algorithms

A wide variety of problems in practically all fields of physical science involve global optimization, that is, determining the global minimum (or maximum) of a function of an arbitrary number of independent variables (Wales 2004). These problems also appear in technology, design, economics, telecommunicati ons, logistics, financial planning, travel scheduling, and the design of microprocessor circuitry.
In chemistry and biology, global optimization arises in connection with the structure of clusters of atoms, protein conformation, and molecular docking (the fitting and binding of small molecules at the active sites of bio macromolecules such as enzymes and DNA). The quantity to be minimized is nearly always the energy of the system(see below). point in a very rugged landscape.
In most cases of prac-Global optimization is like trying to find the deepest tical interest it is very difficult because of the ubiquityof local minima, or holes in the landscape, the number of which tend to increase exponentially with the size of the problem. Conventional minimization tech-niques are time-consuming and have a tendency to find a nearby hole and stay there: that is, they converge to which ever local minimum they first encounter. The genetic algorithm (GA), an approach inspired by Darwin’s theory of evolution, was introduced in the 1960 s.
This algorithm starts with a set of solutions (repre-sented by “chromosomes”) called a population. Solutions from one population are taken and used to form a new population. This is done in such a way that one expects the new population to be better than the old one. Solutions that are chosen for forming new solutions (“offspring”) are selected according to their “fit-ness”: the more suitable they are the more chances they have to reproduce. This is repeated until some condition is satisfied.
(For example, one might stop after a certain number of generations or after a certain improvement of the solution has been achieved.) Simulated annealing (SA), introduced in 1983, uses an analogy between the annealing process, in whicha molten metal cools and freezes into a minimumenergy structure, and the search for a minimum in a more general system. The process can be thought ofas an adiabatic approach to the lowest-energy state.

VII.1. Mathematics and Chemistry

The algorithm employs a random search which accepts not only changes that decrease the energy, but also some changes that increase it. The energy is repre-sented by an objective functionf , and the energy- increasing changes are accepted with a probability. xp (-δf /T ), where δf is the increase in f and T is thep = system “temperature,” irrespective of the nature of the objective function. SA involves the choice of “annealing schedule,” initial temperature, the number of iterations at each temperature, and the temperature decrease at each step as cooling proceeds.
Taboo (or tabu) search is a general-purpose stochastic global-optimization method originally proposed by Glover in 1989. It is used for very large combinatorial optimization tasks and has been extended to continuous-valued functions of many variables with many local minima. Taboo search uses a modification of “local search,” which starts from some initial solution and attempts to find a better solution. This becomes the new solution and the process restarts from it. The procedure continues step by step until no improvement is found to the current solution.
The algo-rithm avoids entrapment in local minima and gives the optimal final solution. A recent method of global optimization, known as “basin hopping,” has been success-fully applied to a variety of atomic and molecular clusters, peptides, polymers, and glass-forming solids. The algorithm is based upon a transformation of the energy landscape that does not affect the relative energies of local minima. Combined with taboo search, basin hopping shows a significant improvement in efficiency over the best published results for atomic clusters.
2.9 Protein Structure Proteins are linear sequences of amino acids, which are molecules that contain both the amide (–NH ) and carboxylic (–COOH) functional groups. Understanding the means by which a protein adopts its 3 D structure is2 a key scientific challenge (Wales 2004). This problem is also critical to developing strategies, at the molecular level, to counter “protein folding diseases” such as Alzheimer’s disease and “mad cow” disease.
The strategy in tackling protein folding relies upon the fact, observed by Anfinsen, Haber, Sela, and White in 1961, that the structure of a folded protein corresponds tothe conformation which minimizes the free energy of the system. The free energy of a protein depends on the various interactions within the system, and each can be modeled mathematically using the principles of 835 (Courtesy of Dr. D. J. Wales, Cambridge University.)Figure 5 A fifty - five-atom Lennard-Jones cluster. electrostatics and physical chemistry.
As a result, the free energy of a protein can be expressed as a function of the positions of the constituent atoms. The 3 Darrangement of the protein then corresponds to the set of atomic locations providing the minimum possible value of the free energy, and the problem is reduced to finding the global minimum of the potential-energy surface of the protein. The problem is further complicated because some proteins require other molecules,“chaperones,” to enable them to reach a particular configuration.
2.10 Lennard-Jones Clusters Ament of atoms in which every pair of atoms has an Lennard-Jones cluster is a closely packed arrange associated potential energy, given by the classical ard-Jones potential-energy function. The Lennard-Jones Lenncluster problem is to determine the atomic cluster configurations with minimum potential energy (figure 5).Ifn is the number of atoms in the cluster, then one wishes to find points$p^{1}$, p2, . . . , pn so as to minimize the sum

$n^{-}1 n(rij^{-}12 - 2rij^{-}6)$, i=^1^j=^i^+^1

whereandp r, and the atoms of the cluster are positioned atij stands for the Euclidean distance between pijp optimization methods and to computer technology. A1$, p2$, . . . , pn. The problem is still a challenge, both to

836

systematic survey by North by in 1987, which yielded most of the lowest Lennard-Jones potential values in the range 13⩽ n ⩽147, was a significant landmark, and these results have since been improved by about 10%. The results for$n = 148$, 149, 150, 192,$200$, 201, 300, and 309 have now been reported using stochastic global-optimization algorithms.

2.11 Random Structures

Stereology, originally the deduction of 3 D structure from microscope examination of sections, has required the development of a substantial branch of statisti-cal mathematics, in which R. E. Miles and R. Coleman have played leading roles. Stereology concerns the esti-mation of geometrical quantities. Geometrical shapes are used to probe objects to learn about their quanti-ties, such as volume or length. Random sampling is a basic step in all stereological estimation.
The degree of randomness required for any estimate varies. Even apparently simple questions involving randomness with spatial constraints may prove difficult. For example, Gotoh and Finney gave an estimate of 0.6357 as the density expected for a dense random packing of hard spheres of equal size, and their answer to this apparently simple question has not since been improved upon, as far as we know. The problem needs to be defined very carefully, since it is far from obvious what one means by a “random packing” of spheres.
This is even more true when one investigates other, related problems concerning the interaction of molecules using computer simulation. This area, called molecular dynamics, was begun by A. Rahman, and it developed steadily from the 1960 s as computers themselves developed. An example of a problem in molecu-lar dynamics is the modeling of liquid water. This is still difficult, but the immense computing power that is now available has enabled enormous progress to be made.
3 Process In 1951 Belousov discovered there action, in which time-dependent spatial patterns Belousov–Zhabotinski appear in an apparently isotropic medium. The mech-anism of this reaction was elucidated in 1972, and this opened up an entire new research area: chemical dynamics. Oscillatory phenomena have also nonlinear been observed ingogine have shown how patterns in space and time can membrane transport. Winfree and Priappear, and some of these patterns have been fitted topractical examples.

VII. The Influence of Mathematics

Stanisław Ulam, Lindenmeyer systems, and Conway’s“game of life” and continues to this day. With his huge The development of cellular automata began with book, Wolfram (2002) has demonstrated the complex-ity that can arise from apparently simple rules, and recently Reiter has used cellular automata to simulate the growth of snowflakes, beginning to answer questions that Kepler posed in 1611. There is a group of mathematicians in Bielefeld, led by Andreas Dress, who deal with structure-forming processes;
they have made particular progress in modeling actual chemistry and thus revealing possible mechanisms. 4 Search

4.1 Chemical Informatics

A fundamental development in chemistry has been the application of computing to searching multi dimensional databases of chemical compounds and their structures. These databases are now enormous compared with their (already large) predecessors, the clas-sical Gmelin and Beilstein databases. The search process has required fundamental mathematical analyses, as exemplified in the pioneering work of Kennard and Bernal in developing the Cambridge Structural Database (www.ccdc.cam.ac.uk/products/csd/).
What is the best way to encode the structure of a 3 D molecule or a crystal arrangement as a linear sequence of symbols? One would like to be able to restore the structure efficiently from its encoding, and also to search efficiently through a big list of encoded structures. The problems that this raises are of long standing, and need insights both from mathematics and chemistry.

4.2 Inverse Problems

Many of the mathematical challenges of chemistry are inverse problems. Often they involve solving a set of linear equations. If there are as many equations asunknowns and the equations are independent, then this can be done by inverting a square matrix. How-ever, if the system is singular or redundant, or if there are fewer equations or more equations than unknowns, then the corresponding matrix is singular or rectangular and there is no ordinary inverse. Nevertheless, itis possible to define a generalized inverse, which gives a good model for linear problems.
(It is the so-called Moore–Penrose inverse or pseudo-inverse involved in singular value decomposition.) This always exists and it

VII.2. Mathematical Biology

uses all available information; it is related to the prob-lem of reconstructing a 3 D structure from a 2 D projection. The operation has been fully described and is now available in Mathematica. The generalized inverse also enables one to handle redundant axes in quasicrystals, but usually the inter-esting problems are nonlinear. Other inverse problems include the following. (i) Finding the arrangement of atoms that gives riseto the observed scattering patterns of X-rays or (ii) Reconstructing a 3 D image from 2 D projections inelectrons from a crystal.microscopy or X-ray tomography.
(iii) Reconstructing the geometry of a molecule given probable interatomic distances (and perhaps bond (iv) Finding the way in which a protein molecule folds angles and torsion angles).to give an active site, given the sequence of con(v) Finding the pathway to producing a molecule syn-stituent amino acids.thet ically, given that it occurs in nature. (vi) Finding the sequence of rules that generate a mem-brane or a plant or another biological object, given that it takes a certain shape. swers.
For example, the classic question as to whether the shape of a drumhead can be determined from its Some questions of this type do not have unique anvibration spectrum (can you hear the shape of a drum?) has been answered in the negative: two vibrating mem-branes with different shapes may have the same spectrum. It was thought that this ambiguity might also bethe case for crystal structures.
Linus Pauling suggested that there might be two different crystal structures that were pattern), but no definite example has been found.homometric (that is, giving the same diffraction 5 Conclusion As the examples in this article show, mathematics and chemistry have a symbiotic relationship, with developments in one often stimulating advances in the other. Many interesting problems, including several that we have mentioned here, are still waiting to be solved.

Further Reading

Cotton, F. A. 1990.New York: Wiley Interscience. Chemical Applications of Group Theory. Hollas, J. M. 2003.Wiley. Modern Spectroscopy. New York: John

837

Hyde, S., S. Andersson, K. Larsson, Z. Blum, T. Landh, S.Lidin, and B. W. Ninham. 1997. The Language of Shape. The Role of Curvature in Condensed Matter: Physics, Chemistry and Biology. Amsterdam: Elsevier. Parr, R. G., and W. Yang. 1989.Atoms and Molecules. Oxford: Oxford University Press. Density-Functional Theory of Thomas, J. M. 2003. Poetic suggestion in chemical science. Nova Acta Leopoldina NF 88:109–39. Wales, D. J. 2004.bridge University Press. Energy Landscapes. Cambridge: Cam Wells, A. F. 1984.Oxford University Press. Structural Inorganic Chemistry. Oxford: Wolfram, S.
2002.Wolfram Media. A New Kind of Science. Champaign, IL: VII.2 Mathematical Biology Michael C. Reed 1 Introduction Mathematical biology is an extremely large and diverse field. It studies objects ranging from molecules to global ecosystems and the mathematical methods come from many of the subdisciplines of the mathematical sciences: ordinary and partial differential equations, probability theory, numerical analysis, control theory, graph theory, combinatorics, geometry, computer science, and statistics.
The most that one short article cando is to illustrate by selected examples this diversity and the range of new mathematical questions that arise naturally in the biological sciences. 2 How Do Cells Work? From the simplest point of view, cells are large bio-chemical factories that take inputs and manufacture lots of intermediate products and outputs. For example, when a cell divides, its DNA must be copied and that requires the biochemical synthesis of large numbers of adenine, cytosine, guanine, and thymine molecules.
Biochemical reactions are usually catalyzed by enzymes, proteins that facilitate a reaction but arenot used up by it. Consider, for example, a reaction in which chemical A is converted to chemical B with the help of an enzyme E. Ifa(t) and b(t) are the respec- tive concentrations of A and B at time cally writes down a differential equation fort, then one typi-b(t), which takes the form $b (t) = f (a$, b, E) + · · · − · · · . Here, depends onf is the rate of production, which typicallya, b, and E. Of course B may be produced

838

by other reactions (which would lead to additional pos-itive terms+ · · · ) and may be used as a substrate itself in still other reactions (which would lead to additional negative terms− · · · ). So, given a particular cell func- tion or biochemical pathway, we can just write down the appropriate set of nonlinear coupled ordinary differential equations for the chemical concentrations and solve it by hand or by machine computation.
However, this straightforward approach is often unsuccessful. First of all, there are a lot of parameters (and variables) in these equations and measuring them in the context of real living cells is difficult. Second, different cells behave differently and may have different functions, so we would expect the parameters to be different. Third, cells are alive and change what they are doing, so the parameters may themselves be functions of time. Butthe greatest difficulty is that the particular pathway under study is not really isolated. Rather, it is embedded in a much larger system.
How do we know that our model system will continue to behave in the same way when embedded in this larger context? We need new theorems in dynamical systems that answer questions such as this, not for general “complex systems” but forthe particular kinds of complex systems that arise in important biological problems. Cells continue to accomplish many basic tasks even though their environments (i.e., their inputs) are con-stantly changing.
A brief example of this phenomenon, which is known aslem of “context.” Let us suppose that the chemical reac-homeostasis, will illustrate the prob tion above is one step in the pathway for making the thymines necessary for cell division. If the cell is a cancer cell, we would like to turn off this pathway, and a reasonable way to try to do this would be to put into the cell a compound X that binds to E, there by reducing the amount of free enzyme available to make the reaction run. Two homeostatic mechanisms immediately come into play. First, a typical reaction is inhibited by its product:
that is, f decreases as b increases. This makes biological sense because it ensures that B is not over-produced. So, when the amount of free E is reduced and the rate the rate up again. Second, if the ratef declines, the resulting decrease inf is lower thanb drives usual, the concentration a typically rises since A is not being used up as quickly, which also drives the ratef up again since f increases as a increases. Given the network in which A and B are embedded, one can imag-ine calculating how muchf will drop if we put a cer- tain amount of X into the cell.
In fact, f may drop even less than we calculate because of another homeostatic

VII. The Influence of Mathematics

mechanism that is not even in our network. The enzyme E is a protein produced by the cell via instructions from a gene. It turns out that some times the concentration of free E inhibits the messenger RNA that codes forthe production of E itself. Then, if we introduce X and reduce free E, the inhibition is removed and the cell automatically increases its rate of production of E, thus raising the amount of free E and with it raising there action ratef . ing cell biochemistry, indeed a difficulty in studying many biological systems.
These systems are very large This illustrates a fundamental difficulty in study and very complex. To gain understanding, it is natural to concentrate on particular relatively simple subsystems. But one always has to be aware that the subsys-tems exist in a larger context that may contain variables (excluded by the simplification) that are crucial for understanding the behavior and biological function of the subsystem itself. also under go spectacular changes.
For example, cell division requires unzipping of the DNA, synthesis of Although cells exhibit remarkable homeostasis, they two new complementary strands, the movement apartof the two new DNAs, and the pinching off of the mother cell to produce two daughters. How does a cell do all this? In the case of yeast cells, which are compar-ative ly simple, the actions of the biochemical pathways are quite well understood, partly because of the mathe-mat ical work of John Tyson. But as our brief discussion makes clear, biochemistry is not all there is to cell divi-sion;
an important additional feature is motion. Materials are being transported all the time through out cells from one specific place to another (so their motion isnot just diffusion), and indeed, cells themselves move. How does this happen? The answer is that materials are transported by special molecules called molecular motors that turn the energy of chemical bonds into mechanical force.
Since bonds are formed and broken stochastically (that is, some randomness is involved), the study of molecular motors leads naturally to new questions in stochastic ordinary and partial differential equations the mathematics of cell biology is Fall et al. (2002).[IV.24](/part-04/stochastic-processes). A good introduction to 3 Genomics To understand the mathematics that was involved in sequencing the human genome it is useful to start with the following simple question. Suppose that we cut up aline segment into smaller segments and are presented

VII.2. Mathematical Biology

with the pieces. If we are told the order in which the pieces came in the original segment, then we can put them back together and reconstruct the segment. In general, since there are many possible orders, we can-not reconstruct the segment with out extra information of this kind. Now suppose that we have cut upthe segment in two different ways. Think of the line segment as an interval I of real numbers, and let the pieces beway, and B(A1)1, B, (A2)2, . . . , B, . . . , (As)rwhen you cut it up the other when you cut it up the first way.
That is, the sets A form a partition of the inter-i val I into subintervals, and the sets B form a not herj

partition. For simplicity, assume that no A shares ani

endpoint with any I itself. Bj, except for the two endpoints of which the pieces that all we know about them is which Suppose that we know nothing about the order in Ai and Bj come in I. In fact, suppose A overlap withi

which nonempty. Can we use this information to work out the Bj: that is, which of the intersections Ai ∩ Bj are original order of the pieces A and there by reconstruct i

the interval times be yes and some times no. If it is yes, then we$I$(or its reflection)? The answer will some would like to find an efficient algorithm for doing there construction, and if it is no, then we would like to know how many different reconstructions are consis-tent with the given information. This so-called restriction mapping problem is really a problem in graph theory to the sets[III.34]: the vertices of the graph correspond Ai or Bj, and there is an edge between Ai and A second problem is whether we can find the original Bj if Ai ∩ Bj = ∅.
order of the Ai (or the Bj) if what we are told is the length of each setall the lengths of the intersections Ai and each set AB∩j, and the set of B . The catch is that we are not told which length corresponds to which intersection. This is called the double digest problem$ij$. Again one would like to be able to tell when there is only one solution, or to place an upper bound on the number of possible reconstructions if there is more than one. length approximately 3 bet A, G, C, T.
That is, it is a sequence of length 3 Human DNA is, for our purposes here, a word of. imes  109 over a four-letter alpha-. imes  109 in which each entry is A, G, C, or T. In the cell, the word is bound letter by letter to the “complementary” word, which is determined by the rule that A can only be bound to T, and C can only be bound to G. (For example, if the word is ATTGATCCTG, then the complementary word is TAACTAGGAC.) In this brief discussion we will ignore the complementary word.

839

meters if one stretched it out into a straight line) it isvery hard to handle experimentally, but the sequence Since DNA is so long (it would be approximately two of letters in short segments of approximately five hun-dred letters can be determined by a process called gel chromatography. There are enzymes that cut Dna wherever specific very short sequences occur.
So if we digest a DNA molecule with one of these enzymes and digest another copy with a different enzyme, wecan hope to determine which fragments from the first digestion overlap fragments from the second digestion and then use techniques from the restriction mapping problem to reconstruct the original DNA molecule. The interval sets Ai to the fragments. This involves sequencing and I corresponds to the whole DNA word, and the comparing the fragments, which has its own diff i cul-ties.
However, lengths of fragments are not so hard to determine, so another possibility is to digest with the first enzyme and measure lengths, digest with the second and measure lengths, and finally digest with both and measure lengths. If one does this, then the problem one obtains is essentially the double digest problem. many copies of the word, digests with enzymes, and selects at random enough fragments that together they To completely reconstruct the DNA word one takes have a high probability of covering the word.
Eachof the fragments is cloned, in order to get enough mass, and then sequenced by gel chromatography. both processes can introduce errors, so one is left with a very large number of sequenced fragments with known error rates for the letters. These need to be compared to see if they overlap: that is, to see if the sequence near the end of one fragment is the same as (or very similar to) the sequence at the beginning of another. This alignment problem is itself difficult because of the large number of possibilities involved.
So, in the end we have a very large restriction mapping problem except that we can only say that given fragments overlap with probabilities that are themselves hard to estimate. A further difficulty is that DNA tends to have large blocks that repeat in different parts of the word. As a result of these complications, the problem is much harder than the restriction mapping problem described earlier. Itis clear that graph theory, combinatorics, probability theory, statistics, and the design of algorithms all play central roles in sequencing a genome.
Sequence alignment is important in other problems as well. In phylogenetics (see below) one would likea way of saying how similar two genes or genomes

840

are. When studying proteins, one can some times pre-dict protein three-dimensional structure by searching databases for known proteins with the most similar amino acid sequence. To illustrate how complex these problems are, consider a sequence thousand letters from our four-letter alphabet. We wish${ai}1000i = 1 \text{of one}$ to say how similar it is to another sequence Naively, one could just compare a with band define${bi}1000i = 1$. a metric [III.56](/part-03/metric-spaces) liked(ai, {bi})i= δ((ai)i, bi).
How- ever, DNA sequences have evolved typically by inser-tions and deletions as well as by substitutions. Thus if the sequence Acacac aacac· · · , the two sequences would be very far apart· · ·lost its first C to become in this metric even though they are very similar and related in a simple way. The way around this difficulty is to allow sequences to include a fifth symbol, –, which stands for the place of a deletion or a place opposite an insertion.
Thus, given two sequences (of perhaps dif-ferent lengths), we wish to find how they can be augmented with dashes to give the minimum possible distance between them. A little thought will convince the reader that it is not feasible to use a brute-force search for a problem like this, even for the fastest computers—there are so many potential augmentations that the search would take far too long. Serious and thought-ful algorithm development is required. Two excellent introductions to the material discussed in this section are Waterman (1995) and Pevzner (2000).
4 Correlation and Causality The central dogma of molecular biology is DNARNA\to  proteins. That is, information is stored in DNA,\to it is transferred out of the nucleus by RNA, and the RNAis then used in the cell to make proteins that carry out the work of the cell through the metabolic processes discussed in section 2. Thus DNA directs the life of the cell. Like most things in biology, the true situation is much more complicated. Genes, which are segments of DNA that code for the manufacture of particular proteins, are some times turned on and some times turned off.
Usually, they are partially turned on; that is, the protein they code for is manufactured at some intermediate rate. This rate is controlled by the binding (orlack of binding) of small molecules or specific proteins to the gene, or to the RNA that the gene codes for. Thus genes can produce proteins that inhibit (or excite) other genes; this called a gene network. In a way, this was obvious all along. If cells can respond to their environments by changing what they

VII. The Influence of Mathematics

do, they must be able to sense the environment and signal the DNA to change the protein content of the cell. Thus, while sequencing DNA and understanding specific biochemical reactions are important first steps in understanding cells, the hard and interesting workto come is to understand networks of genes and biochemical reactions. It is these networks, in which pro-teins control genes and genes control proteins, that carry out and control specific cellular functions.
The mathematics will be ordinary differential equations for chemical concentrations and variables that indicate to what extent a gene is turned on. Since transport into and out of the nucleus occurs, partial differential equations will be involved. And, finally, since some of the molecular species occur in very small numbers, con-cent ration (molecules per unit volume) may not be a useful approximation for computations about chem-ical binding and dissociation: they are probabilistic events. the components of these gene networks.
First, there are large numbers of population studies that corre-Two kinds of statistical data can give hints about late specific genotypes to specific phenotypes (such as height, enzyme concentration, cancer incidence). Second, tools known asthe relative amounts of a large number of different mes-microarrays allow us to measure senger RNAs in a group of cells. The amount of Rna tells us how much a particular gene is turned on. Thus, microarrays allow us to find correlations that may indicate that certain genes are turned on at the same timeor perhaps in a sequence.
Of course, correlation is not causality and a consistent sequential relationship isnot necessarily causal either (sure, football causes winter, a sociologist once said). Real biological progress requires understanding the gene networks discussed above; they are the mechanisms by which the genotypes play out in the life of the cell. A nice discussion of the relationship between population correlations and mechanisms occurs in Nijhout (2002), from which we take the following simple exam - ple. Most phenotypic traits depend on many genes;
suppose that we consider a trait that depends on only two genes. Figure 1 depicts a surface that shows how the trait in an individual depends on how much each of the genes is turned on. All three variables are scaled from 0 to 1. Suppose that we study a population whose members have a genetic makeup that puts the individ-uals near the point X on the graph. If we do a statistical analysis of the population, we will find that gene B is highly statistically correlated to the trait, but gene A is VII.2.
Mathematical Biology 1.0 0.8 0.6 rait T 0.4 Y 0.2 X 0.81.0 0.6 1.0 0.8 0.6 0.20.4 Gene B Gene A 0.4 0.2 Figure 1 A phenotypic surface. not. On the other hand, if the individuals in the population all live near the point Y on the surface, wewill discover in our population study that gene A is highly statistically correlated to the trait, but gene B is not. More detailed examples with specific biochemical mechanisms are discussed in Nijhout’s paper. Similar examples can be given for microarray data. This does not mean that population studies or microarray data are unimportant.
Indeed, in studying hugely complex biological systems, statistical information can suggest where to look for the mechanisms that will ultimately give biological understanding. 5 The Geometry and Topologyof Macromolecules To illustrate the natural geometric and topological questions that arise when one studies macromolecules, we will briefly discuss molecular dynamics, protein–protein interactions, and the coiling of DNA. Genes code for the manufacture of proteins, which are large molecules made up of sequences of amino acids.
There are twenty amino acids, each coded by a triplet of base pairs, and a typical protein might have five hundred amino acids. Interactions among the amino acids cause the protein to fold up into a complicated three dimensional shape. This three-dimensional structure is crucial for the function of the protein since the exposed groups and the nooks and crannies in the shape govern the possible chemical interactions with small molecules and other proteins.
Three-dimensional structures of 841 proteins can be approximately determined by X - ray crystallography and nontrivial inverse scattering calculations. The forward problem—namely, given the sequence of amino acids, predict the three-dimensional structure of the protein—is important not only for understanding existing proteins, but also for the pharmacological design of new proteins to accomplish spe-cific tasks.
Thus, in the past twenty years a large field called uses classical mechanical methods.molecular dynamics has arisen, in which one Let Suppose we have a protein that consists of xde note the position (specified by three real N atoms.i

coordinates) of thetor formed from all these coordinates (which belongs ith atom, and let x denote the vec- to R3 N). For each pair of atoms, one attempts to write down a good approximation to the potential energy, Ebe the electrostatic interaction, for example, or thei, j(xi, xj), due to their pairwise interaction. This could van der Waals interaction, which is a classical mechani-cal formulation of quantum effects. The total potential energy isof motion take the form$E(x) ≡ E^{i}$, j(xi, xj)and Newton’s equations

$v$ ̇$= −∇E(x)$, x ̇$= v$,

where initial conditions one can try to solve these equationsv is the vector of velocities. Starting with some to follow the dynamics of the molecule. Note that this is a very high-dimensional problem. A typical amino acid has twenty atoms, so that is sixty coordinates right there, and if we are looking at a protein made up of five hundred amino acids, then thirty thousand coordinates. Alternatively, one couldx will be a vector with assume that the protein will fold to the configuration that has the minimum potential energy.
Finding this configuration would mean finding the roots of$\nabla E(x)$, byto see which root gives the lowest energy. Again this is newton’s method [II.4 §2.3](/part-02/algorithms) say, and then checking an enormous computational task. tions have been only moderately successful and have It is not surprising that molecular dynamics calcul a predicted the shapes of only relatively small molecules and proteins. The numerical problems are substantial and the choice of energy terms is some what specu-lative. Even more importantly, context matters, as it does in many biological problems.
The way proteins fold depends on properties of the solution in which they sit. Many proteins have several preferred configurations and switch from one to the other dependingon interactions with small molecules or other proteins. Finally, it has recently been discovered that proteins do

842

not fold up by themselves from their linear configura-tion to their three-dimensional shape, but are helped and guided by other proteins called chaperones. It is natural to ask whether there are quantifiable geometri-cal units larger than points (atoms) that could reasonably form the basis for a good approximation to the dynamics of large molecules. A start has been made in this direction by research groups studying the interactions of proteins with small molecules and other proteins.
These interactions are fundamental to cell biochemistry, cell-transport pro-cesses, and cell signaling, and so progress is vital to understanding how cells work. Suppose one has two large proteins that are bound to each other. The first thing one would like to do is describe the geometry of the binding region. One could do this as follows. Con-sider an atom in either protein that is at pointx. Given another atom at point R3 into two open half-spaces: the points closer toy, there is a plane that divide sx and the points closer to section of all such open half-spaces asy .
Now let Rx denote the inter-y ranges over the positions of all other atoms: that is, of those points that are closer tox than to any other Rx consists atom. The union of the boundaries, a Voronoi surface, consists of triangles and pieces of$"^{x} \partial(R^{x})$, called planes and has the property that each point on the sur-face is equidistant from at least two atom positions.
To model the binding region between the two proteins, we discard all pieces of the Voronoi surface that are equidistant from two atoms that belong to the same protein and keep just the ones that are equidistant from two atoms that are in different proteins. This surface goes off to infinity, so we clip off the parts that are not “close” to either protein. The result is a sur-face with a boundary made up of polyhedral faces that is a reasonable approximation of the interaction inter-face between the two proteins. (This is not quite an accurate description:
in the actual construction, “distance” is weighted in a way that depends on the atoms involved.) Now choose colors representing the twenty amino acids and color each side of each polyhedral piece with the color of the amino acid that the closest atom is in. This divides each side of the surface into large colored patches corresponding to nearnessof a particular amino acid on that side.
The coloring of the two sides of the boundary surface will be differ-ent, of course, and the placement of the patches gives information about which amino acids in one protein are interacting with which amino acids in the other. In particular, one amino acid in one protein may interact with

VII. The Influence of Mathematics

several in the other. This gives a way of using geometryto classify the nature of the particular protein–protein interaction. Finally, let us touch on questions involving the packaging of DNA. The basic problem is easy to see. Asmentioned earlier, the human DNA double helix when stretched out linearly is about two meters long. A typ-ical cell has a diameter of about one-hundredth of a millimeter and its nucleus has a diameter of about one third that size. All of that DNA has to be packed into the nucleus. How is this done?
DNA double helix is wound around proteins called tones At least the first stages are well understood. The, which consist of about two hundred base pairs his each, yielding chromatin, which is a sequence of such DNA-wrapped histones connected by short segments of DNA. Then the chromatin is itself wrapped up and compacted; the geometrical details are not completely understood. It is important to understand the packing and the mechanisms that create it, because the life ofthe cell requires unpacking!
When the cell divides, the entire DNA helix must be unzipped to form two sepa-rate strands, which are the templates on which the two new copies of DNA will be built. Clearly this cannot be done all at once but must involve local unwinding ofthe DNA off the histones, local unzipping, synthesis, and then local repacking. It is equally challenging to understand the sequence of events that occurs when a protein is synthesized from a gene. Transcription factors diffuse into the nucleus and bind to specific short segments of DNA (of about ten base pairs) in the regulatory region ofthe gene.
Of course, they will randomly bind wherever they see the same segment. Typically, one needs the binding of several different transcription factors in the regulatory region along with RNA polymerase to start transcription of a gene. That process involves the unwinding of the gene-coding region from the histones so that it can be transcribed, the transport of the resulting RNA out of the nucleus, and the re compact-ification of the DNA.
To understand these processes fully, one will have to solve problems in partial differ-ential equations, geometry, combinatorics, probability theory, and topology.
De Witt Sumners is the mathematician who brought the topological problems in the study of DNA (links, twists, knots, supercoiling) to the attention of the mathematics community. A good ref-erence for molecular dynamics and the general mathematical issues posed by biological macromolecules is Schlick (2002).

VII.2. Mathematical Biology

6 Physiology

When one first studies human physiological systems, they seem almost miraculous. They accomplish enormous numbers of tasks simultaneously. They are ro-bust but capable of quick changes if the situation warrants. They are made up of large numbers of cells that actively cooperate so that the tasks of the whole can bedone. It is the nature of many of these systems that they are complex, controlled by feedback, and integrated with each other. It is the job of mathematical physiology to understand how they work.
We will illustrate some of these points by discussing problems in biological fluid dynamics. The heart pumps blood through out a circulatory system that consists of vessels of diameter as large as2.5 cm (the aorta) and as small as 6. imes 10 - 4 cm (the cap- illaries). Not only are the vessels flexible, but many are surrounded by muscle and can contract to exert local force on the blood. The main force-generating mech-anism (the heart!) is approximately periodic, but the period can change. The blood itself is a very complicated fluid. About 40% of its volume is made up of cells:
red blood cells carry most of the oxygen and CO2; white blood cells are immune system cells that hunt bacte-ria; and platelets are part of the blood clotting process. Some of these cells have diameters that are larger than the smallest capillaries, which raises the nice question of how they get through. You notice that we are very far away from most of the simplifying assumptions ofclassical fluid dynamics. In a significant number of people, the mitral valve, which is the inflow valve to the left side of the heart, Here is an example of a circulatory-system question. becomes defective.
It is common to replace the valve by an artificial one and this leads to an important question: how should one design the artificial valve so that the resulting flow in the left heart chamber has as few stagnant points as possible, since clots tend to form at these points? Charles Peskin did the pioneering work on this problem. Here is another question. The white blood cells are not carried in the middle of the fluid but tend to roll along the walls. Why do they do that?
It is a good thing that they do, because their job is to sniff out inflammation out side the blood vessel and, when they find it, to stop and burrow through the blood ves-sel wall to get to the inflamed site. Another circulatory fluid dynamics question is discussed in section 10. systems. The heart has its own pacemaker cells, but its The circulatory system is connected to many other

843

frequency of contraction is regulated by the autonomic nervous system. Through the baroreceptor reflex, the sympathetic nervous system tightens blood vessels to avoid a dramatic drop in blood pressure when we stand. Over all average blood pressure is maintained bya complicated regulatory feedback mechanism involving the kidneys.
It is worthwhile remembering that all these things are being accomplished by living tissues whose parts are always decaying and being replaced. For example, the gap junctions that transmit current at very low resistance between heart muscle cells have a half-life of approximately one day. As a final example, we consider the lung, which has a fractal branching structure that terminates after twenty-three levels in about 600 million air sacs called alveoli the circulating blood.
The Reynolds number of the air, in which oxygen and CO2 are exchanged with flow varies by about three orders of magnitude between the large vessels near the throat and the tiny vessels near the alveoli. Premature infants often have res pir a-tory difficulty because they lack surfactants that reduce surface tension on the inner surfaces of the alveoli. The high surface tension makes the alveoli collapse, which makes breathing difficult. One would like the infants to breathe in air that includes tiny aerosol drops of surfactant.
How small should the drops be so that as much surfactant as possible makes it to the alveoli? ordinary and partial differential equations. However, The mathematics of physiology consists mostly of there is a new feature: many of these equations have time delays. For example, the rate of respiration is controlled by a brain center that senses the COof blood. It takes almost fifteen seconds for blood to2 content go from the lungs to the left heart and from there to the brain center.
This time delay is even longer in patients with weak hearts and often these patients display Cheyne–Stokes breathing: very rapid breathing alternates with periods of little or no breathing. such oscillations in control systems are well-known as the time delay gets longer. Since partial differential equa-tions are often involved, new mathematical results are needed that go well beyond the standard theory of ordi-nary differential equations with delay, which was initiated by Bellman in the 1950 s.
An excellent reference for the applications of mathematics to physiology is Keener and Sneyd (1998). 7 What’s Wrong with Neurobiology? The short answer is that there is not enough theory. This may seem an odd thing to say, since neurobiology

844

is the home of the Hodgkin–Huxley equations, which are often cited as a triumph of mathematics in biology. In a series of papers in the early 1950 s, Hodgkin and Huxley described several experiments, and gave a theoretical basis for explaining them.
Building on the work of physicists and chemists (for example, Wal-ter Nernst, Max Planck, and Kenneth Cole), they discovered the relationship between certain ionic conductances and the trans-membrane electrical potential, v(x, t), in the axons of neurons, and they formulated a mathematical model:. artial v . artial2 v. artial y. artial t. artial ti == αfi. artial x(v, y2 + i), g(v$, yi = 11, y,22$,$, y3.3)$, Here they are related to the membrane conduc-i

tances of various ions. The equations have solutions that are pulses that keep their shape and travel at constant velocity in a way that corresponds to the observed behavior of action potentials in real neurons. The ideas, both explicit and implicit, in these discover-ies form the basis of much single-neuron physiology. Of course, mathematicians should not be too proud about this since Hodgkin and Huxley were biologists. The Hodgkin–Huxley equations were part of the stimulus for interesting work by mathematicians on travel-ing waves and pattern formation in reaction–diffusion equations.
of just one neuron. Watch your hand as it reaches out gracefully to pick up an object. Think about the so-However, not everything can be explained at the level called ocular–vestibular reflex in which motions of the head are automatically compensated for by motions of the eyes so that your gaze can remain fixed. Con-sider the fact that you are looking at stereotypical black marks on a page and they mean something inside your head. These are system properties, and the systems are large indeed.
There are approximately 1011 neurons in the central nervous system and on average each makes about one thousand connections to other neurons. These systems will not be understood justby examining their parts (the neurons) and, for obvious reasons, experimentation is limited. Thus, experimental neurobiology, like experimental physics, needs input from deep and imaginative theorists. robustly with experimental ists is to some extent ahistorical accident.
Grossberg asked how groups of The lack of a large theory community interacting (quite simple) model neurons, if they were connectedin the right ways, could accomplish various tasks such

VII. The Influence of Mathematics

as pattern recognition and decision making, or could exhibit certain “psychological” properties (Grossberg 1982). He also asked how these networks could be trained. At about the same time it was shown that networks of neuron-like elements connected in the right way could automatically compute good solutions of large, difficult problems like the problem [VII.5 §2](/part-07/the-mathematics-of-algorithm-design).
These and other factors, including traveling salesman the great interest in software engineering and artificial intelligence, led to the emergence of a large communityof researchers studying “neural networks.” The members of this community were mostly computer scien-tists and physicists, so it was natural for them to concentrate on the design of devices, rather than biology. This was noticed, of course, by experimental neurobi-ologists, who lost interest in collaborating with these theorists. This brief history is of course an over simplification.
There are mathematicians (and physicists and computer scientists) who are essentially theoreticians for neuroscience. Some of them work on hypothetical networks, typically either very small networks or networks with strong homogeneity properties, to discover what are the emergent behaviors of the systems. Others work on modeling real physiological neural networks, often collaboratively with biologists. Usually, the models consist of ordinary differential equations for the firing rates of the individual neurons or mean-field models that involve integral equations.
These mathematicians have made real contributions to neurobiology. But much more is needed, and to see why, it is useful to think about just how difficult these problems really are. First, there is no one-to-one correspondence between the cells of the central nervous system in different members of the same species (except in spe-cial cases like C. elegans). Second, neurons in the same animal differ widely in their anatomy and physiology. Third, the details of a particular network may well depend on the life history of the animal.
Fourth, most neurons are some what unreliable devices in that they give different outputs under repeated trials with the same input. Finally, one of the prime characteristics of neural systems is that they are plastic, adaptable, and ever changing. After all, if you remember anything of what is written here, then your head is different from when you began. Between the level of the single neuron and the psychological level, there are probably twenty levels of networks, each network feeding into and being controlled by networks at other levels.
The mathematical objects that will enable us to classify, analyze, and

VII.2. Mathematical Biology

understand how this all works have probably not yet been discovered. 8 Population Biology and Ecology Let us begin with a simple example. Imagine a large orchard of equally spaced trees and suppose that one tree has a disease. The disease can be transmitted only to nearest neighbors, and is transmitted with prob-abilityp. What is E(p), the expected percentage of trees that will be infected? Intuitively, ifp is small, E(p)be close to 100%.
In fact, one can prove that should be small, and ifp is large, E(p) should E(p) changes very rapidly from being small to being largeasp passes through a small transition region around a particular critical probabilityp . One would expect p c to decrease as the distance, farmers should choosed in such a way thatd, between trees increases; p is less than the critical probability, in order to make E(p) small. We see here a typical issue in ecological prob-lems: how does behavior on the large scale (tree epidemic or not) depend on behavior at the small scale(the distance between trees).
And, of course, the example illustrates that understanding the biological situa-tion requires mathematics. For other examples of sharp global changes in probabilistic models, see probabilistic models of critical phenomena Suppose that we now widen our gaze to consider[IV.25]. forests—let us say the forests on the East coast of the United States. We would like to understand how they have come to be as they are. Most of them were not planted in neat rows, so that is already a complication. But there are two other really new features.
First, there is not one species but many, and each species of tree has different properties: shape, seed dispersal, need for light, and so forth. The species are different, but their properties affect each other because they are living in the same space. Second, the species, and the interactions between the species, are affected by the physics of the environment. There are physical param-eters that vary on long timescales, like average temperature, and there are other parameters that vary on very short timescales, like wind speed (for seed dispersal).
Certain properties of forests may depend on the fluc-tuations in these parameters as much as on the values themselves. Finally, one might have to take into account the reaction of the ecosystem to catastrophic events such as hurricanes or prolonged drought. other problems in mathematical biology. One would The difficulties are similar to those we have seen for

845

like to understand the emergent behavior on the largescale. To do this one creates mathematical models that relate the behavior on the small scale to the large scale. However, on the small scale one is overwhelmed by the biological details. Which of these details should be in the model? Of course, there is no simple answer to this because, in fact, this is the heart of what we want to know. Which of the bewildering variety of local properties or variables give rise to the large-scale behavior andby what mechanisms? Further more, it is not obvious what kinds of model are best.
Should we model each individual and its interactions, or should we use population densities? Should we use deterministic models or stochastic models? These are also hard questions, andthe answers depend on the system being studied and the questions being asked. A nice discussion of these different modeling choices can be found in Durrett and Levin (1994). SIRS model A crucial parameter is the Let us focus again on a simple model:
the so-called for the spread of a disease in a population.infectious contact number, σtions that an infected individual creates in the suscep-, which represents the average number of new infec- tible population. For a serious disease one would liketo bring the value ofσ down to below 1 (so that an epi- demic will be unlikely) by vaccination, which takes indi-vi duals from the susceptible category and puts them in the removed category.
Since vaccination is expensive and it is difficult to vaccinate high percentages of the population, it is an important public-health problem to know how much vaccination is needed to bring below 1. A little reflection shows us how difficult thisσ to problem really is. First of all, the population is not well mixed, so one may not be able to ignore spatial separation, as is done in the SIRS model. Even more important,σ depends on the social behavior of individuals and the subclasses of the population to which they belong (as any one with small children in school will attest).
Thus, we see a genuinely new issue here: if an ecological problem involves animals, then the social behavior of the animals may affect the biology. groups, or species, or subpopulations, vary and it isjust this variation on which natural selection acts. So, In fact, the issues are even deeper. Individuals in to understand how an ecosystem got to where it is today, one may have to take this individual variability into account. Social behavior is also transmitted from generation to generation, both biologically and culturally, and therefore also evolves.
For instance, there are many examples of plant and animal species

846

in which the biology of the plants and the sociology ofthe animals clearly coevolved, to the benefit of both. Game-theory models have been used to study the evo-lution of certain human behaviors such as altruism. Therefore, ecological problems, which some times seem simple at first, are often very deep, because the biology and its evolution are connected in complicated ways to both the physics of the environment and the social behavior of the animals. A good introductory review of these questions can be found in Levin et al. (1997).
9 Phylogenetics and Graph Theory Since Darwin, a deep on going problem in biology has been to determine the history of the evolution of species that has brought us to our current state. It is natural when thinking about such questions to draw directed species (past or present) and an edge from species graphs [III.34] in which the vertices, V, are\nu1 to species\nu . Indeed, Darwin himself wrote down such graphs.\nu2 indicates that \nu2 evolved directly from To explain the mathematical issues, we will consider a simple special case. A connected graph with no cycles is1 called a tree.
If we distinguish a particular vertex,ρ, and call it the ti ces of the tree that have degree one (i.e., have only one root, then the tree is called rooted. The ver attached edge) are called is not a leaf. Notice that, because there are no cycles, leaves. We will assume thatρ there is exactly one path in the tree from tex\nu. We say that \nu ⩽ \nu if the path fromρ to each ver-ρ to \nu contains\nu1(see figure 2).
The problem is to determine1 2 2 which trees with a given set of leaves and a given root vertexρ (a hypothesized ancestral X (current species) species) are consistent with experimental information and theoretical assumptions about the mechanisms of evolution. Such a tree is called a X-tree. One can always add extra intermediate species, rooted phylogenetic so typically one imposes the additional restriction that the phylogenetic trees be as simple as possible. Suppose that we are interested in a certain characteristic, the number of teeth, for example.
We can use it to define a functionf from X, the set of current species, to the nonnegative integers: given a spec i esf (x) be the number of teeth of members ofx inx X. In gen-, we let eral, asible values of a particular characteristic (having or not character is a function from X to a set C of pos- having a gene, the number of vertebrae, the presenceor absence of a particular enzyme, etc.). It is characters such as these that are measured by biologists in current

VII. The Influence of Mathematics

root

●

ν

● ●

● ● ● ● ●

leaves

● ● ●ν● ● ●

Figure 2 A rooted tree.

species. In order to say something about evolutionary history, one would like to extend the definition off from X to the larger set V of all the vertices in a phy- lo genetic tree. To do this, one specifies some rules forhow characters can change as species evolve. A character is called$f$ ̄ from V to convex C in such a way that for every if f can be extended to a functionc \in  C, the subse. ar{t}f-1(c) of V is a connected subgraph of the tree.
That is, between any two species ter valuec there should be a path back in evolution ar yx and y with charac- history from species in between have the same valuex and forward again to y such that all thec. This essen- tially forbids new values from arising and then reverting back and forbids two values evolving separately (indifferent parts of the tree). Of course, we have the current species and lots of characters. What is unknown is the phylogenetic tree, that is, the collection of inter-media te species and the relations between them that link the current species to a common ancestor.
A collection of characters is called a phylogenetic tree on which they are all convex. Deter-compatible if there exists mining when this is the case and finding an algorithm for constructing such a tree (or a minimal such tree)is called the perfect phylogeny problem. This problem is understood for collections of characters with binary values, but not in general. have been treating all the edges alike when in fact some may represent longer or shorter evolutionary steps. An alternative problem is the following. Note that we Suppose that we have a function tive number to each edge.
Then, since there is a uni quew that assigns a posi- shortest path between any two vertices in the tree, induces a distance functiond on V . imes V , and in partic-w ular onfunction Xδ. Now, suppose that we are given a distanceon X . imes  X that tells us how far apart cur-w rent species are. The question is whether there exists a phylogenetic tree and a weighting functionw so that

VII.2. Mathematical Biology

δ(x, y)like an algorithm to construct the tree and the weights.= dw (x, y) for all x, y \in  X. If so, one would If not, one would like to construct a family of trees that satisfy the relation approximately. Markov processes on trees where the partial order on Vfin ally, we note that there is a blossoming field of forms the basis for the Markov condition. Not only are there wonderful mathematical questions relating the geometry of the tree to the processes, but there are important issues for phylogenetics.
Suppose that one starts with characters defined only at the root and then allows them to “evolve” down the tree by (possibly different) Markov processes. Then, given the dis tr ibu-tion of characters on the leaves, when can we reconstruct the tree? These questions have even given rise to problems in algebraic geometry. Phylogenetics is useful not only for determining our past but also for controlling our present and future: see Fitch et al. (1997), where you can find a phylogenetic reconstruction for the influenza A virus.
An excellent recent graduate text in this field is Semple and Steel (2003).
 10 Mathematics in Medicine It is clear that an improved understanding of biological systems leads, at least indirectly, to improved medical care. However, there are many cases in which mathematics has a direct impact on medicine. We give two brief examples. who works on the fluid dynamics of the cardiovascular system. He wants to use fast simulations of flows as Charles Taylor is a biomedical engineer at Stanford part of the medical decision-making process.
Suppose that a patient presents with leg weakness and is found on magnetic resonance imaging (MRI) to have an arterial constriction in the thigh. Typically, the surgical group will meet and consider a variety of options including shunting blood from other vessels to a point below the constriction or shunting blood around the con st ric-tion with vessels removed from some other site in the patient’s body. Among a fairly large number of possi-ble choices, the surgical group chooses based on what they have been taught and on their own experience.
the characteristics of the flow after the graft are important not just for recovery of function but to prevent the for-mation of possibly destructive clots. An important difficulty is that patients treated successfully are rarely seen again, so one does not know the actual characteristics of the flow after the operation. Taylor wants to be in

847

on the discussion with the surgical team with immedi-ate fluid dynamical simulations based on the patient’s actual vasculature (as revealed by the MRI) for each proposed graft suggested. And he wants followup on each patient to check how well his simulations predicted the actual postoperative flow. worked on health policy for thirty years. He first David Eddy is an applied mathematician who has became prominent when he published Cancer: Theory, Analysis and Design (Eddy 1980), which Screening for grew out of his Ph. D. thesis.
Because of this book, the American Cancer Society changed its recommendation for the frequency of Pap smears from once ayear to once every three years, since Eddy’s modeling showed that the change would have little effect onthe life expectancy of the average American woman. A short calculation easily estimates the amount of money saved in an economy that spends 15% of its gross domestic product (GDP) on health care.
Through out his career Eddy has criticized both the indiscriminate useof diagnostic tests and the in correct use of the results by physicians and policy boards often ignorant of the basic facts of conditional probability. He has criticized specific health-policy guidelines as based on seat-of-the-pants guesswork instead of quantitative analysis. In a classic case he distributed questionnaires to physicians at a conference on colorectal cancer.
The physi-cians were asked to estimate the percentage drop in mortality from colorectal cancers if all Americans overage fifty were to have the two most common diagnostic tests each year: fecal blood smear and flexible sigmoidoscopy. The answers were approximately uniformly distributed in a range from 2% to 95%. Even more startling was the fact that the physicians did not even know that they disagreed so dramatically.
He has used mathematical models to analyze the costs and benefits of new and existing surgeries, medical treatments, and drugs, and he has participated robustly in debates on the current health-policy crisis. Through out, he has pointed out that a hefty percentage of GDPis spent on devices, drugs, and procedures with almost no mathematical analysis of which are effective. ics and medicine, see statistics For more on the interrelations between mathemat-[VII.11](/part-07/mathematics-and-medical-statistics).
mathematics and medical 11 Conclusions Mathematics and mathematicians have played impor-tant roles in many fields of biology that this brief

848

article has not had the space to cover: immunology, radiology, developmental biology, and the design of medical devices and synthetic biomaterials, to name just a few of the most obvious omissions. Nevertheless, this collection of examples and introductory dis-cus sions allows us to draw a few conclusions about mathematical biology. The range of biological problems needing explanation by mathematics is enormous and techniques from many different branches of mathe-matics are important.
It is not so easy in mathematical biology to extract simple, clear mathematical ques-tions to work on, because biological systems typically operate in a complex environment where it is diffi-cult to decide what should be counted as the system and what as the parts. Finally, biology is a source ofnew, interesting, and difficult questions for mathematicians, whose participation in the biological revolution is necessary for a full understanding of the biology itself.

Further Reading

Durrett, R., and S. Levin. 1994. The importance of being discrete (and spatial). Theoretical Population Biology 46: Eddy, D. M. 1980.363–94.and Design. Englewood Cliffs, NJ: Prentice-Hall. Screening for Cancer: Theory, Analysis Fall, C., E. Marland, J. Wagner, and J. Tyson. 2002.tational Cell Biology. New York: Springer. Compu Fitch, W. M., R. M. Bush, C. A. Bender, and N. J. Cox. 1997.Long term trends in the evolution of H(3) HA1 human influenza type A.Sciences of the United States of America Proceedings of the National Academy of94:7712–18. Grossberg, S.
1982.ples of Learning, Perception, Development, Cognition, and Studies of Mind and Brain: Neural Princi Keener, J., and J. Sneyd. 1998.Motor Control York: Springer.. Boston, MA: Kluwer. Mathematical Physiology. New Levin, S., B. Grenfell, A. Hastings, and A. Perelson. 1997.Mathematical and computational challenges in popula Nijhout, H. F. 2002. The nature of robustness in develop-tion biology and ecosystems science.ment. Bioessays 24(6):553–63. Science 275:334–43. Pevzner, P. A. 2000.Algorithmic Approach Computational Molecular Biology: An. Cambridge, MA: MIT Press. Schlick, T. 2002.York:
Springer. Molecular Modeling and Simulation. New Semple, C., and M. Steel. 2003.ford University Press. Phylogenetics. Oxford: Ox Waterman, M. S. 1995.ogy: Maps, Sequences, and Genomes Introduction to Computational Biol-. London: Chapman and Hall.

VII. The Influence of Mathematics

VII.3 Wavelets and Applications

Ingrid Daubechies

1 Introduction

One of the best ways to understand a function is to exp and it in terms of a well-chosen set of “basic” functions, of which perhaps the best-known example. Wavelets are fami-trigonometric functions [III.92](/part-03/trigonometric-functions) are lies of functions that are very good building blocks for a number of purposes. They emerged in the 1980 s froma synthesis of older ideas in mathematics, physics, electrical engineering, and computer science, and have since found applications in a wide range of fields.
The following example, concerning image compression, illustrates several important properties of wavelets. 2 Compressing an Image Directly storing an image on a computer uses a lot of memory. Since memory is a limited resource, it is highly desirable to find more efficient ways of storing images, or rather to find main ways of doing this is to express the image as a compressions of images. One of the function and write that function as a linear combina-tion of basic functions of some kind.
Typically, most of the coefficients in the expansion will be small, and if the basic functions are chosen in a good way it may well be that one can change all these small coefficients to zero with out changing the original function in a way that is visually detectable. of Digital images are typically given by large collections The boat image in figure 1 is made up of 256 pixels (short for picture elements; see figure 1).$\times 384$ pixels; each pixel has one of 256 possible gray values, ranging from pitch black to pure white.
(Similar ideas apply to color images, but for this exposition, it is simpler to keep track of only one color.) Writing a num-ber between 0 and 255 requires 8 digits in binary; the resulting 8-bit requirement to register the gray level for each of the 256. imes  384 = 98 304 pixels thus gives a total memory requirement of 786 432 bits, for just this one image. duced. In figure 2, two squares of 36 light ed, in different areas of the image.
As is clear from This memory requirement can be significantly re-. imes 36 pixels are high- its blowup, square A has fewer distinctive characteris-tics than square B (a blowup of which is shown in figure 1), and should therefore be describable with fewer bits. Square B has more features, but it too contains

(smaller) squares that consist of many similar pixels; again this can be used to describe this region with fewer than the 36. imes  36 . imes  8 bits given by the naive estimate of assigning 8 bits to each pixel. resentation of the image can lead to reduced memory requirements: instead of a huge assembly of pixels, all These arguments suggest that a change in the rep equally small, the image should be viewed as a combi-nation of regions of different size, each of which has more or less constant gray value;
each such region can then be described by its size (or scale), by where it appears in the image, and by the 8-bit number that tells us its average gray value. Given any subregion of the image, it is easy to check whether it is already of this simple type by comparing it with its average gray value. For square A, taking the average makes virtually no difference, but for square B, the average gray value is not sufficient to characterize this portion of the image (see figure 3).
squares, some of them have a virtually constant gray level (e.g., in the top-left or bottom-left regions of When square B is subdivided into yet smaller subsquare B); others, such as subsquares 2 and 3 (see figure 4), that are not of just one constant gray level may still have a simple gray level substructure that can be easily characterized with a few bits.

Figure 4 squares 2 and 3 do not, but they can be split horizontally (2)Subsquare 1 has constant gray level, while subor vertically (3) into two regions with (almost) constant gray level. Subsquare 4 needs finer subdivision to be reduced to “simple” regions. one should be able to implement it easily in an auto-mated way. This could be done as follows: To use this decomposition for image compression, •first, determine the average gray value for the whole image (assumed to be square, for simplic•ity); compare a square with this constant gray value with the original image;
if it is close enough, thenwe are done (but it will have been a very boring •image); if more features are needed than only the average gray value, subdivide the image into four equal-sized squares;

850

• for each of these subsquares, determine their aver- age gray value, and compare with the subsquare itself; •for those subsquares that are not sufficiently characterized by their average gray value, subdivide again into four further equal-sized subsquares (each now having an area one sixteenth of the original image);

• and so on.

In some of the subsquares it may be necessary to divide down to the pixel level (as in subsquare 4 in figure 4, for example), but in most cases subdivision can be stopped much earlier. Although this method is very easy to implement automatically, and leads to a description using many fewer bits for images such as the one shown, it is still some what wasteful. For instance, if the average gray level of the original image is 160, and we next determine the gray levels of eachof the four quarter images as 224, 176, 112, and 128, then we have really computed one number too many:
the average of the gray levels for the four equal-sized subimages is automatically the gray level of the whole image, so it is unnecessary to store all five numbers. In addition to the average gray value for a square, one just needs to store the extra information contained in the average gray values of its four quarters, given by the three numbers that describe • how much darker (or lighter) the left half of the• square is than the right, how much darker (or lighter) the top half of the• square is than the bottom, andhow much darker (or lighter) the diagonal from lower left to upper right is
than the other diagonal. Consider for example a square divided up into four subsquares with average values 224, 176, 112, and 128, as shown in figure 5. The average gray value for the whole square can easily be checked to be 160. Nowlet us do three further calculations. First, we work out the average gray values of the top half and the bottom half, which are 200 and 120, respectively, and calculate their difference, which is 80. Then we do the same forthe left half and the right half, obtaining the difference 168 - 152 = 16. Finally, we divide the four squares up diagonally:
the average over the bottom-left and top-right squares is 144, the average over the other two is 176, and the difference between these two is - 32. four original averages. For example, the average for From these four numbers one can reconstruct the VII. The Influence of Mathematics 224 176 112 128 Figure 5 four subsquares of a square. The average gray values for the top-right subsquare is given by 160(-32)]/2 = 176. + [80 - 16 + over smaller and smaller squares as described above, that needs to be repeated.
We now turn to the ques-It is thus this process, rather than simply averaging tion of making the whole decomposition procedure asefficient as possible. A complete decomposition of a 256. imes 256 square, from “top” (largest square) to “bottom” (the three types of “differences” for the 2. imes 2 subsquares), involves the computation of many numbers (in fact exactly256$\times 256 \text{before pruning})$, some of which are them- selves combinations of many of the original pixel val - ues.
For instance, the grayscale average of the whole 256 . imes 256 square requires adding 256 . imes 256 = 65 536 numbers with values between 0 and 255 and then divid-ing the result by 65 536; another example, the difference between the averages of the left and right halves, requires adding the 256. imes 128 = 32 768 grayscale num-bers for the left half and then subtracting from this sum A the sum B of another 32 768 numbers.
On the other hand, the sum of the pixel grayscale values over the whole square is simply A + B, a sum of two 33- bit numbers instead of 65 536 numbers of 8 bits each. This allows us to make a considerable saving in computational complexity ifthe average over the whole square. A computationally A and B are computed before optimal implementation of the ideas explained so far must therefore proceed along a different path from the one sketched above. Indeed, a much better procedure is to start from the other end of the scale.
Instead of starting with the whole image and repeatedly subdividing it, one begins at the pixel level and builds up. If the image has 2 J . imes 2 Jpixels in total, then it can also be viewed as consisting of 2 J-1 . imes (2 J)-1“superpixels,” each of which is a small square of 2. imes  2 pixels. For each 2 . imes  2 square, the aver- age of the four gray values can be computed (this is the gray value of the superpixel), as well as the three types of differences indicated above. More over, these computations are all very simple.

VII.3. Wavelets and Applications

for each of the 2 ages, the gray values of the 2 The next step is to store the three difference values. imes  2 squares and organize their aver-J -1 . imes  (2 J)-1 superpixels, into a new square. This square can be divided, in turn, into 2$J - 2 \times (2J)-2$“super-superpixels,” each of which is a small square of 2. imes  2 superpixels (and thus stands$for 4after$. imes Jlevels of “zooming out,” there is only one super4 “standard” pixels), and so on. At the very end, J- pixel remaining; its gray value is the average over the whole image.
The last three differences that were computed in this pixel - level-up process correspond exactly to the largest-scale differences that the top-down procedure would have computed computational expense. first, at much greater Carrying out the procedure from the pixel level up, none of the individual averaging or differencing compu-tat i ons involves more than two numbers; the total number of these elementary computations, for the transform, is only 8((22)J - 1)/3.
For the 256 . imes 256 square whole discussed before, J = 8, so the total is 174 752, which is about the number of computations needed for just one level in the top-down procedure. How can all this lead to compression? At each stage of the process, three species of difference numbers are accumulated, at different levels and corresponding to different positions. The total number of differences cal-culated is 3(1 + 22 + · · · + (22)((J-1))) = (22)J - 1.
Together with the gray value of the whole square, this means we end up with exactly as many numbers as we had gray values for the original 2 J . imes 2 J pixels. However, many of these difference numbers will be very small(as argued before), and can just as well be dropped or put to zero, and if the image is reconstructed from the remainder there will be no perceptible loss of quality. Once we have set these very small differences tozero, a list that enumerates all the differences (in some prearranged order) can be made much shorter:
when-ever a long stretch of Z zeros is encountered, it can be replaced by the statement “insert which requires only a prearranged symbol (for “insert Zzeros now,” zeros now”), followed by the number of bits needed for Z, i.e., log Z. This achieves, as desired, a significant compression of the data that need to be stored for large images. (In practice, however, image compression2 involves briefly below.)many more issues, to which we shall return above is an elementary example of a The very simple image decomposition described wavelet decomposition.
The data that are retained consist of 851 •• a very coarse approximation, and additional layers giving detail at successively finer scales to J - 1 (the first superpixel level).j, with j ranging from 0 (the coarsest level) More over, within each scale of many pieces, each of which has a definite localiza - j the detail layer consists tion (indicating to which of the supertains), and all the pieces have “size” 2 jj-pixels it per-.
(That is, the size, in pixel widths, of the corresponding superj-pixel$is 2$ j.) In particular, the building blocks are very small at fine scales and become gradually larger as the scale becomes coarser. 3 Wavelet Transforms of Functions In the image-compression example we needed to lookat three types of differences at each level (horizontal, vertical, and diagonal) because the example was a two-dimensional image. For a one-dimensional signal, one type of difference suffices. Given a function R, one can write a wavelet transform off that is entire lyf from R to analogous to the image example.
For simplicity, let uslook at a functionf such that f (x) = 0 except when x belongs to the interval[0, 1]. by Let us now consider successive approximations ofstep functions: that is, functions that change value in$f$ only finitely many places. More precisely, for each posi-tive integerj, divide the interval [0,1] up into 2 j equal intervals, denoting the interval from$k2^{-j} to (k + 1)2^{-j}$ byfunction$I^{j}$, k (so that P (f ) by setting its value onk runs from 0 to 2 j -I1). Then define ato be the average value ofure 6, which shows the step functionj fon that interval.
This is illustrated in fig- Pj, k(f ) for a func- tionf whose graph is shown as well. As3 j increases, the width of the intervals closer tof . (In more precise mathematical terms, if Ij, k decreases, and Pj(f ) getsp < . nfty and f belongs to the function space [III.29](/part - 03/function - spaces)Lp, then P (f ) converges to f in Lp.)j Each approximation P (f ) of f can be computed eas - j ily from the approximation(Pj)+1(f )at the next-finer scale:
the average of the values that the two intervals I+ and (I+)+Pjgives the value+1(f ) takes on that Pj(f ) takes onj Ij$, k1$,2 k$.j 1\\{}$,\\2 k 1 move from Of course, some information about Pj+1(f ) to Pj(f ). On every intervalf is lost when we I^j, k, the difference between tion, with constant levels on the(Pj)+1(f ) and PIj(f )+, that takes onis a step func- exactly opposite values on each pairj ((I1)\\\{}}$,\\ (lj()+1()\\\\{,\\\\}){2}k$, (Ij)+1$,2 k + 1)$.$852 (a) (b) f (x) P f (x) 0$1 1 x 01 1 x22$Figure 6mation$ P3 Graphs of (a) the function(f ), which is constant on every interval
betwee nf and (b) its approx i - l/the average of8 and (l + 1)/f8, withon each of these intervals.l = 0, 1, . . . ,7, and exactly equal to The difference tion functions, over all of(Pj)+1(f ) - Pj[(f )0,1 of the two approxima-], consists of a juxta- position of such up - and - down (or down - and - up) step functions, and can therefore be written as a sum of translates of the same up - and-down function, with appropriate coefficients:(Pj)+1(f )(x) - Pj(f )(x) =2 k j=-0 1 aj$, k Uj(x - 2 - jk)$, where \{|||1 for x between 0 and (2-)((j+1)), Uj(x) = ⎨⎪⎪⎪⎩-1 for x between 2^-^(j+1^) and 2 . imes
2^-^(j+1^),0 for all other x. More over, the “difference functions”U at the dif-j

ferent levels are all scaled copies of a single function H, which takes the value 1 between 0 and1 and -1 between that each difference12 and 1; indeed, P (f )(x)Uj(x)-=PH((f )(x)2 jx). It follows is a linear2 combination of the functions ing from 0 to 2 j -1; adding many such differences, for j+1 H(2 jxj - k), with k rang- successive combination of the collection of functionsj, shows that PJ (f )(x)-P0(f )(x)H(is a linear2 jx - k),$with2$ j - j1. Picking larger and larger ranging from 0 to J - 1 and J makesk ranging from 0 to P (f ) closer J

and closer to ference between$f$; one finds thatf and its average) can be viewed as af - P0(f ) (i.e., the dif- (possibly infinite) linear combination of the functions$H(2^{j}x - k)$, now with j ranging over all the nonnegative integers. This decomposition is very similar to what was done for images at the start of the article, but in one dimen-sion instead of two and presented in a more abstract way. The basic ingredients are thatf minus its aver- age has been decomposed into a sum of layers at suc-cessively finer and finer scales, and that each extra

VII. The Influence of Mathematics

layer of detail consists of a sum of simple “difference contributions” that all have width proportional to the scale. More over, this decomposition is realized by using translates and dilates of the called the Haar wavelet, after Alfred Haar, who first single function H(x), often defined it at the start of the twentieth century (though not in a wavelet context). The functions$H(2^{j}x - k)$ constitute anthat the inner product orthogonal H(set of functions, meaning2 jx - k)H(2 j^ x - k^ )dx is zero except when$j = j and k = k$; if we define $H[H^{j}$, k (x)(x)]=2 d2 xj/2=H(1.
A consequence of this is that the2 jx - k), then we also have that wavelet coefficients the “j, kjth layer”P w(f )(x)j, k(f )-that appear when we write P (f )(x) of the function fby the formula as a linear combination(wj)+1(f ) = f (x)Hk (wj)j, k(f )H(x)j, kd(x)x. are given most applications, including image compression, they Haar wavelets are a good tool for exposition, but for$j$, kj, k are not the best choice.
Basically, this is because replac-ing a function simply by its averages over intervals (in one dimension) or squares (in two dimensions) results in a very-low-quality approximation, as illustrated in figure 7(b). (i.e., as thef As the scale of approximation is made finer and finer and P (f )j in becomes smaller; with a piecewise-con-$P^{j}(f )$increases), the difference between

$j$

stant approximation, however, this requires corrections at almost every scale “to get it right” in the end. Unless the original happens to be made up of large areas where it is roughly constant, many small-scale Haar wavelets will be required even in stretches where the function just has a consistent, sustained slope, with out “genuine” fine features. is that of scheme can be defined by providing a family of “build-The right framework to discuss these questions approximation schemes. An approximation ing blocks,” often with a natural order in which they are usually enumerated.
A common way of measuring the quality of an approximation scheme is to define VN to be the space of all linear combinations of the first N building blocks, and then to let A f be the clos- est function inby the L -norm (though other norms can also be used).VN to f , where distance is measured N Then one examines how the distance[ |f (x)2 - A f (x)|2 dx(]1()/){2} decays as N f tends to infin-$- A^{N} f^{2} =$ ity. An approximation scheme is said to befor a class of functions N F if f - A f ⩽ CNof order-L for all L functions must be independent off in F , where NC.
The order of an approxima-typically depends o(n N)2 f but tion scheme for smooth functions is closely linked to

VII.3. Wavelets and Applications

(a) f (x) (b)

0 1 x 0

Figure 7[k2-3, (k +(a) The original function. (b), (c) Approximations of1)2 - 3). The best approximation of fby a piecewise-constant function is shown in (b); the best by a continuous piecewise-linear function is in (c). the performance of the approximation scheme on polynomials (because smooth functions can be replaced in estimations, at very little cost, by the polynomials given by their Taylor expansions). In particular, the types of approximation schemes considered here can have order degree at most L only if they perfectly reproduce polynomials of L - 1.
In other words, there should exist some most LN - 0 such that if1 and N ⩾ Np, thenis any polynomial of degree at A p = p. from zero only between 0 and 1, the building blocks In the Haar case, applied to functions0 N fthat differ consist of the function and 0 out side, together with the familiesφ that takes the value 1 on{H};$[0k$,1=] 0$P$, . . . ,^Haar(f )2^j -can be written as a linear combination of the1 for j = 0, 1, 2, . . . . We saw above that^j, kj$first 1$+ 2^0 + 2^1 + · · · + 2^j^-^1 = 2^j building blocks φ, H^0^,^0, H^1^,^0, H^1^,^1, H^2^,^0, . . . , H^j^-^1^,^2^j^-^1^-^1.
Because the Haar wavelets are orthogonal to each other, this is also the linear combination of these basis functions that is closest toj = 3) bothf , so that AHaar Pf Haarjand(f )APL=f A, which is the best approx-Haa(r2)j. Figure 7 shows (for imation ofwith breakpoints atf by a continuous, piecewise-linear functio(n2)j k2-j,2 kj = 0, 1, . . . , 2 j - 1. It turns out that if you are trying to approximate a function Haar wavelets, then the best decay you can obtain, evenf using iforf is smooth, is of the formf - AHaarf ⩽ CN-1 forf N- P=j Haar2 j(f ).
This means2 ⩽ C2 - j, that approximation by Haar wavelets is a approximation scheme. Approximation by continuous N2 first-order piecewise-linear functions is afor smooth$f$, f - APLf ⩽ Cn second - order-2 for N = 2 scheme: j. Note that the difference between the two schemes can alsobe seen from the maximal degree$N^{2} d \text{of polynomials they}$ “reproduce” perfectly: clearly both schemes can repro-duce constants ($d =$0); the piecewise-linear scheme can also reproduce linear functions ($d = 1)$, where as the Haar scheme cannot.

853

(c)

1 x 0 1 x

f by a function that equals a polynomial on each interval defined on the interval equals about Take now any continuously differentiable function$C2^{-j}$; for an approximation scheme of[0,1]. Typically f -(Pj)Haar(f ) f2 order 2, that same difference would be about In order to achieve the same accuracy as$P^{H}aar(f )C 2$, th(e-2)j.j

piecewise-linear scheme would thus require only levels instead ofj levels. For higher orders L, the gainj/2 would be even greater. If the projections to a higher-order approximation scheme like this, then$P^{j} \text{gave rise}$ the difference as not to matter, even for modest values of(Pj)+1(f )(x)-Pj(f )(x) would be so smallj, wherever the function ues of$j$, the difference would be important only nearfwas reasonably smooth;
for these val points where the function was not as smooth, and soonly in those places would a contribution be needed from “difference coefficients” at very fine scales. This is a powerful motivation to develop a framework similar to that for Haar, but with fancier “generalized averages and differences” corresponding to successive Pj(f ) associated with higher-order approx- imation schemes. This can be done, and was done in anexciting period in the 1980 s to which we shall return briefly below.
In these constructions, the generalized averages and differences are typically computed by combining more than two finer-scale entries each time, in appropriate linear combinations. The corresponding function decomposition represents functions as(possibly infinite) linear combinations of waveletsψ derived from a wavelet is defined to be 2 j/^2ψ(ψ2^j. As in the case ofx - k). Thus$, \text{the functions} H, ψ^{j}$, k(x)j, kψsinglej, k are again normalized translates and dilates of a function;
this is due to our using systematically the same averaging operator to go from scale$j + 1 to$ scale the difference between levels$j$, and the same differencing operator to quantify$j + 1 and j$, regard less of the value ofto use the same averaging and differencing opera torj. There is no absolutely compelling reason for the transition between any two successive levels,

854

and thus to have all theand dilating a single function. However, it is very conve-$ψ^{j}$, k generated by translating nient for implementing the transform, and it simplifies the mathematical analysis. One can additionally require that, like the$H$, theψ(L2)j, k(R)constitute an. The basis part means that every function can orthonormal basis for the spacej, k be written as a (possibly infinite) linear combination of the orthogonal$ψ^{j}$, k;
theto each other, except if they are equal, in orthonormality means that the$ψ^{j}$, k are which case their inner product is 1.As we have already mentioned, the projections$Pj$

for the waveletψ will correspond to an approximation scheme of order all polynomials of degree less than L only if they can reproduce perfectly L. If the functionsψ0 when everj, k are orthogonal, thenj^  > j. The ψψj^, kcan thus be associ-(x)Pj(f )(x) dx = ated with an approximation scheme of order if$ψ (x)p(x) dx =$0 for sufficiently largej, k Lj only and for all polynomials ing and translating, this reduces to the requirementj, k p of degree less than L. By scal-xlψ(x) dx = 0 for l = 0,1, . . . , L - 1.
When this requirement is met,ψ is said to have L vanishing moments Figure 8 shows the graphs of some choices for.ψ that give rise to orthonormal wavelet bases and that are used in various circumstances. ular for similar to that for the Haar wavelet can be used to carry For the wavelets of the type(ψ[()4){]}, ψ[ 6 ], and ψ[ 12] ψin figure 8, an algorithm[2 n], and thus in partic- out the decomposition, except that instead of combin-ing two numbers from P+ to obtain an average or a difference coefficient at level positions require weighted combinations of four,
six,(j1)\\\{}},\\\\\\\\\\\\\\\\\\\} k j, these wavelet decom- or twelve finer-level numbers, respectively. (More gen - erally, 2$n$finer-level numbers are used forψ[ 2 n].) Because the Meyer and Battle–Lemarié wavelets$ψ^{[}M^{]} and$ψ[ BL]are not concentrated on a finite interval, different algorithms are used for wavelet expansions with respect to these wavelets. be sides the examples given above. Which one to choose There are many useful orthonormal wavelet bases depends on the application one has in mind.
For instance, if the function classes of interest in the application have smooth pieces, with abrupt transitions or spikes, then it is advantageous to pick a smoothψ, corresponding to a high-order approximation scheme. This allows one to describe the smooth pieces effi-ciently with coarse-scale basis functions, and to leave the fine-scale wavelets to deal with the spikes and

VII. The Influence of Mathematics

1 H =$\psi [2] 1 \psi [12] 4 0 4 1$. si  [4] 10 . si  [M]

4−4 4$1$. si  [6] 10 . si  [BL]

0 4−4 4$Figure 82$ j/2ψ(2 j x Six different choices of- k), j, k \in  Z, constitute an orthonormal basisψ for which the ψj, k(x) = forple of a family$L^{2}(R)$. The Haar wavelet can be viewed as the first exam-ψ[2 n], of which the wavelets for n = 2, 3, and 6 are also plotted here. Each ing moments and is supported on (i.e., is equal to zero$ψ^{[}2 n] has n vanish-$ out side) an interval of width 2 wavelets are not supported on an interval; however, the$n - 1$. The remaining two Fourier transform of the Meyer wavelet on$[-8π/3$, -2π/3] ∪ [2π/3, 8π/3];
allψmoments of^[^M^] is supportedψ^[^M^] vanish. The Battle–Lemarié wavelet en ti able, is piecewise polynomial of degree 3, and hasψ[ BL]is twice differ exponential decay; it has four vanishing moments. abrupt transitions. In that case, why not always use a wavelet basis with a very high approximation order? The reason is that most applications require numerical computation of wavelet transforms; the higher the order of the approximation scheme, the more spread out the wavelet, and the more terms have to be usedin each generalized average/difference, which slows down numerical computation.
In addition, the wider the wavelet, and hence the wider all the finer-scale wavelets derived from it, the more often a dis continu-ity or sharp transition will overlap with these wavelets. This tends to spread out the influence of such transitions over more fine-scale wavelet coefficients. There-fore, one must find a good balance between the approximation order and the width of the wavelet, and the best balance varies from problem to problem. of orthonormality is relaxed.
In this case one typically uses two different “dual” wavelets There are also wavelet bases in which the restrictionψand  ̃ψ, such that The approximation order of the scheme that approx i-mates functions$−\infty\infty ψ^{j}$, k(x)ψ ̃j, k^ (x)f by linear combinations of thedx = 0 unless j = j^  and k =ψkj, k.

VII.3. Wavelets and Applications

is then governed by the number of vanishing moments of  ̃ψ. Such wavelet bases are called biorthogonal. They have the advantage that the basic wavelets can both be symmetric and concentrated on an inter-ψand  ̃ψ val, which is impossible for orthonormal wavelet bases other than the Haar wavelets. composition, where preference is usually given to two-dimensional wavelet bases derived from one-dimen-The symmetry condition is important for image desional bases with a symmetric functionto which we return below.
When an image is com-ψ, a derivation press ed by deleting or rounding off wavelet coeffi-cients, the difference between the original image I and its compressed version$I^{c}\text{omp is a combination}$, with small coefficients, of these two-dimensional wavelets. It has been observed that the human visual system is more tolerant of such small deviations if they are symmetric;
the use of symmetric wavelets thus allows for slightly larger errors, which translates to higher compression rates, before the deviations cross the threshold of perception or acceptability. Another way of generalizing the notion of wavelet bases is to allow more than one starting wavelet.
Such systems, known as multiwavelets, can be useful even in one dimension. When wavelet bases are considered for functions defined on the interval R, the constructions are typically adapted, giving bases[a, b] rather than the whole of of interval wavelets in which specially crafted wavelets are used near the edges of the interval. It is some-times useful to choose less regular ways of subdividing intervals than the systematic halving considered above: in this case, the constructions can be adapted to give irregularly spaced When the goal of a decomposition is compression of wavelet bases.
the information, as in the image example at the start, it is best to use a decomposition that is itself as efficient as possible. For other applications, such as pat-tern recognition, it is often better to use redundant families of wavelets, i.e., collections of wavelets that contain “too many” wavelets, in the sense that all functions in L2(R) could still be represented even if one dropped some of the wavelets from the collection.tinuous wavelet families and wavelet frames are the Contwo main kinds of collections used for such redundant wavelet representations.
4 Wavelets and Function Properties Wavelet expansions are useful for image compression because many regions of an image do not have features

855

at very fine scales. Returning to the one-dimensional case, the same is true for a function that is reasonably smooth at most but not all points, like the function illustrated in figure 6(a). If we zoom in on such a func-tion near a pointx where it is smooth, then it will look almost linear, so we will be able to represent that part0 of the function efficiently if our wavelets are good at representing linear functions. their power:
the waveletsψ^[This is where wavelet bases other than Haar show BL^]depicted in figure 8 all define approximationψ[4], ψ[6], ψ[12], ψ[M], and schemes of order 2 or higher, so that for allj, k. This is also seen in the numerical imple-xψj, k(x) dx = 0 mentation schemes:
the corresponding generalized dif-fer enc ing that computes the wavelet coefficients of$f$ gives a zero result not only when the graph is flat, but also when it is a straight but sloped line, which is not true for the simple differencing used for the Haar basis. As a result, the number of coefficients needed for the wavelet expansion of smooth functions preassigned accuracy is much smaller when one usesf to reach a more sophisticated wavelets than the Haar wavelets. For a functionfthat is twice differentiable except at a finite number of discontinuities, and with a basic wavelet that has, say,
three vanishing moments, typically only very few wavelets at fine scales will be needed to write a very-high-precision approximation to fcontinuity points. This feature is characteristic for all. More over, those will be needed only near the dis- wavelet expansions, whether they are with respect toan orthonormal basis, a basis that is nonorthogonal, or even a redundant family. Figure 9 illustrates this for one type of redundant expansion, which uses the so-called Mexican hat wavelets, which are given by. qrt{}. qrt{}ψ(x) = (2 2/ 3)π-1/4(1 - 4 x2)e-2 x 2;
this wavelet gets its name from the shape of its graph, which looks like the cross section of a Mexican hat (see the figure). is differentiable), the faster its wavelet coefficients will decay as The smoother a functionj increases, provided the waveletf is (i.e., the more times itψ has suf- fic ient ly many vanishing moments. The converse statement is also true: one can read off how smooth the func-tion is atx0 from how the wavelet coefficientsw (f ) decay, asthe “relevant” pairsj increases. Here one restricts attention to(j, k).
In other words, one consid-j, k ers only the pairs where more precise terms, this converse statement can be$ψ^{j}$, k is localized near x0. (In$856$1−2 −1−1 0 1 2. si  [MH]

Figure 9 approximated by finite linear combinations of Mexican hat A function with a single discontinuity (top) is wavelets figure. Adding finer scales leads to increased precision. Left:ψj$, l^{[}MH^{]}$; the graph ofψ^[MH^] is at the bottom of the successive approximations for contributions from the wavelets at the scales needed to$j =$1, 3, 5, and 7. Right: total bridge from onein steps of 1 .) The finer the scale, the more the extra det a ilj to the next. (In this example, j increases is concentrated near the discontinuity point.
reformulated as an exact character ization of the so-called Lipschitz spaces$C^{α}$, for all noninteger α that are strictly less than the number of vanishing momentsofψ.) many other useful properties of functions, both global and local. Because of this, wavelets are good bases not Wavelet coefficients can be used to characterize just for L2-spaces or the Lipschitz spaces, but also for many other function spaces, such as, for instance, the Lp-spaces with 1 < p < . nfty , the [III.29 §2.4](/part-03/function-spaces), and a wide range of Besov spaces.
The sobolev spaces versatility of wavelets is partly due to their connec-tion with powerful techniques developed in harmonic analysis through out the twentieth century. bases are associated with approximation schemes ofdifferent orders. So far we have considered approx i-We have already seen in some detail that wavelet mation schemes in which the combinations of the same N building blocks, regard-AN f are always linear

VII. The Influence of Mathematics

less of the function tion, because the collection of all functions of the formf . This is called linear approxima-AN f is contained in the linear span VNof the first N basis functions. Some of the function spaces mentioned above can be characterized by specifying the decay of f - AN f2 as N increases, where ANis defined in terms of an appropriate wavelet basis. ested in, we are really carrying out a different kind of approximation.
Given a function However, when it is compression that we are inter-f , and a desired accu- racy, we want to approx i ma tef to within that accuracy by a linear combination of as few basis functions aspossible, but we are not trying to choose those functions from the first few levels. In other words, we are no longer interested in the ordering of the basis functions and we do not prefer one label(j, k) over another. imation that is made up of at most If we want to formalize this, we can define an approx-AN f to be the closest linear combination to N basis functions.
By anal-f ogy with linear approximation, we can then define theset V as the set of all possible linear combinations of Nli near spaces: two arbitrary elements basis functions. However, the sets N VVN are no longer are typically combinations of two different collections of N N basis functions, so that their sum has no reason to belongto V (though it will belong to V ). For this reason, AN f Nis called a nonlinear approximation o(f2)N f . by imposing conditions on the decay ofas One can go further and define classes of functions N increases, with respect to some function spacef − ANf , norm· .
This can of course be done starting from any basis; wavelet bases distinguish themselves from many other bases (such as the trigonometric functions) in that the resulting function spaces turn out to bestandard function spaces, such as the Besov spaces, for example. We have referred several times to func-tions that are smooth in many places but have possible discontinuities in isolated points, and argued that they can be approximated well by linear combinations of a fairly small number of wavelets.
Such functions are special cases of elements of particular Besov spaces, and their good approximation properties by sparse wavelet expansions can be viewed asa consequence of the character ization of these Besov spaces by nonlinear approximation schemes using wavelets.1 can be found on the Internet at www.wavelet.org.1. More types of wavelet families, as well as many generalizations, VII.3. Wavelets and Applications 5 Wavelets in More than One Dimension There are many ways to extend the one-dimensional constructions to higher dimensions.
An easy way to construct a multi dimensional wavelet basis is to com-bine several one-dimensional wavelet bases. The image decomposition at the start is an example of such a combination: it combines two one-dimensional haar decompositions. We saw earlier that a 2$\times 2 superpixel$ could be decomposed as follows. First, think of it asarranged in two rows of two numbers, representing the gray levels of the corresponding pixels. Next, for each row replace its two numbers by their average and their difference, obtaining a new 2 . imes 2 array.
Finally, do the same process to the columns of the new array. this produces four numbers, the result of, respectively, • averaging both horizontally and vertically,••averaging horizontally and differencing vertically, differencing horizontally and averaging vertically, • and differencing both horizontally and vertically. The first is the average gray level for the superpixel, which is needed as the input for the next round of the decomposition at the next scale up. The other three correspond to the three types of “differences” already encountered earlier.
If we start with a rectangular image that consists of 2 we end up with 2 KKrows, each containing 2-1 . imes (2 J)-1 numbers of each of the J pixels, then four types. Each collection is naturally arranged in a rectangle of half the size of the original (in both direc - tions); it is customary in the image-processing literature to put the rectangle with gray values for the super-pixels in the top left; the other three rectangles each group together all the differences (or wavelet coeffi - cients) of the other three kinds.
(See the level 1 decomposition in figure 10.) The rectangle that results from horizontal differencing and vertical averaging typically has large coefficients at places where the original image has vertical edges (such as the boat masts in the exam-ple above); likewise, the horizontal averaging/vertical differencing rectangle has large coefficients for hori-zontal edges in the original (such as the stripes in the sails); the horizontal differencing/vertical differencing rectangle selects for diagonal features.
The three differ-ent types of “difference terms” indicate that we have here three basic wavelets (instead of just one in theone-dimensional case). scenario is repeated on the rectangle that contains the In order to go to the next round, one scale up, the 857 superpixel gray values (the results of averaging both horizontally and vertically); the other three rectangles are left unchanged.
Figure 10 shows the result of this process for the original boat image, though the wavelet basis used here is not the Haar basis, but a symmetric biorthogonal wavelet basis that has been adopted in the JPEG 2000 image compression standard. The result is a decomposition of the original image into its component wavelets. The fact that so much of this is gray indicates that a lot of this information can be discarded with out affecting the image quality.
moments is important not just when the wavelet basis is used for characterizing properties of functions, but Figure 11 illustrates that the number of vanishing also when it comes to image analysis. It shows an image that has been decomposed in two different ways: once with Haar wavelets, the other with the JPEG 2000 stan-dard biorthogonal wavelet basis. In both cases, all but the largest 5% of the wavelet coefficients have been set to zero, and we are looking at the corresponding reconstructions of the images, neither of which is perfect.
However, the wavelet used in the JPEG 2000 standard has four vanishing moments, and therefore gives a much better approximation in smoothly varying parts of the image than the Haar basis. More over, the reconstruction obtained from the Haar expansion is “blockier” and less attractive. 6 Truth in Advertising: Closer to True Image Compression Image compression has been discussed several times in this article, and it is indeed a context in which wavelets are used.
However, in practice there is much more to image compression than the simple idea of dropping all but the largest wavelet coefficients, taking the resulting truncated list of coefficients, and replacing each ofthe many long stretches of zeros by its runlength. In this short section we shall give a glimpse of the large gap between the mathematical theory of wavelets as discussed above and the real-life practice of engineers who want to compress images. get,” and all the information to be stored has to fit First of all, compression applications set a “bit bud with in the bit budget;
statistical estimates and infor - mation-theoretic arguments about the class of images under consider at i on are used to allocate different num-bers of bits to different types of coefficients. This bit allocation is much more gradual and subtle than just Figure 10 decompositions are shown after one level of averaging and differencing, as well as after two and three levels. In the rectangles Wavelet decomposition of the boat image, together with a grayscale rendition of the wavelet coefficients.
The corresponding to wavelet coefficients (i.e., not averaged in both directions), where numbers can be negative, the convention is to use gray scale 128 for zero, and darker/lighter gray scales for positive and negative values. The wavelet rectangles are mostly at gray scale 128, indicating that most of the wavelet coefficients are negligibly small. Figure 11 basis, and discarding the 95% smallest wavelet coefficients. Left: Haar wavelet transform. Right: wavelet transform using the Top: original image, with blowup. Bottom:
approximations obtained by expanding the image into a wavelet so-called 9–7 biorthogonal wavelet basis. retaining or dropping coefficients. Even so, many coef-ficients will get no bits assigned to them, meaning that tion about the addresses cancel out a large portion of the gain made by the nonlinear wavelet approx i ma they are indeed dropped altogether. tion.
Every practical wavelet-based image-compression be taken that each of the remaining coefficients is given its correct Because some coefficients are dropped, care has to address, i.e., its(j$, k1$, k2) label, which scheme uses some sort of clever approach to deal with this problem.
One implementation exploits the obser-vation that at locations in the image where wavelet coef- is essential for “decompressing” the stored informa-tion in order to reconstruct the image (or rather, an ficients of some species are negligibly small at some scale$j$, the wavelet coefficients of the same species approximation to it). If you do not have a good strat- at finer scales are often very small as well.
(Check it egy for doing this, then you can easily find that the computational resources needed to encode informa- out on the boat image decomposition given above.) Ateach such location, this method sets a whole tree of

VII.3. Wavelets and Applications

finer-scale coefficients (four for scalescalej +2, etc.) automatically to zero; for those loca- j + 1, sixteen for tions where this assumption is not borne out by the wavelet coefficients that are obtained from the actual decomposition of the image at hand, extra bits must then be spent to store the information that a correction has to be made to the assumption. In practice, the bits gained by the “zero - trees” far outweigh the bits needed for these occasional corrections. can play a role.
For instance, if the compression algo-rithm has to be implemented in an instrument on a Depending on the application, many other factors satellite where it can only draw on very limited power supplies, then it is also important for the computations involved in the transform itself to be as economical aspossible. considerations of this kind can find them discussed in Readers who want to know more about (important!) the engineering literature.
Readers who are content tostay at the lofty mathematical level are of course welcome to do so, but are hereby warned that there is moreto image compression via wavelet transforms than has been sketched in the previous sections. 7 Brief Over view of Several Influences on the Development of Wavelets Most of what is now called “wavelet theory” was devel-oped in the 1980 s and early 1990 s.
It built on existing work and insights from many fields, including har-monic analysis (mathematics), computer vision and computer graphics (computer science), signal analysis and signal compression (electrical engineering), coherent states (theoretical physics), and seismology (geo - physics). These different strands did not come together all at once but were brought together gradually, often asthe result of serendipitous circumstances and involving many different agents. back to work by1930 s.
An important general principle in Fourier analy-In harmonic analysis, the roots of wavelet theory go littlewood [VI.79](/part - 06/john - edensor - littlewood - 18851977) and Paley in the sis is that the smoothness of a function is reflectedin its fourier transform [III.27](/part - 03/the - fourier - transform): the smoother the function, the faster the decay of its transform. Littlewood and Paley addressed the question of characteriz-ing local smoothness.
Consider, for example, a periodic function with period 1 that has just one discontinuity in the interval[0, 1) (which is then repeated at all inte- ger translates of that point), and is smooth elsewhere. Is the smoothness reflected in the Fourier transform?

859

the answer is no: a discontinuity causes the fourier coefficients to decay slowly, however smooth the rest of If the question is understood in the obvious way, then the function is. Indeed, the best possible decay is of the form|fˆ| ⩽ C[1 + |n|]-1. If there were no dis continu- ity, the decay would be at least as good aswhenfnis k-times differentiable.$Ck[1+|n|]^{-}k$ local smoothness and Fourier coefficients. Let periodic function, and let us write its However, there is a more subtle connection between nth Fourier coef-f be a fic ient ˆ$f$ˆ and e(fn)iθasis itsaneiθphasen, where.
When examining the decay ofan is the absolute value of the Fourier coefficients, we look just at$n^{n} a^{n} \text{and for}-$ get all about the phases, which means that we can-not detect any phenomenon unless it is unaffected by arbitrary changes to the phases. Ifity, then we can clearly move it about by changing thef has a dis continu- phases. It turns out that these phases play an important role in determining not just where the singular i-ties are, but even their severity:
if the singularity at$x^{0}$ is not just a discontinuity but a divergence of the type|f (x)| ∼ |x - x (|-)β, then one can change the value ofβabsolute values just by changing the phases and with out altering the0|a |. Thus, changing phases in Fourier series is a dangerous thing to do:
it can greatly change the properties of the function in question.$n$ the phases of Fourier coefficients are more innocuous. Littlewood and Paley showed that some changes of In particular, if you choose a phase change for the first Fourier coefficient, another one for both the next two coefficients, another for the next four, another for the next eight, and so on, so that the phase changes are constant on “blocks” of Fourier coefficients that keep doubling in length, then local smoothness (or absence of smoothness) properties off are preserved.
Similar statements hold for the Fourier transform of functionson R (as opposed to Fourier series of periodic functions). This was the first result of a whole branch of harmonic analysis in which at ically to deal with detailed local analysis, and in which scaling was exploited system very powerful theorems were proved that, with hind-sight, seem ready-made to establish a host of powerful properties for wavelet decompositions.
The simplest way to see the connection between Littlewood–Paley theory and wavelet decompositions is to consider the Shannon wavelet1 whenπ ⩽ |ξψ|^[<Sh^], which is defined by ˆ2π, and ˆψ[ Sh](ξ) =ψ[0 other-Sh](ξ) = wise. Here, ˆwavelet(ψ[)Shψ]. The corresponding function(s[)Sh] denotes the Fourier transform of theψ[ Sh](x) =$2$ j/2ψ[ Sh](2jx - k) constitute an orthonormal basis forj, k

860

Difference between

Blur

original and first blur

Blur Difference between first and second blur

Figure 12 blurs give detail at different scales. Differences between successive L2(R), and for each. nfty  f[and each Sh] j the collection of inner products restricts to the set 2(−$\infty$ f (x)ψj -j, k1 ⩽(x)π^-d1 x)|ξ|k\in <Z2 tells us how ˆj. In other words, f (ξ) it gives us the Scaling also plays an important role in computer jth Littlewood–Paley block off .
vision, where one of the basic ways to “understand” an image (going back to at least the early 1970 s) is to blur it more and more, erasing more detail each time, so asto obtain approximations that are graded in “coarseness” (see figure 12). Details at different scales can then be found by considering the differences between successive coarsenings.
The relationship with wavelet transforms is obvious!An important class of signals of interest to electrical engineers is that of fun ct io nsf , usually of one variable only, for which the bandlimited signals, which are Fourier transform ˆf vanishes out side some interval. In other words, the frequencies that make upsome “limited band.” If the interval is[-Ω, Ω]f come from, then f is said to havepletely characterized by their values, often called bandlimitΩ. Such functions are com-sam- ples, at integer multiples of$π/Ω$. Most manipulations on the signal at i ons on this sequence of samples.
For instance, wef are carried out not directly but by oper-

VII. The Influence of Mathematics

might want to restrict To do this, we would define a function fto its “lower-frequency half.”g by the condi- tion that ˆEquivalently, we could say that ˆ$g(ξ) = f (ξ)$ˆ if|ξ| ⩽ Ω/g(ξ)2 and is 0 otherwise.= f (ξ)ˆ ˆL(ξ)$, where ^{L}(ξ) = 1 if |ξ| ⩽ Ω/2 and 0 otherwise$. The next step is to let$L Lf ((k^{n} be - L(nπ/Ω)n)π/Ω)$. To put this more neatly, if we, and we find that$g(kπ/Ω) =$ writetively, then  ̃n \in Z (an)nand  ̃b bn=for f (nπ/Ω)L a .
On the other hand, and g(nπ/Ω), respec-g clearly has band lim i tf i ces to know only the sequence of samples at integer kn\in Ω/Z 2, so to characteriz(en()k)-n g it suf- multiples of 2 know the numbersπ/Ω. In other words, we just need tob = b ̃ . The transition from fto appropriate electrical engineering vocabulary, we haveg is therefore given byk (b2()k)k = n\in Z Ln(a2()k)-n.
In the gone from a critically sampled sequence for sampling rate corresponded exactly to its bandlimit) tof (i.e., its a critically sampled sequence forg byfiltering (multiplying ˆ(f (nπ/Ω))f by some function, or convolving the sequencen\in Z with a sequence of filter coefficients) and because these are the only samples necessary to char - downsampling (retaining only one sample in two, acterize the more narrowly bandlimit edg). The upper- frequency half Fourier transform of the restriction of ˆh of f can be obtained by the inversef (ξ) to |ξ| >Ω/ized by its values at multiples of 22.
Like g, the function h is also completely character-π/Ω, and h can also be obtained from fby filtering and downsampling. This split ofsubbandsf , is thus given by formulas that are the exact into its lower and upper frequency halves, or equivalent of the generalized averaging and differ-encing encountered in the implementation of wavelet transforms for orthonormal wavelet bases supported on an interval.
Subband filtering followed by critical downsampling had been developed in the electrical engineering literature before wavelets came along, but were typically not concatenated in several stages.
A concept of central importance in quantum physics is that of alie group [III.48 §1](/part - 03/lie - theory) on some unitary representation hilbert space[IV.15 §1.4](/part - 04/operator - algebras) of a[III.37](/part - 03/bayesian - analysis). In other words, given a Lie group H, one interprets the elements g Gofand a Hilbert space G as unitary trans- formations of H. The elements of H are called states, and for certain Lie groups, ifthe family of vectors gv; gvis some fixed state, then∈ G is called a family of coherent states Schrödinger in the 1920 s. Their name dates back to.
Coherent states go back to work by the 1950 s, when they were used in quantum optics: the word “coherent” referred to the coherence of the light they were describing. These families turned out

VII.3. Wavelets and Applications

to be of interest in a much wider range of settings in quantum physics, and the name stuck, even out side the original setting of optics. In many applications it helps not to use the whole family of coherent states but only those coherent states that correspond to a certain kind of discrete subset of G. Wavelets turn out to be just such a subfamily of coherent states:
one starts with a single, basic wavelet, and the transformations that convert it (by dilation and translation) into the remaining wavelets form a discrete semigroup of such transformations. Despite the fact that wavelets synthesized ideas from all these fields, their discovery originated in another area altogether. In the late 1970 s, the geophysicist J. Morlet was working for an oil company. Dissatisfied with the existing techniques for extracting spe-cial types of signals from seismograms, he came up with an ad hoc transform that combined translations and scalings:
nowadays, it would be called a redun-dant wavelet transform. Other transforms in seismology with which Morlet was familiar involve comparing the seismic traces with special functions of the form Wm$, n(t) = w(t - nτ) \cos (mωt)$, where w is a smooth function that gently rises from 0 to 1 and then gently decays to 0 again, all within a finite interval. Several different examples of functions different scientists, are used in practice:
because thew, proposed by several functions$W^{m}$, n look like small waves (they oscillate, but have a nice beginning and end because ofare typically called “wavelets of X,” named after pro-w) they poser X for that particular in Morlet’s new ad hoc family, which he used to com-w. The reference functions pare pieces of seismic traces, were different in that they were produced from a functionof multiplying them by increasingly oscillating trigono-w by scaling instead metric functions.
Because of this, they always had the same shape, and Morlet called them “wavelets of constant shape” (see figure 13) in order to distinguish them from the wavelets of X (or Y, or Z, etc.).Morlet taught himself to work with this new transform and found it numerically useful, but had difficulty explaining his intuition to others because he had no underlying theory. A former classmate pointed him in the direction of A.
Grossmann, a theoretical physicist, who made the connection with coherent states and, together with Morlet and other collaborators, started to develop a theory for the transform in the early 1980 s. Out side the field of geophysics it was no longer necessary to use the phrase “of constant shape,” so this was quickly dropped, which annoyed geophysicists when,

861

w(t)

0 t

0 t 0 t

0 t

0 t 0 t

Figure 13 is used in practice by geophysicists, with just below it two Top: an example of a window functionw that examples ofwavelets. Bottom: a wavelet as used by Morlet, with twow(t-nτ)(ei)mt, i.e., two “traditional” geophysics translates and dilates just below it—these have constant shape, unlike the “traditional” ones. some years later, more mature forms of wavelet theory impinged on their field again. photocopy machine at his university, harmonic analy-sis expert Y.
Meyer heard about this work and real-A few years later, in 1985, standing in line for a ized it presented an interestingly different take on the scaling techniques with which he and other har-monic analysts had long been familiar. At the time, no wavelet bases were known in which the initial func-tionψ combined the properties of smoothness and good decay. Indeed, there seemed to be a subliminal expectation in papers on wavelet expansions thatno such orthonormal wavelet bases could exist.
Meyer set out to prove this, and to every one’s surprise and delight he failed in the best possible way—by finding a counterexample, the first smooth wavelet basis! Except that it later turned out not to have been the very first: a few years before, a different harmonic analyst, O. Stromberg, had constructed a different example, but this had not attracted attention at the time. of some seemingly miraculous cancellations, which is Meyer’s proof was ingenious, and worked because

862

always unsatisfactory from the point of view of mathe-mat ical understanding. Similar miracles played a role in independent constructions by P. G. Lemarié (now Lemarié-Rieusset) and G. Battle of orthonormal wavelet bases that were piecewise polynomial. (They came tothe same result from completely different points of departure—harmonic analysis for Lemarié and quan-tum field theory for Battle.) in computer vision in the United States, learned about A few months later, S. Mallat, then a Ph. D. candidate these wavelet bases.
He was on vacation, chatting on thebeach with a former classmate who was one of Meyer’s graduate students. After returning to his Ph. D. work, Mallat kept thinking about a possible connection with the reigning paradigm in computer vision. On learning that Meyer was coming to the United States in the fall of 1986 to give a named lecture series, he went to see himand explain his insight. In a few days of feverish enthusiasm, they hammered out different approach to Meyer’s construction inspired by multiresolution analysis, a the computer vision framework.
In this new setting, allthe miracles fell into place as inevitable consequences of simple, entirely natural construction rules, embody-ing the principle of successively finer approximations. Multiresolution analysis has remained the basic princi-ple behind the construction of many wavelet bases and redundant families.
to that point was supported inside an interval, so the None of the smooth wavelet bases constructed up algorithms to implement the transform (which were using the subband filtering framework with out their creators knowing that it had been named and devel-oped in electrical engineering) required, in principle, infinite filters that were impossible to implement. Inpractice, this meant that the infinite filters from the mathematical theory had to be truncated; it was not clear how to construct a multiresolution analysis that would lead to finite filters.
Truncation of the infinite filters seemed to me a blemish on the whole beautiful edifice, and I was unhappy with this state of affairs. Ihad learned about wavelets from Grossmann and about multiresolution analysis from explanations scribbledby Meyer on a napkin after dinner during a conference. In early 1987 I decided to insist on finite filters for the implementation. I wondered whether a whole multiresolution analysis (and its corresponding orthonormal basis of wavelets) could be reconstructed from appropriate but finite filters.
I managed to carry out this pro-gram, and as a result found the first construction of an

VII. The Influence of Mathematics

orthonormal wavelet basis for which supported on an interval.ψ is smooth and Soon after this, the connection with the electrical engineering approaches was discovered. Especial-ly easy algorithms were inspired by the needs of computer graphics applications. More exciting con-struc tions and generalizations followed: biorthogonal wavelet bases, wavelet packets, multiwavelets, irregularly spaced wavelets, sophisticated multidimensiona lwavelet bases not derived from one-dimensional constructions, and so on. It was a heady, exciting period.
The development of the theory benefited from all the different influ-ences and in its turn enriched the different fields with which wavelets are related. As the theory has matured, wavelets have become an accepted addition to the mathematical toolbox used by mathematicians, scien-tists, and engineers alike. They have also inspired the development of other tools that are better adapted to tasks for which wavelets are not optimal.

Further Reading

Aboufadel, E., and S. Schlicker. 1999.New York: Wiley Interscience. Discovering Wavelets. Blatter, C. 1999.Peters. Wavelets: A Primer. Wellesley, MA: AK Cipra, B. A. 1993. Wavelet applications come to the fore. SIAM News 26(7):10–11, 15. Frazier, M. W. 1999.Linear Algebra. New York: Springer. An Introduction to Wavelets through Hubbard, B. B. 1995.The Story of a Mathematical Technique in the Making The World According to Wavelets:. Meyer, Y., and R. Ryan. 1993.Wellesley, MA: AK Peters. Applications. Philadelphia, PA: Society for Industrial and Wavelets: Algorithms and Mulcahy, C. 1996.
Plotting & scheming with wavelets. Applied Mathematics (SIAM).ematics Magazine 69(5):323–43. Math VII.4 The Mathematics of Traffic in Networks Frank Kelly 1 Introduction We are all familiar with congested roads, and perhaps also with congestion in other networks such as the Internet, so it is obviously important to have a gen-eral understanding of how and why congestion occurs in networks. However, the pattern of the flow of traf-fic through a network is the consequence of a subtle VII.4. The Mathematics of Traffic in Networks and complex interaction between different users.
For example, in a road network we would normally expect each driver to attempt to choose the most convenient route, and this choice will depend upon the delays the driver expects to encounter on different roads; but these delays will in turn depend upon the choices of routes made by others. This mutual interdependence makes it difficult to predict the effects of changes to the system, such as the construction of a new road or the introduction of tolls in certain places. Related issues arise in other large-scale systems like the telephone network or the Internet.
In these systems a major practical concern is the extent to which control can be Web, the rate at which a Web page is transferred to decentralized. When you are browsing the you across the network is controlled by software proto-cols running on your computer and on the Web server hosting the Web page, and not by some huge central computer. This decentralized approach to flow control has been outstandingly successful as the Internet has evolved from a small-scale research network to today’s interconnection of hundreds of millions of hosts, but is beginning to show signs of strain.
In developing new protocols, the challenge is to understand just which aspects of decentralized flow control are important ifthe network as a whole is to continue to expand and evolve. In this article we introduce the reader to some of the mathematical models that have been used to address these issues. The models need to be able to represent several distinct aspects of the system. We shall see that the language of[I.3 §4.2](/part - 01/fundamental - definitions) is needed to capture the pattern of connec-graph theory [III.34] and matrices tions within the network.
Calculus is needed to describe how congestion depends upon traffic volumes. And optimization concepts are needed to model the wayin which self-interested drivers choose their shortest routes, or the way that decentralized controls in communication networks can cause the system as a whole to perform well. 2 Network Structure Figure 1 illustrates a set of three nodes connected by aset of five directed links. We might imagine the nodes as representing towns or locations within a city, and the links as representing road capacity between different nodes.
A two-way road is represented by two links, one in each direction. Notice that there are two routes from node c to node a that a driver can choose: the first route, 863 b c a 12⎛⎜⎜ ab ac10 10 ba01 bc ca1 ca2 cb1 cb200 00 01 00 10 ⎞⎟⎟A = 3 ⎜⎜⎜⎜⎜ 0 1 0 1 0 0 0 0 ⎟⎟⎟⎟⎟4⎝ 0 0 0 0 0 1 1 0 ⎠$5 0 0 0 0 1 0 0 1 abac$⎛⎜⎜ ab ac10 01 ba00 bc ca1 ca2 cb1 cb200 00 00 00 00 ⎞⎟⎟H = babc ⎜⎜⎜⎜⎜⎜ 00 00 10 01 00 00 00 00 ⎟⎟⎟⎟⎟⎟$ca$⎜⎝ 0 0 0 0 1 1 0 0 ⎟⎠$cb 0 0 0 0 0 0 1 1 Figure 1matrix$,$A$. The matrix A simple network and its link-route incidence H represents which routes serve which source–destination pairs.
let us call it ca1, is the direct route, using link 5; the second route, let us call it ca2, is via node b and uses links 4 and 2. of possible routes. One way to describe the relation-Let J be the set of directed links and let R be the set ship between links and routes is with a table, or defined as follows. Set A = 1 if link j lies on route matrix, r A, and set= (A , j A. njr J, r=0 otherwise. This defines a matrix\in  R) called thejr link-route incidence matrix. Each column of the matrix corresponds to one$jr$ of the routes the network.
The column for router , and each row to one of the linksr is composed ofj of 0 s and 1 s: the 1 s tell us which links are on route for the rows, the 1 s in the row for linkj tell us whichr . As routes pass through that link. Thus, for example, the incidence matrix in figure 1 has a column for each of the two routes, ca1 and ca2, between node c and node a.

864

D(y)

0 y

Figure 2 expressed as a function of the total flow The time taken to travel along a link, y along the link. D(y), As the flow increases, congestion effects cause additional delay. These columns encode the information that route ca1 uses link 5 and that route ca2 uses links 4 and 2. Note that the incidence matrix does not tell us the order ofthe links on the route. Also the incidence matrix shown does not include all logically possible routes, but it could if we wanted it to.
And while we have illustrated a very small network, there is no limit to the number of nodes and links there could be in the network, or to the number of choices of route each driver might have—the incidence matrix would just be bigger. One quantity of interest in a network is the volume of traffic along a particular route or link. Let flow on route r, defined as the number of cars per hour$x^{r} \text{be the}$ that travel along that route. We can list the flows along all the routes in the network as a sequence of numbers $x = (x$, r \in R), and we can think of this sequencer as a vector.
From this vector we can calculate the total flow through a link: for example, the total flow through link 5 in figure 1 is the sum of the flows along routes ca1 and cb2, since these are the routes that pass through link 5. In general, since through linkj and A = Ajr0 when it does not, the total = 1 when a route r passes flow through linkj, coming from all of the routes thatjr use it, is yj =r \in R Ajr xr, j \in  J.

Again, the numbers(y , j \in  J) can be thought ofj

as forming a vector. The above equations can then be represented succinctly in matrix form as y = Ax. on the total flow through the link, and we expect thisto influence the time taken to travel along the link. We We expect the level of congestion at a link to depend VII. The Influence of Mathematics shall call this time theway in which the delay might depend on the amount of delay. Figure 2 shows a typical flow. At small values of the flow just the time taken to travel along an empty road;
fory the delay D(y) is larger values ofpossibly much larger, owing to congestion effects.y the delay D(y) is larger, and quite1 through that link is Let Dj(yj) be the delay along linky; the nature of this delay mayj when the flow j depend upon characteristics of link and width, so we have to use the subscriptj such as its len gthj on the function D to indicate that the functions for thej various links can be different. 2.1 Routing Choices Given two nodes in a network there will in general be a variety of possible routes capable of linking them.
For example, in figure 1 we have seen that the incidence matrix A records two routes between nodes c and a. The pair ca is an example of a Flow originating from source c and destined for node a source–destination pair. can use either ca1 or ca2, the two routes that serve this source–destination pair. We now need another matrix, this time to describe the relationship between source–destination pairs and routes. Let us uses to denote a typical source–destination pair, and letof all source–destination pairs.
Then, for each source–S be the set destination pair can be served by the routes and each router , and letr, let H Hsr= =0 other - 1 if s wise. This defines a matrix$H = (Hsr$, ssr \in S, r \in R); figure 1 gives an example. Observe that the row labeled ca has 1 s for the two routes, r = ca1, ca2, that serve the source–destination pairs = ca. Each column of H cor- responds to a route, and contains a single 1: this identi-fies the source–destination pair served by the route. For each route pair served byr let us writer:
for example, in figure 1, s(r )for the source–destinations(ac) = ac ands(ca1 From the vector) = ca. x = (x , r \in R) we can calculate ther total flow from a source to a destination: for example, the flow from node c to node a in figure 1 is the sum of for the curve representing delay as a function of flow to bend backup on itself, so that higher delays than shown in the graph correspond1. The graph shown in figure 2 is single valued. It is quite possible to flows part of the graph when you experience stop–start driving conditions smaller than the maximum flow shown there.
You are in this on a congested but otherwise incident-free highway. Part of the aimof traffic management is to keep flows and delays away from this part of the graph, which we will not consider further. We will assume that the graph is increasing and smooth, which will make our use of calculus later more straightforward. Formally, we shall assume that D(y)is a continuously differentiable and strictly increasing function of its argument figure 2.y, as in the graph shown in VII.4.
The Mathematics of Traffic in Networks flows along routes ca1 and ca2, since from the matrix we see that these are the routes that serve the source– H destination pair ca. More generally, iffis the total flow

s

of traffic added up over all of the routes serving source–destination pair$s$, thenfs =r \in R Hsr xr$, s \in S$.

Thus the vector$f = (f$, s \in S)of source–destination

$s$

flows can be expressed succinctly in matrix form as$f = Hx$. 3 Wardrop Equilibria We are now able to approach the central issue: howdo the traffic flows between the various sources and destinations distribute themselves over the links of the network? Each driver will try to use whatever route isquickest, but this may make other routes quicker or slower and cause other drivers to change their routes. Only when they cannot find alternative, quicker routes will drivers not have an incentive to change routes.
What does this mean mathematically?Let us first calculate the time taken for a driver to travel along route A tells us which linksr . The column labeledj are on route r . If we add up ther of the matrix delays on each of these links, we get the time taken to travel along router as the expression Dj(yj)Ajr .j\in J

Now the driver using route other route that served the same source–de st in at i onr could have used any pairr , we requires(r ). So, for the driver to be content with route for every other route destination pair$j \in^{J} D^{j}s(r )(y^{j}$.)Arjr^that serves the same source–$⩽^{j} \in^{J} D^{j}(y^{j})A^{j}r$ a vector Define ax Wardrop equilibrium= (x , r \in  R) of nonnegative numbers(Wardrop 1952) to ber

such that for every pair of route sr , r^ serving the same source–destination pair, $x^{r} > 0 ⇒^{j} \in^{J} D^{j}(y^{j})A^{j}r ⩽^{j} \in^{J} D^{j}(y^{j})A^{j}r$, where$y = Ax$. The inequality expresses the defining characteristic of a Wardrop equilibrium: that if a router is actively used, then it achieves the minimum delay over all routes serving its source–destination pair Does a Wardrop equilibrium exist? It is not at alls(r ). clear whether it is possible to find a vectorx such

865

that all of the above inequalities, for the various routes through the network, are satisfied simultaneously. To answer the question, we shall proceed by addressing a seemingly different question: what is the answer to the following optimization problem? Minimize yj D (u)duj

over$x^{j} \in ⩾^{J} 0^{0}$, y,

subject to$Hx = f$, Ax = y.

Let us see in out line why this optimization problem hasa solution(x, y), and why, if (x, y) is a solution, the vector The optimization problem has some aspects that arex is a Wardrop equilibrium. quite natural. An obvious constraint is that the flows along each route are nonnegative, which is why we insist thatx ⩾ 0. The constraints Hx = f , Ax = y just enforce the accounting rules we have seen earlier—the rules that allow the source–destination flowsf and the link flows using the matrices yto be calculated from the route flows H and A, respectively.
We view thex source–destination flows fas fixed, to be distributed over the various routes. Given a choice ofis then to find the route flowsx and consequently thef , our task link flowsy will be nonnegative, sincey . At a solution to the optimization problemx is. This much is fairly natural, but the function to be minimized looks some what strange. Its importance rests on the fact that the rate of change of the integral yj Dj(u) du

with respect to theorem of calculus$y^{j} is^{0}D$[I.3 §5.5](/part-01/fundamental-definitions), and the function to be$j(y^{j})$, by the fundamental minimized is the sum of these integrals over all links. We shall see that the link between Wardrop equilibria and the optimization problem is a direct consequence of this observation. To find a solution to the optimization problem, we will use the method of Define the function lagrange multipliers [III.64](/part-03/optimization-and-lagrange-multipliers).

L(x, y;λ, \mu)=y j D (u) du + λ · (f - Hx) - \mu · (y - Ax), jj\in^J^0

where Lagrange multipliers, to be fixed later. The idea is that ifλ = (. ambda s$, s \in S)$, \mu = (\mu j, j \in  J) are vectors of we make the right choices of Lagrange multipliers, the minimization of the function solution to the original problem. The reason this works L over x and ywill find a

866

is that, for the right choices of Lagrange multipliers, the constraints Hx = f and Ax = y are consistent with the minimization of To minimize the function L. Lwe need to differentiate. First,. artial L. artial y = Dj(yj) - \mu j.j

Second,

$\partial L\partial x^{r} = −λ^{s}(r ) + j \in^{J} \mu^{j} A^{j}r$.

Note that the form of the matrix with respect toxr to pick out exactly one component H causes the derivative ofthe derivative to pick out just those components ofλ$, namely λ^{s}(r )$, and the form of the matrix A causes\mu that correspond to links on route allow us to deduce that a minimum ofr . These derivatives L, over all x ⩾ 0 and all$y$, occurs when\mu j = Dj(yj) and . ambda s(r ) =j\in J \mu j Ajr if xr > 0⩽ \mu j Ajr if xr = 0.

The equality condition forx > 0 then small variations up or down inj\in J . ambda s(r )is straightforward: ifx should not decrease the functionr L(x, y;λ, μ)$, \text{and hence we}^{r}$ deduce that the partial derivative with respect tomust be zero. But ifx = 0 then we can only vary xxr upward, and so all we can deduce is that the partial derivative with respect to$r x^{r} \text{is nonnegative}$, and fromr this we deduce the inequality condition for Minimizing the function$L \text{corresponds to allowing}λ^{s}(r )$. the constraints Hx = f , Ax = y to be violated, but at a cost:
now one charges a price. ambda for any shortfall ofs

the sumj\in J Ajr xr below fs and a price \mu j for any excess of the sum results on convex optimization it is known that there$j \in^{J} A^{j}rx^{r} over y^{j}$. From general exist Lagrange multipliers such that(x, y) minimizes(λ, μ)L(x, yand a vector;λ, μ), satisfies the(x, y) constraints Hx = f , Ax = y, and solves the original optimization problem. Our solution for the Lagrange multipliers shows that they have a simple interpretation:\mu is the delay on linkjj and . ambda is the minimum delay over all routes servings

the node pairs. The various conditions established for the multipliers thus show that an optimum of the func-tion L, known as the objective function, corresponds precisely to a Wardrop equilibrium.

VII. The Influence of Mathematics

accordance with the self-interested choices of drivers, the equilibrium flows Thus if traffic in the network distributes itself in(x, y) will solve an optimiza- tion problem. This result is originally due to Beckmannet al. (1956), and it provides a remarkable insight into the equilibrium patterns achieved in road traffic networks.
The pattern of traffic resulting from the indi-vidual decisions of a large number of self-interested drivers behaves as if a central intelligence were direct-ing flows to optimize a certain (rather strange) objective function. The result does not mean that average delays in the network will be minimal: a striking illustration ofthis fact is provided by Braess’s paradox (Braess 1968), which we describe next. 4 Braess’s Paradox Consider the network illustrated in figure 3(a). Cars travel from node S to node N, via either node W or node E. The total flow is 6, and the link delays D (y)are givenj

next to the links in the figure. One can imagine the fig-ure illustrating rush hour as commuters travel from the center of a city in the south to their homes in the north. Commuters learn from experience what the delays are likely to be along the eastern and western routes. the distribution of traffic shown is the Wardrop equilibrium: there is no incentive for any drivers to change their routes, since the two possible routes incur the same delay, namely(10 . imes 3)+(3 + 50) = 83 units of time. Now suppose that a new link is added, between nodes W and E, as shown in figure 3(b).
Traffic is attracted onto the new link, since to begin with it offers a shorter journey time from the south to the north. Eventually, after every one knows about the new link and traffic patterns have settled down, a new Wardrop equilibrium will be established, and this is shown in figure 3(b). In the new equilibrium there are three routes used, which each incur the same delay, namely$(10 \times 4)+ (2 + 50) =(10 \times 4) + (2 + 10) + (10 \times 4) =$92. Thus in figure 3(b) each car incurs a delay of 92, while in figure 3(a) the delay of each car was only 83.
Adding the new link has increased every one’s delay! lows. At a Wardrop equilibrium each driver is using The explanation for this apparent paradox is as fola route which, given the choices of others, gives the minimum delay over the routes available between that driver’s source and destination. But there is no intrinsic reason why this equilibrium should correspond to par-ticular ly low delays relative to what could be achieved

VII.4. The Mathematics of Traffic in Networks

(a)

N

y + 50 10 y

W E

10 y y + 50

S

Figure 3 journey time to lengthen. (After Braess (1968) and Cohen (1988).)Braess’s paradox. The addition of a link causes every one’s by another flow pattern. If all drivers could be encouraged to depart from their own self-interested choices, it is quite possible that all might benefit. And in the above example, if all drivers in the second network could agree to avoid the new link, effectively converting the network back into the first network, then all would incur lower delays.
To explore the point further, note that the product of the flowat linkj yper unit time, aggregated over all the vehiclesj and the delay Dj(yj) is the delay incurred using link imizes the total delay per unit time, summed over the$j$. Let us try to find the flow pattern that min entire network. Consider then the following problem. Minimize$j \in^{J} y^{j}D^{j}(y^{j})$ over$x ⩾ 0$, y, subject to$Hx = f$, Ax = y. Note that the problem is of the same form as the earlier optimization problem, but the function to be minimized now measures the total network delay per unit time.
(Recall that the function to be minimized in the first optimization problem seemed initially to be rather arbitrary, with its eventual motivation being that its minimization was achieved by a Wardrop equilibrium.)

867

(b)

N

y + 50 10 y

W y + 10 E

10 y y + 50

S

Again define the function

L(x, y;λ, \mu)=j \in J yj Dj(yj) + λ · (f - Hx) - \mu · (y - Ax).

Again

$\partial L\partial x^{r} = −λ^{s}(r ) + j \in^{J} \mu^{j} A^{j}r$,

but now

$\partial L\partial y = D^{j}(y^{j}) + y^{j} D^{j}(y^{j}) - \mu^{j}$.j

Hence a minimum of L over x ⩾ 0 and y occurs when\mu j = Dj(yj) + yj Dj(yj)

and

. ambda s(r ) =j\in J \mu j Ajr if xr > 0⩽j\in J \mu j Ajr if xr = 0.

cated interpretation. Suppose that, in addition to the delay The Lagrange multipliers now have a more sophisti-D (y ), users of link jincur a traffic-dependent

$j^{j}$

toll

$T^{j}(y^{j}) = y^{j} D^{j} (y^{j})$.$868 Then$\mu is the generalized cost of using link j, defined

$j$

as the sum of the toll and the delay, andmum generalized cost over all routes serving the node$λ^{s} \text{is the mini}-$ pairmize the sum of their tolls and their delays, then theys. If users select routes in an attempt to mini- will produce a flow pattern that minimizes total delay in the network. Notice that the generalized cost\mu isj(∂/∂ytotal delay at linkj)(yj D(yj))j, which is the rate of increase in theas the flowy is increased. So thej

assumption now is that, in a certain sense, drivers try to minimize their contribution to the total delay rather than minimizing their own delay. their own delay, then the resulting equilibrium flows will minimize a certain objective function defined for We have seen that if drivers attempt to minimize the network. However, the objective function is cer-tainly not the total network delay, and thus there is no guarantee that when capacity is added to a network the situation is improved.
We have also seen that, with the imposition of appropriate tolls, it is possible for the self-interested behavior of drivers to lead to an equilibrium pattern of flow that minimizes total delay. A major challenge for governments and transport planners is to understand how insights from these and more sophisti-cated models might be used to encourage more efficient development and use of road networks (Department for Transport 2004).
5 Flow Control in the Internet When a file is requested over the Internet, the com-puter that hosts that file breaks it into small packets of data that are then transferred across the network by the transmission control protocol of the Internet, or TCP. The rate at which packets enter the network is con-trolled by TCP, which is implemented as software on the two computers that are the source and destination of the data. The general approach is as follows (Jacobson 1988). When a link within the network becomes overloaded, one or more packets are lost;
loss of a packet is taken as an indication of congestion, the desti-nation in forms the source, and the source slows down. TCP then gradually increases its sending rate until it again receives an indication of congestion. This cycle of increase and decrease enables the source computersto discover and use the available capacity, and to share it between different flows of packets. TCP has been outstandingly successful as the Internet has evolved from a small-scale research network to today’s interconnection of hundreds of millions of endpoints and links. This in itself is a striking observation.
VII. The Influence of Mathematics Each of a large but indeterminate number of flows is controlled by a feedback loop that can know only of that flow’s experience of congestion. A flow does not know how many other flows are sharing a link on its route, or even how many links are on its route. The links vary in capacity by many orders of magnitude, as do the numbers of flows sharing different links. It is remarkable that so much has been achieved in such a rapidly growing and heterogeneous network with congestion controlled just at the endpoints. Why does this algorithm work so well?
TCP’s success, by interpreting the protocol as a decen-tralized parallel algorithm that solves an optimization In recent years theoreticians have shed some light on problem, just as the decentralized choices of drivers ina road network solve an optimization problem. We shall out line the argument, beginning with a more detailed description of TCP.2 tainshould arrive at their destination in that order. When Packets transferred by TCP across the Internet con-sequence numbers indicating their order, and they a packet is received at the destination, it is acknow - ledged:
an acknowledgment is a short packet sent by the destination back to the source. If a packet has been lost in the transfer, the source can tell this from the sequence numbers contained in the acknowledgments. The source keeps a copy of each packet sent until it has been positively acknowledged; these copies form what is called a sliding window, and allow packets lost in transfer to be sent again by the source. Meanwhile, stored in the source computer there is a numerical variable known as theand denoted cwnd.
The congestion window directs the congestion window size of the sliding window in the following sense: if the size of the sliding window is less than cwnd, then the computer increases it by sending out a packet; if it is greater than or equal to cwnd, then it waits for positive acknowledgments to come in, which have the effect ofreducing the size of the sliding window and, as we shall see, increasing window continually changes, moving in the direction ofcwnd as well.
Thus, the size of the sliding a target size that is given by the congestion window. The congestion window itself is not a fixed number: rather, it is constantly being updated, and the precise rules for how this is done are critical for TCP’s sharing of capacity. The rules currently used are as follows. just the congestion-avoidance part of the protocol and omitting dis-cussion of timeouts or of reactions to multiple congestion indication2. Even our detailed description of TCP is simplified, concerning signals received within a single round-trip time. VII.4.
The Mathematics of Traffic in Networks Every time a positive acknowledgment comes in, is increased by cwnd - 1, and every time a lost packet cwnd is detected, cwnd is halved.3 Thus, if the source computer detects a lost packet, it realizes that there has been some congestion and backs off for a while, but if all its packets are getting through then it allows the rate at which it sends packets to inch up again. probability 1 by Ifcwndp is the probability that a packet is lost, then with-1 and with probability - p the congestion window will increasep it will decrease by 12 cwnd.
The expected change in the congestion window cwnd per update step is therefore cwnd-1(1 - p) -1 2 cwnd p. The expected change will be positive for small values of cwnd, but will become negative if cwnd is big enough. We might therefore expect an equilibrium for arise when the expression is zero: that is, when cwnd to cwnd$= 2(1p - p)$. to networks. Suppose that a network consists of a setof nodes connected by directed links, like the net-Now let us see how this calculation can be extended work illustrated in figure 1.
As earlier, letof directed links, let R be the set of routes, and let J be the set Amatrix. When a request reaches a computer in this net-= (Ajr , j \in  J, r \in  R) be the link-route incidence work, that computer will set up a congestion window for the flow of packets that will result. Since there willbe many different such congestion windows, they need to be labeled, and it is convenient to label them with the route that will be used for the flow.
(Exactly how these flows are routed is a complicated and important ques-tion, but one that we shall not discuss here.) So, for each route window for that route. Letr that is being used, let T be the cwnd rr ou nd-trip timebe the congestion forr

the route of a packet and the receiving of an acknowledgment for$r$: that is, the time between the sending out it.4 Finally, define a variable$x^{r} \text{to be cwnd}^{r}/T^{r}$. and indeed it is only recently that many of their macroscopic conse-quences have begun to be understood. The rules have worked well3. These increase and decrease rules may appear rather mysterious, for more than a decade, but they are now beginning to show signs of age, and much current research is aimed at understanding the full consequences of changing them.4.
The round-trip time comprises the time taken for a packet to travel along links, called the propagation delay, together with pro-cess ing times and queueing delays at nodes. Processing times and queueing delays tend to decrease with increasing computer speeds, but the finite speed of light places a fundamental lower bound on propagation delays. We shall treat the round-trip time for a route as a constant. Hence, we assume that congestion at a link makes itself felt by packet loss rather than additional packet delay.

869

of those packets that have been sent but not acknow-ledged. Therefore, if a packet has just been acknow-Now at any given time the sliding window consists ledged and its round-trip has taken time T , the slid-r

ing window consists of all packets sent out in the last Tr time units. Since the source computer is aiming for the number of such packets to be about interpretx to be the rate at which packets are trans-cwndr, we canr

ferred over router . Thus, the numbers xform a flow

r

vector that is closely analogous to the traffic flow vector discussed earlier. As we did then, let us define a vectory = Ax, so thaty(xj)ris the total flow through link over each router that passes through linkj, obtained by sum mi ngj. Let pj be the proportion of packets that are lost, or “dropped,”at linkj. We expect p to be related to y , the total flow through linkj, as follows. Ifj y is less than thejj

capacity$C^{j} \text{of link} j$, then pjwill be zero; there will be no dropped packets at linkif$p > 0 then y = C$; if packets are dropped then thej if the link is not full. And link is full. If we assume that the proportions of packets dropped at links are small, then the probability that a(jj)j packet is lost on router is approximate lypr =j \in J pj Ajr.

(The exact formula would bebut when thep are small we can ignore their products.)(1-pr) =j\in J(1-pj()A)jr , j

Since now gives us thatxr = cwndr /Tr, our earlier calculation of cwndxr = T1 r 2(1 p-rpr ).

Is it possible to choose the rates$x = (x$, r \in R) andr

the drop probabilities$p = (p$, j \in J) in a consist entj

fashion, so that the last two equations are satisfied andeitherp is zero or y = C for each j \in  J? The remarkable observation is that such a choice corresponds pre-cisely to the solution of the following optimization(jj)j$problem (Kelly 2001$; Low et al. 2002).$\sqrt{}$ Maximize$r \in^{R} T^{r}2 arctan x \sqrt{}r T2^{r}$ over$x ⩾ 0$, subject to Ax ⩽ C. might expect: in particular, the inequality ply adds up the flows through link Some aspects of this optimization problem are as wej and requires that Ax ⩽ C sim- the sum not exceed the capacity linkj \in  J.
But, as before, the function being optimized Cj of link j, for each is undoubtedly strange. The arctan function, illustrated

870

arctan(x)

$0$

x

Figure 4 itly maximizes a sum of utilities over all the connections The arctan function. The Internet’s TCP im pl ic present in a network: this function shows the shape of the utility function for a single connection. The horizontal axis is proportional to the rate of the connection, and the vertical axis is proportional to the usefulness of that rate. Both axes are scaled in terms of the round-trip time of the connection. in figure 4, is the inverse function to the trigonometric function tan, and can also be defined as arctan$(x) =0^{x} 1 + 1u^{2} du$.
From this form, we see that its derivative with respect to$x is 1/(1 + x^{2})$. Let us sketch the relationship between the optimization problem and the equilibrium rates and drop probabilities. Define the function

L(x, z;μ). qrt=r\in R T2 r arctan x. qrtr T2 r + μ · (C - Ax - z)$, where\mu = (\mu$, j \in  J) is a vector of Lagrange multi-j

pliers, andz = C - Ax is a vector of slack variables, measuring the spare capacity on each of the linksj \in J of the network. Then, using the derivative of the arctan function, . artial x. artial Lr = (1 +1 2(x2)r (Tr)2)-1 -j \in J \mu j Ajr and . artial z. artial Lj = −\mu j. We look for a maximum ofthat this maximum is, under the identification L over x, z ⩾$0$;
it turns out$\mu = p$, precisely the collection rates and drop probabilities that we were looking for.$(x^{r}$, r \in  R), (pj$, j \in^{j} J) of^{j}$ For example, setting to zero the partial derivative with respect tox gives the desired equation for x . In summary, for each linkr j \in J the Lagrange multi-r pliercisely the proportion\mu j arising from the optimization problem is pre-pj of packets dropped at that link, much as the Lagrange multipliers arising earlier were precisely the delays on links of a road traffic network.
And the equilibrium reached by the interaction of many competing TCPs, each implemented only on the source and destination computers, is effectively maximizing

VII. The Influence of Mathematics

an objective function for the entire network. The objec-tive function has a surprising interpretation: it is as if the usefulness of the flow ratexto the sourcer destination pair served by this route is given by a function$\sqrt{utility}T2 arctan x \sqrt{}r T2^{r}$, r

and the network is attempting to maximize the sumof these utility functions across all source–destination pairs, subject to constraints arising from the limited capacities of the links. The arctan function, illustrated in figure 4, is concave. Thus, if two or more connections share an overloaded link, the rates achieved will be approximately equal, since otherwise the total utility could be increased byreducing the largest rate a little and increasing the smallest rate a little. As a result, there is a tendency for TCP to share resources more or less equitably.
This is very different from resource-control mechanisms in traditional telephone networks where, if the network is overloaded, some calls are blocked in order that the calls that are accepted are unaffected by the overload. 6 Conclusion The behavior of large-scale systems has been of great interest to mathematicians for over a century, with many examples coming from physics. For example, the behavior of a gas can be described at the microscopic level in terms of the position and velocity of each molecule.
At this level of detail a molecule’s velocity appears as a random process, as the molecule bounces around off other molecules and the walls of the container. Yet consistent with this detailed microscopic description of the system is macroscopic behavior best described by quantities such as temperature and pres-sure. Similarly, the behavior of electrons in an electrical network can be described in terms of random walks, and yet this simple description at the micro-scopic level leads to rather sophisticated behavior at the macroscopic level:
Kelvin showed that the pattern of potentials in a network of resistors is exactly the one that minimizes heat dissipation for a given level of current flow (Kelly 1991). The local, random behavior of the electrons causes the network as a whole to solvea rather complex optimization problem. large-scale engineered systems are often best under-In the last fifty years we have begun to realize that stood in similar terms. Thus a microscopic description of traffic flow in terms of each driver’s choice of the

VII.5. The Mathematics of Algorithm Design

most convenient route can be consistent with macro-scopic behavior described in terms of a function minimization. And the simple, local rules that control how packets are transmitted through the Internet can cor-respond to a maximizing of aggregate utility across the entire network. the microscopic rules governing physical systems are fixed, for engineered systems such as transport or com-One thought-provoking difference is that, where as munication networks we may be able to choose the microscopic rules so as to achieve the macroscopic consequences we judge desirable.

Further Reading

Beckmann, M., C. B. Mc Guire, and C. B. Winsten. 1956.ies in the Economics of Transportation. Cowles Commis-Stud Braess, D. 1968. Über ein Paradox on aus der Verkehrspla-sion Monograph. New Haven, CT: Yale University Press.nung. unternehmenfors chung 12:258–68. Cohen, J. E. 1988. The counter intuitive in conflict and cooperation. American Scientist 76:576–84. Department for Transport. 2004. Feasibility study of road pricing in the UK. Available from www.dft.gov.uk. Jacobson, V. 1988. Congestion avoidance and control.puter Communication Review 18(4):314–29. Com Kelly, F. P. 1991.
Network routing.tions of the Royal Society of London Philosophical Trans ac-A 337:343–67. Mathematics Unlimited—2001 and Beyond Engquist and W. Schmid, pp. 685–702. Berlin: Springer.. 2001. Mathematical modeling of the Internet. In, edited by B. Low, S. H., F. Paganini, and J. C. Doyle. 2002. internet congestion control. IEEE Control Systems Magazine 22: Wardrop, J. G. 1952. Some theoretical aspects of road traffic28–43.research. Proceedings of the Institute of Civil Engineers 1: 325–78. VII.5 The Mathematics of Algorithm

Design

Jon Kleinberg

1 The Goals of Algorithm Design

When computer science began to emerge as a subject at universities in the 1960 s and 1970 s, it drew some amount of puzzlement from the practitioners of more established fields. Indeed, it is not initially clear why computer science should be viewed as a distinct aca-demic discipline. The world a bounds with novel technologies, but we do not generally create a separate field around each one; rather, we tend to view them

871

as by-products of existing branches of science and engineering. What is special about computers? Viewed in retrospect, such debates highlighted an important issue: computer science is not so much about the computer as a specific piece of technology as it is about the more general phenomenon of com put a-tion itself, the design of processes that represent and manipulate information. Such processes turn out to obey their own inherent laws, and they are performed not only by computers but by people, by organizations, and by systems that arise in nature.
We will refer to these computational processes as algorithms. For the purposes of our discussion in this article, one can think of an algorithm informally as a step-by-step sequenceof instructions, expressed in a stylized language, for solving a problem. This view of algorithms is general enough to capture both the way a computer processes data and the way a person performs calculations by hand. For exam-ple, the rules for adding and multiplying numbers that we learn as children are algorithms; the rules used byan airline company for scheduling flights constitute an algorithm;
and the rules used by a search engine like Google for ranking Web pages constitute an algorithm. It is also fair to say that the rules used by the human brain to identify objects in the visual field constitute akind of algorithm, though we are currently a long way from understanding what this algorithm looks like or how it is implemented on our neural hardware. A common theme here is that one can reason about all these algorithms with out recourse to specific com-puting devices or computer programming languages, instead expressing them using the language of mathematics.
In fact, the notion of an algorithm as we now think of it was formalized in large part by the work of mathematical logicians in the 1930 s, and algorithmic reasoning is implicit in the past several millennia of mathematical activity. (For example, equation-solving methods have always tended to have a strong alg or ith-mic flavor; the geometric constructions of the ancient Greeks were inherently algorithmic as well.) Today, the mathematical analysis of algorithms occupies a central position in computer science;
reasoning about algorithms independently of the specific devices on which they run can yield insight into general design principles and fundamental constraints on computation. At the same time, computer-science research struggles to keep two diverging views in focus: this more abstract view that formulates algorithms mathemat-ically, and the more applied view that the public

872

generally associates with the field, the one that seeks to develop applications such as Internet search engines, electronic banking systems, medical imaging software, and the host of other creations we have come to expect from computer technology. The tension between these two views means that the field’s mathematical formulations are continually being tested against their imple-mentation in practice; it provides novel avenues for mathematical notions to influence widely used appli-ca tions;
and it some times leads to new mathematical problems motivated by these applications. The goal of this short article is to illustrate this balance between the mathematical formalism and the motivating applications of computing. We begin by building up to one of the most basic definitional ques-tions in this vein: how should we formulate the notion of efficient computation?
2 Two Representative Problems To make the discussion of efficiency more concrete, andto illustrate how one might think about an issue like this, we first discuss two representative problems—both fundamental in the study of algorithms—that are similar in their formulation but very different in their computational difficulty. The first in this pair is the traveling salesman problem (TSP), which is defined as follows. We imagine a salesman contemplating a map withrently located in one of them).
The map gives the dis-n cities (he is cur- tance between each pair of cities, and the salesman wishes to plan the shortest possible tour that visits all cities and returns to the starting point. In other words,$n$ we are seeking an algorithm that takes as input the setof all distances among pairs of cities, and produces a tour of minimum total length. Figure 1(a) depicts the optimal solution to a sample input instance of the TSP;
the circles represent the cities, the dark lines (with lengths labeling them) connect cities that the salesman visits consecutively on the tour, and the lighter lines connect all the other pairs of cities, which are not visited consecutively. problem with access to the same map of A second problem is the(MSTP). Here we imagine a construction firm minimum spanning treen cities, but with a dif- ferent goal in mind.
They wish to build a set of roads connecting certain pairs of the cities on the map, so that after these roads are built there is a route from each of the here is that each road must go directly from one cityn cities to each other one. (A key point

VII. The Influence of Mathematics

(a)

8 12

(b)

Figure 1 problem and (b) the minimum spanning tree problem, on Solutions to instance of (a) the traveling salesman the same set of points. The dark lines indicate the pairs of cities that are connected by the respective optimal solutions, and the lighter lines indicate all pairs that are not connected. to another.) The goal is to build such a road network as cheaply as possible—in other words, using as little total road material as possible.
Figure 1(b) depicts the optimal solution to the instance of the MSTP defined by the same set of cities used for part (a).Both of these problems have a wide range of practical applications. The TSP is a basic problem concerned with sequencing a given set of objects in a “good” order;
it has been used for problems that run from planning the motion of robotic arms drilling holes on printed circuit boards (where the “cities” are the locations where the holes must be drilled) to ordering genetic markers on a chromosome in a linear sequence (with the markers constituting the cities, and the distances derived from probabilistic estimates of proximity). The MSTP is a basic issue in the design of efficient communica-tion networks;
this follows the motivation given above, with fiber-optic cable acting in the role of “roads.” The MSTP also plays an important role in the problem of clustering data into natural groupings. Note, for example, how the points on the left-hand side of figure 1(b)are joined to the points on the right-hand side by a relatively long link; in clustering applications, this can

VII.5. The Mathematics of Algorithm Design

be taken as evidence that the left and right points form natural groupings. It is not hard to come up with an algorithm for solving the TSP. We first list every possible way of ordering the cities (other than the starting city, which is fixed in advance). Each ordering defines a tour—the sales-man could visit the cities in this order and then return to the start—and for each ordering we could compute the total length of the tour, by traversing the cities inthis order and summing the distances from each city to the next.
As we perform this calculation for all pos-sible orders, we keep track of the order that yields the smallest total distance, and at the end of the process we return this tour as the optimal solution. While this algorithm does solve the problem, it is extremely inefficient. There aren - 1 cities other than the starting point, and any possible sequence of them defines a tour, so we need to consider$(n - 1)(n - 2)(n - 3) · · · (3)(2)(1) = (n - 1)$! possible tours. Even forn = 30 cities, this is an astronomically large quan- tity;
on the fastest computers we have today, running this algorithm to completion would take longer than the life expectancy of the Earth. The difficulty is that the algorithm we have just described is performing abrute-force search: the “search space” of possible solutions to the TSP is very large, and the algorithm is doing nothing more than plowing its way through this entire space, considering every possible solution.
algorithm that simply performs a brute-force search. Things tend to get interesting when one finds a way to For most problems, there is a comparably inefficient improve significantly on this brute-force approach. The MSTP provides a nice example of how such an improvement can happen. Rather than considering all possible road networks on the given set of cities, suppose we try the following myopic, “greedy” approachto the MSTP. We sort all the pairs of cities in order of increasing distance, and then work through the pairs in this order.
When we get to a pair of cities, say Aand B, we test if there is already a way to travel from A to B in the collection of roads constructed thus far. If there is, then it would be superfluous to build a direct road from A to B—our goal, remember, is just to make sure every pair is connected byand A and B are already connected in this case. But some sequence of roads, if there is no way to get from A to B using what has already been built, then we construct the direct road from A to B.
(As an example of this reasoning, note that the potential road of length 14 in figure 1(a) would notget built by this MSTP algorithm; by the time this direct

873

route is considered, its endpoints are already joined bythe sequence of two shorter roads of length 7 and 11, as depicted in figure 1(b).) should have the minimum possible cost, but in fact this It is not at all obvious that the resulting road network is true. In other words, one can prove a theorem that says, essentially, “On every input, the algorithm just described produces an optimal solution.” The payoff from this theorem is that we now have a way to compute an optimal road network by an algorithm that ismuch, much more efficient than brute-force search:
it simply needs to sort the pairs of cities by their distances, and then make a single pass through this sorted list to decide which roads to build. insight into the nature of the TSP and the MSTP. Rather than experimenting with actual computer programs, we This discussion has provided us with a fair amount of described algorithms in words, and made claims about their performance that could be stated and proved as mathematical theorems. But what can we abstract from these examples if we want to talk about computational efficiency in general?
3 Computational Efficiency Most interesting computational problems share the fol-lowing feature with the TSP and the MSTP: an input of size nimplicitly defines a search space of possible solutions whose size grows exponentially with One can appreciate this explosive growth rate as fol - n. lows: if we simply add one to the size of the input, the time required to search the entire space increases by a multiplicative factor. We would prefer algorithms to scale more reasonably:
their running times should only increase by a multiplicative factor when the input itself increases by a multiplicative factor. Running times that are bounded by a polynomial function of the input size—in other words, proportional to fixed power—exhibit this property. For example, if ann raised to some algorithm requires at most siz en, then it requires at mostn2 (steps on an input of2 n)2 = 4 n2 steps on an input twice as large. entists in the 1960 s adopted ing definition of efficiency:
an algorithm is deemed to In part because of arguments like this, computer sci-polynomial time as a workbe efficient if the number of steps it requires on an input of sizen grows like nraised to a fixed power. Using the concrete notion of polynomial time as a surrogate for the fuzzier concept of efficiency is the kind of modeling decision that ultimately succeeds 874 or fails based on its utility in guiding the develop-ment of real algorithms. And in this regard, polynomial time has turned out to be a definition of surprising power in practice:
problems for which one can develop a polynomial-time algorithm have turned out in general to be highly tractable, while those for which welack polynomial-time algorithms tend to pose serious challenges even for modest input sizes. provides a further benefit: it becomes possible to pose, in a precise way, the conjecture that certain problems A concrete mathematical formulation of efficiency cannot be solved by efficient algorithms. The TSP is a natural candidate for such a conjecture;
after decades of failed attempts to find an efficient algorithm forthe TSP, one would like to be able to prove a theorem that says, “There is no polynomial-time algorithm that finds an optimal solution to every instance of the TSP.” A theory known as np - completeness [IV.20 §4](/part - 04/computational - complexity) provides a unifying framework for thinking about such questions; it shows that a large class of computational problems, containing literally thousands of naturally arising problems (including the TSP), are equivalent with respect to polynomial-time solvability:
there is an efficient algorithm for one if and only if there is anefficient algorithm for all. It is a major open problem to decide whether or not these problems have efficient algorithms; the deeply held sense that they do not has become the P versus NP conjecture, which has begun to appear on lists of the most prominent problems in mathematics. ematically precise, polynomial time as a definition of efficiency in practice begins to break down around Like any attempt to make an intuitive notion math its boundaries.
There are algorithms for which onecan prove a polynomial bound on the running time, but which are hopelessly inefficient in practice. Con - versely, there are well-known algorithms (such as the standard simplex method [III.84](/part - 03/the - simplex - algorithm) for linear programming) that require exponential running time on certain pathological instances, but which run quickly on almost all inputs encountered in real life. And for computing applications that work with massive data sets, an algorithm with a polynomial running time may not be efficient enough;
if the input is a trillion bytes long (ascan easily occur when dealing with snapshots of the Web, for example), even an algorithm whose running time depends quadratically on the input will be unusable in practice. For such applications, one generally needs algorithms that scale linearly with the size of the input—or, more strongly, that operate by “stream - VII. The Influence of Mathematics ing” through the input in one or two passes, solving the problem as they go.
The theory of such streaming algorithms is an active topic of research, drawing on tech-niques from information theory, Fourier analysis, and other areas. None of this means that polynomial timeis losing its relevance to algorithm design—it is still the standard benchmark for efficiency—but new com-puting applications tend to push the limits of current definitions, and in the process raise new mathematical problems.
4 Algorithms for Computationally Intractable Problems In the previous section we discussed how researchers have identified a large class of natural problems, including the TSP, for which it is strongly believed that noefficient algorithm exists. While this explains our difficulties in solving these problems optimally, it leaves open a natural question: what should we do when actually confronted by such a problem in practice?There are a number of different strategies for approaching such computationally intractable problems. One of these is approximation:
for problems like the TSP that involve choosing an optimal solution from among many possibilities, we could try to formulate an efficient algorithm that is guaranteed to produce a solution almost as good as the optimal one. The design of such approximation algorithms is an active area ofresearch; we can see a basic example of this process by considering the TSP. Suppose we are given an instanceof the TSP, specified by a map with distances, and we set ourselves the task of constructing a tour whose total length is at most twice that of the shortest tour. At first this goal seems a bit daunting:
since we do not know how to compute the optimal tour (or its length), how will we guarantee that the solution we produce is short enough? It turns out, however, that this can be doneby exploiting an interesting connection between the TSP and the MSTP, a relationship between the respec-tive optimal solutions to each problem on the same set of cities. Consider an optimal solution to the MSTP on the given set of cities, consisting of a network of roads; recall that this is something we can compute efficiently.
Now, the salesman interested in finding a short tour for these cities can use this optimal road network to visit the cities as follows. Starting at one city, he follows roads until he hits a dead end, that is, a city with no new roads exiting it. He then backs up, retracing his steps until he gets to a junction with a road he has not yet VII.5. The Mathematics of Algorithm Design taken, and he proceeds down this new road. For exam - ple, starting in the upper left corner of figure 1(b), the salesman would follow the road of length 8 and then choose one of the roads of length 10 or 20;
if he selects the former, then after reaching the dead end he would back up to this junction again and continue the tour by following the road of length 20. A tour constructed inthis way traverses each road twice (once in each direction), so if we letin the optimal MSTP solution, we have found a tour ofm denote the total length of all roads length 2 m. possible tour? Let us first argue that true because, in the space of all possible solutions to How does this compare tot, the length of the bestt ⩾ m.
This is the MSTP, one option is to build roads between cities that the salesman visits consecutively in the optimal TSP tour, for a total mileage ofis the total length of the shortest possible$t$; on the other hand, road network,$m$ and hencecluded that the optimal solution to the TSP has len gtht cannot be smaller than m. So we have con- at leastrithm that finds a tour of length 2 m. However, we have just exhibited an algo-m, so, as we wanted, we have an efficient way to find a tour that is at most twice as long as the shortest one possible.
tion ally hard problems in practice frequently use algo-rithms that have been observed empirically to give People trying to solve large instances of com put an early optimal solutions, even when no guarantees on their performance have been proved. Local-search algorithms form one widely used class of approaches like this. A local-search algorithm starts with an initial solution and repeatedly modifies it by making some “local”change to its structure, looking for a way to improve its quality.
In the case of the TSP, a local-search algo-rithm would seek simple improving modifications to its current tour; for example, it might look at sets of cities that are visited consecutively and see if visiting them in the opposite order would shorten the tour. researchers have drawn connections between local-search algorithms and phenomena in nature; for example, just asa large molecule contorts itself in space trying to find a minimum-energy conformation, we can imagine the TSP tour in a local-search algorithm modifying itself as it tries to reduce its length.
Determining how deeply this analogy goes is an interesting research issue. 5 Mathematics and Algorithm Design: Reciprocal Influences Many branches of mathematics have contributed to aspects of algorithm design, and the issues raised

875

by the analysis of new algorithmic problems have, in a number of cases, suggested novel mathematical questions. tatively transformed by the growth of computer sci-ence, to the extent that algorithmic questions have Combinatorics and graph theory have been qualibecome thoroughly intertwined with the mainstream of research in these areas. Techniques from probability have also become fundamental to many areas ofcomputer science:
probabilistic algorithms draw power from the ability to make random choices while they are being executed, and probabilistic models of the input to an algorithm can give one a more realistic view of the problem instances that arise in practice. This style of analysis provides a steady source of new questions in discrete probability. A computational perspective is often useful in thinking about “character ization” problems in mathematics. For example, the general issue of characterizing prime numbers has an obvious algorithmic component: given a number whether it is prime?
(There exist algorithms that arenas input, how efficiently can we determine exponentially better than the approach of dividing$\sqrt{n}$ by all numbers up to theory [IV.3 §2](/part-04/computational-number-theory).) Problems in$n$: see computational number knot theory [III.44](/part-03/knot-polynomials), such as the character ization of unknotted loops, have a similar algorithmic side. Suppose we are given a cir-cular loop of string in three dimensions (described as a jointed chain of line segments), and it wraps around itself in complicated ways.
How efficiently can we deter-mine whether it is truly knotted, or whether by moving it around we can fully untangle it? We can ask this sort of question in many similar mathematical contexts; it is clear that these algorithmic issues are extremely concrete as problems, though they may lose part of the original intent of the mathematicians who posed the questions more generally.
tion of algorithmic ideas with all the different branchesof mathematics, we conclude this article with two case Rather than attempting to enumerate the inter sec studies that involve the design of algorithms for partic-ular applications, and the ways in which mathematical ideas arise in each instance. 6 Web Search and Eigenvectors As the World Wide Web grew in popularity through out the 1990 s, computer-science researchers grappled with a difficult problem: the Web contains a vast amount of useful information, but its anarchic structure makes

876

it very hard for users, unassisted, to find the specific information they are looking for. Thus, early in the Web’s history, people began to develop search engines that would index the information on the Web, and pro-duce relevant Web pages in response to user queries. But of the thousands or millions of pages relevant to atopic on the Web, which few should the search engine present to a user? This is the ranking problem: how to determine the “best” resources on a given topic. Note the contrast with concrete problems like the TSP. There, the goal (the shortest tour) was not in doubt;
the diffi-culty was simply in computing an optimal solution efficiently. For the search engine ranking problem, on the other hand, formalizing the goal is a large part of the challenge—what do we mean by the “best” page on a topic? In other words, an algorithm to rank Web pages is really providing a definition of the quality of a Web page as well as the means to evaluate this definition. purely on the text it contained.
These approaches began to break down as the Web grew, because they did The first search engines ranked each Web page based not take into account the quality judgments encoded in the Web’s hyperlinks: in browsing the Web, we often discover high-quality resources because they are“endorsed” through the links they receive from other pages. This insight led to a second generation of search engines that determined rankings using link analysis. The simplest such analysis would just count the number of links to a page:
in response to the query “newspa-pers,” for example, one could rank pages by the number of in coming links they receive from other pages con-taining the term—in effect, allowing pages containing the term “newspapers” to vote on the result. Such a scheme will generally do well for the top few items, placing prominent news sites like The New York Times and this, however, it will quickly break down, favoring a The Financial Times at the head of the list; beyond large number of highly linked but irrelevant sites. latent information in the links.
Consider pages that linkto many of the sites ranked highly by this simple voting It is possible to make much more effective use of the scheme; it is natural to expect that these are authoredby people with a good sense for where the interesting newspapers are, and so we could run the voting again, this time giving more voting power to these pages that selected many of the highly ranked sites. This revote might elevate certain lesser-known newspapers favored by Web-page authors who were more knowledgeable on the topic;
in response to the results of this revote, we could further sharpen our weighting of the voters. This

VII. The Influence of Mathematics

“principle of repeated improvement” uses the informa-tion contained in a set of page-quality estimates to produce a more refined set of estimates. If we perform these refinements repeatedly, will they converge to a stable solution?In fact, this sequence of refinements can be viewed as an algorithm for computing the principal tor [I.3 §4.3](/part-01/fundamental-definitions) of a particular matrix; this both estab-eigen ve cl i sh es the convergence of the process and characterizes the end result. To establish this connection, we intro-duce some notation. Each Web page is assigned two scores:
an primary source on the topic; and aauthority weight, measuring its quality as ahub weight, measuring its power as a voter for the highest-quality content. Pages may score highly in one of these measures butnot in the other—one should not expect a prominent newspaper to simultaneously serve as a good guide to other newspapers—but there is also nothing to prevent a page from scoring well in both. One round of voting can now be viewed as follows:
we update the authority weight of each page by summing the hub weights of all pages that point to it (receiving links from highly weighted voters makes you a better authority); we then reweight all the voters, updating each page’s hub weight by summing the authority weights of the pages it points to (linking to high-quality content makes you a better hub).How do eigenvectors come into this? Suppose we define a matrix M with one row and one column for each page under consider at i on;
the1 if pagei links to page j, and it equals 0 otherwise.(i, j) entry equals We encode the authority weights in a vector the coordinatea is the authority weight of pagea, wherei.i

The hub weights can be similarly written as a vector Using the definition of matrix–vector multiplication, h. we can now check that the updating of hub weights in terms of authority weights is simply the act of setting Mthh updates the authority weights. (Here equal to Ma; correspondingly, setting Ma T equal todenotesthe transpose of the matri xn times each from starting vectors M.) Running these updates a and h , we obtain(MTM)ana=.
This is the power-iteration method for(MT(M(MT(M · · · (MT(M(a0)0)) · · · ))))0 = computing the principal eigenvector ofwe repeatedly multiply some fixed starting vector by0 MTM, in which larger and larger powers of MTM. (As we do this, we also divide all coordinates of the vector by a scaling factor to prevent them from growing unboundedly.) Hence this eigenvector is the stable set of authority weights toward which our updates are converging. By

VII.5. The Mathematics of Algorithm Design

completely symmetric reasoning, the hub weights are converging toward the principal eigenvector of MMT. by a different procedure that is also based on repeated A related link-based measure is Page Rank, defined refinement. Instead of drawing a distinction between the voters and the voted-on, one posits a single kind of quality measure that assigns a current set of page weights is then updated by having weight to each page. A each page distribute its weight uniformly among the pages it links to. In other words, receiving links from high-quality pages raises one’s own quality.
This toocan be written as multiplication by a matrix, obtained from MTby dividing each row’s entries by the number of out going links from the corresponding page; repeated updates again converge to an eigenvector. (There is a further wrinkle here: repeated updating in this case tends to cause all weight to pool at “dead-end”pages that have no out going links and hence no where to pass their weight. Thus, to obtain the Page Rank measure used in applications, one adds a tiny quantityin each iteration to the weight of each page;
this is$ε > 0$ equivalent to using a slightly modified matrix.) engine Google; hubs and authorities form the basis for Ask’s search engine Teoma, as well as a num-Page Rank is one of the main ingredients in the search ber of other Web search tools. In practice, current search engines (including Google and Ask) use highly refined versions of these basic measures, often combin-ing features of each; understanding how relevance and quality measures are related to large-scale eigenvector computations remains an active research topic.
7 Distributed Algorithms Thus far we have been discussing algorithms that run on a single computer. As a concluding topic, we briefly touch on a broad area in computer science concerned with computations that are communicating computers. Here the problem of effi-distributed over multiple ciency is compounded by concerns over maintaining coordination and consistency among the communicating processes. sider a network of automatic teller machines (ATMs).As a simple example illustrating these issues, con When you with draw an amount of money these ATMs, it must do two things:
(1) notify a cent ralx at one of bank computer to deduct(2) emit the correct amount of money in physical bills.xfrom your account; and Now, suppose that between steps (1) and (2) the Atm crashes so that you do not get your money; you would

877

like it to be the case that the bank does not sub trac tx from your account anyway. Or suppose that the ATM executes both of steps (1) and (2), but its message to the bank is lost; the bank would like for subtracted from your account anyway. The field of dis-x to be eventually tributed computing is concerned with designing algo-rithms that operate correctly in the presence of such difficulties. experience long delays, some of them may fail in mid-computation, and some of the messages between them As a distributed system runs, certain processes may may be lost.
This leads to significant challenges in rea-soning about distributed systems, because this pattern of failures can cause each process to have a slightly different view of the computation. It is easily possible for there to be two runs of the system, with different patterns of failure, that are “in distinguishable” from the point of view of some process$P$; in other words, $P$differences in the runs did not affect any of the com-will have the same view of each, simply because the munications that it received.
This can pose a problem if$P$’s final output is supposed to depend on its having noticed that the two runs were different. about in the 1990 s, when a connection was made to techniques from algebraic topology. Consider for sim-A major advance in the study of such systems came plicity a system with three processes, though every-thing we say generalizes to any number of processes. We consider the set of all possible runs of the system; each run defines a set of three views, one held by each process.
We now imagine the views associated with a single run as the three corners of a triangle, and we glue these triangles together according to the following rule: for any two runs that are in distinguishable to some process P , we paste the two corresponding triangles together at their corners associated with This gives us a potentially very complicated geometric P . object, constructed by applying all these pasting oper-ations to the triangles; we call this object the complex associated with the algorithm.
(If there were more than three processes, we would have an object in a higher number of dimensions.) While it is far from obvious, researchers have been able to show that the correctness of distributed algorithms can be closely connected with the topological properties of the complexes that they define. mathematical ideas can appear unexpectedly in the study of algorithms, and it has led to new insights into This is another powerful example of the way in which the limits of the distributed model of computation.

878

Combining the analysis of algorithms and their com-plexes with classical results from algebraic topology has in some cases resolved tricky open problems in this area, establishing that certain tasks are provably impossible to solve in a distributed system.

Further Reading

Algorithm design is a standard topic in the under grad-uate computer-science curriculum, and it is the subject of a number of textbooks, including Cormen et al.(2001) and a book by Kleinberg and Tardos (2005). The perspective of early computer scientists on how to formalize efficiency is discussed by Sipser (1992).The TSP and the MSTP are fundamental to the field of combinatorial optimization; the TSP is used as a lens through which this field is surveyed in a book edited by Lawler et al. (1985).
Approximation algorithms and local-search algorithms for computationally intractable problems are discussed in books edited by Hochbaum(1996) and by Aarts and Lenstra (1997), respectively. Web search and the role of link analysis is covered in a book by Chakrabarti (2002); beyond Web applications, there are a number of other interesting con-nections between eigenvectors and network structures, as described by Chung (1997). Distributed algorithms are covered in a book by Lynch (1996), and the topological approach to analyzing distributed algorithms isreviewed by Rajsbaum (2004).
Aarts, E., and J.
 K. Lenstra, eds. 1997.Combinatorial Optimization. New York: John Wiley. Local Search in Chakrabarti, S. 2002.gan Kaufman. Mining the Web. San Mateo, CA: Mor Chung, F. R. K. 1997.American Mathematical Society. Spectral Graph Theory. Providence, RI: Cormen, T., C. Leiserson, R. Rivest, and C. Stein. 2001.Introduction to Algorithms. Cambridge, MA: MIT Press. Hochbaum, D. S., ed. 1996.NP-hard Problems. Boston, MA: PWS Publishing. Approximation Algorithms for Kleinberg, J., and É. Tardos. 2005.MA: Addison-Wesley. Algorithm Design. Boston, Lawler, E. L., J. K. Lenstra, A. H. G. Rinnooy Kan, and D.
B.Shmoys, eds. 1985. The Traveling Salesman Problem: A Guided Tour of Combinatorial Optimization John Wiley. . New York: Lynch, N. 1996.Morgan Kaufman. Distributed Algorithms. San Mateo, CA: Rajsbaum, S. 2004. Distributed computing column 15.SIGACT News 35:3. ACM Sipser, M. 1992. The history and status of the P versus NP question. In Proceedings of the 24 th ACM Symposium on Theory of Computing Computing Machinery. . New York: Association for

VII. The Influence of Mathematics

VII.6 Reliable Transmission of

Information

Madhu Sudan

1 Introduction

The notion of “digital information” emerged in the middle of the twentieth century, in response to the advent of the telegraph and to the beginnings of computer science, which at the time was principally a theoretical discipline. Of course, the use of electricity to commu-nicate signals goes back further, but the earlier uses involved signals of a “continuous” nature: music, voice, etc.
The new era was characterized by the transmission of (or the need to transmit) more “discrete” messages, i.e., messages such as English sentences, which can be described as finite sequences of letters taken from some finite alphabet. The phrase “digital information” came to be applied to such families of messages. the engineers and mathematicians charged with the Digital information posed some novel challenges to task of communicating such messages. The root cause of these challenges is “noise.” Every communication medium is noisy, and never transmits any signal completely accurately.
In the case of continuous signals, some how the receivers (typically, our ears and eyes) can adjust to such errors and learn to discount them. For example, if you play a very old recording of a musi-cal performance, then there will typically be a crackling noise, but it is possible to ignore this, unless the quality is very bad indeed, and concentrate on the music. How-ever, in the case of digital information errors can have a more catastrophic effect.
To see this, suppose that weare communicating in English sentences and that the communication medium makes occasional mistakes by altering one of the transmitted letters. In such a scenario the message

WE ARE NOT READY

could easily be changed into the message

WE ARE NOW READY.

All it takes is one error on the part of the communica-tion medium, and the entire intention of the message is reversed. Digital information tends to be inherently intolerant of errors, and the mathematicians and engi-neers of the time were charged with the task of inventing methods that would make communication reliable even if the process of transmission is not.

VII.6. Reliable Transmission of Information

any message, the sender of the message repeats every letter, say five times. For example, to send the message Here is one way of achieving this. To communicate

WE ARE NOT READY

the sender says something like

WWWWWEEEEE AAAAA. . . .

The receiver can then detect errors (as long as there are not too many) by checking that every block of five successive letters repeats the same letter. If this ever fails to be the case, then it is clear that errors have occurred during transmission. If it is not possible for five successive symbols to be in error (or even if it isjust very unlikely), then it follows that the resulting scheme is also more reliable than the underlying means of transmission.
Finally, if even less error is possible, then it may be possible for the receiver to determine the actual message, rather than simply being able totell when errors have occurred. For example, if at most two symbols in any block of five can be erroneous, then the most commonly occurring letter in each block of five must be the letter from the original message: for instance, a sequence such as WWWMWEFEEE AAAAA. . . would be interpreted by the receiver as WE A. . . .
to correct two errors does not appear to be a very effi-Repeating every symbol five times in order to be able cient way to use the communication channel. Indeed, aswe will show in the rest of this article, when transmitting long messages one can do much better. However, in order to understand this issue, we need to define the process of communication, the model of error, andthe measures of performance more carefully. We do so next. 2 Model

2.1 Channel and Errors

The central object of attention in the problem of information transmission is the “channel of communica-tion,” or simply the channel. The channel has an input (the original signal to be communicated) and anput (the signal after it is transmitted). The input con-outsists of a sequence of elements from some finite set: by analogy with the English-language example, these elements are called typically denotedΣ, is called an letters and the finite set, which isalphabet. The channel attempts to transmit the input to the receiver, but while

879

doing so it may make some errors. The alphabet and the process that under lies the errors are what specifies the channel. the example described above, the alphabet consisted ofthe English characters The alphabetΣ varies from scenario to scenario. In{A, B, . . . , Z}, and possibly some punctuation symbols. In most communication scenar - ios, the alphabet is the “binary alphabet” that consists just of the “letters” 0 and 1, which are known as On the other hand, in applications involving storage bits.
of digital information (in compact discs (CDs), digital versatile discs (DVDs), etc.), the alphabet contains 256 elements (the alphabet of “bytes”). define a good mathematical model for the way that errors are produced, then a lot more care is needed. At Specifying an alphabet is easy, but if we wish to one extreme is a worst-case model suggested by Ham - ming (1950), where there is some limit on the number of errors that the channel can make, but within that limit it chooses the errors to be as damaging as possible.
A more benign class of errors was proposed by Shannon(1948), who suggested that errors could be modeled by a probabilistic process. many of the concepts below. In this model, the error of the channel is specified by a real number param-We will focus on one probabilistic model to illustrate eterp, where 0 ⩽ p ⩽ 1. Every use of the channel results in an error with probability if the sender transmits an elementσp. To be precise,\in  Σ, then with probability 1 with probability- ppthe output for that element isit is some other element σ^ σofbutΣ, chosen uniformly at random.
Further more, and this isvery crucial to this model, the errors are assumed to be independent each letter it transmits with out any memory of how it, i.e., the channel repeats this process for acted on previous symbols. We refer to this model astheΣ-symmetric channel with parameter p (or Σ-SC(p)) in the rest of this article. A special case of particular importance is thetheΣ-symmetric channel when binary symmetric channelΣ is the binary alpha-, which is bet sponding output bit will be 0 with probability 1$\\{0}$, 1\\\\\\\\\\\\\\\\\\\}.
Then, if the input bit is 0, say, the corre-- p and1 with probability p. plified (and even unnatural ifbet While this model of error may seem rather over sim-$\\{0}$,1\\\\\\\\\\\\\\\\\\\\\}), it turns out that it captures the essence ofΣ is not the binary alpha- most mathematical challenges that arise when one tries to make communication reliable. Further more, many of the solutions found to make communication reliable inthis setting have been generalized to other scenarios, 880 so this simple model is very useful both in practice andin the theoretical study of communication.
2.2 Encoding and Decoding Suppose the sender wishes to transmit a sequence through a channel that makes errors. One way to compensate for these errors is to send through the chan-nel not the sequence itself but a modified version of the sequence that contains redundant information. The process of modification that we choose is called the encoding method of encoding, namely repeating each term in theof the message. We have already seen one sequence several times. However, this is by no means the only way of doing it, so to discuss encoding we usethe following general framework:
if the sender has a message consisting of a sequence ofthen by some means or another it expands the messagek elements of Σ, into a new sequence, now consisting offor somen > k. Formally, the sender applies ann elements ofencod-Σ, ing function the set of sequences of length E:Σk \to Σn to the message. (k with letters inΣk stands forΣ, and Σn for the set of sequences of length a mess a gem = (m , m , . . . , m ) nto the receiver, the.) Thus, to convey sender transmits over the channel not theof m but the n symbols o(f1)2 E(m)k.
k symbols receiver receives a sequenceis then to “compress” the sequence Errors may then be introduced, after which the$r = (r^{1}$, rr2 back to a, . . . , rn); its goalk-letter sequence, removing the error and obtaining the orig-inal mess a gem (at least if not too many errors have occurred). It does this by applying a$D$:$Σ^{n} \to Σ^{k}$, which tells it how sequences of length decoding functionn are converted back into sequences of len gthk. options available to the designers of the communi-cation system.
Their choice determines the perfor-The possible pairs of functions E, D describe the mance of the system. Let us now describe how this performance is measured.

2.3 Goals

Very informally, our goals are threefold. We would liketo make the communication as reliable as possible. At the same time, we would like to maximize the utilization of the channel. Finally, we would like to do sowith effective computation. We describe these goals more carefully below, in the case of the model described earlier.Σ-SC(p) sage Consider first the reliability. If we start with a mes-m, encode it as E(m), and pass it through the

VII. The Influence of Mathematics

channel, then the output, after some random errors have been introduced, will be a stringy . The receiver will decode mess a gem, there is a certain probability of ay , producing a new message D(y). For each decoding error fact be equal to the original message, i.e., a certain probability that D(y)m. The reliabil-will not in ity of the communication is measured by the largest of these probabilities. If this is small, then we know that, whatever the original mess a gem, a decoding error is unlikely, and then we regard the communication asreliable.
is measured by the tity Next, let us look at the utilization of the channel. Thisk/n. In other words, it is the ratio of the length ofrate of the encoding, i.e., the quan- the original message to the length of the encoded mes-sage: the smaller this ratio, the less efficiently one is using the channel. able to encode and decode quickly: a pair of reliable and efficient encoding and decoding functions will not Finally, practical considerations also require us to be be of much use if they are very time-consuming to com-pute.
Adopting the standard convention in algorithm design, we regard our algorithms as feasible if they runin polynomial time: that is, if their running time can be bounded above by a polynomial function of the length of their input and output. To illustrate the above ideas, let us analyze the “repetition encoding” that repeats every letter of the alpha-bet five times. For simplicity, take the alphabetΣ to be\\{0},1\\$, let the probability pbe fixed, and let us consider the behavior of the model as the message length to. nfty .
Our encoding function takes strings of len gthk tendsk to strings of length 5 particular block of five transmissions, the probabilityk and thus has a rate o(f1)5. Given any that it contains three or more errors is  p^ = 53 p3(1 - p)2 + 54 p4(1 - p) + 55 p5. The probability that that block does not give rise to a decoding error is 1- p^ , so the probability that there is no decoding error is there is a decoding error is 1(1 - p^ )-k and the probability that(1 - p^ )k.
If we fix p > 0 and letk → . nfty, then (1 - p^ )k tends to 0 (exponentially quickly), so the probability of decoding error tends to 1.Thus, this encoding/decoding pair is highly unreliable, and its rate is not too good either. The only redeeming feature is that it is very easy indeed to compute. (Its computational efficiency is easily seen to be bounded by a number of operations that is linear in One way to salvage the repetition code is to rep eatk.) every sym bolc . og k times. For a largish constant c, the

VII.6. Reliable Transmission of Information

probability of a decoding error goes to 0, but now the rate of the code goes to 0 as well. Prior to the work of Shannon it may have even been believed that a trade-offof this kind was inevitable: every encoding/decoding scheme would either achieve a vanishingly small rateor make mistakes with probability tending to 1. As we will see later in the article, it is in fact possible to define encoding schemes that achieve all three of our goals:
they operate at a positive rate, they can correct errors that occur a positive proportion of the time (in either the probabilistic or the worst-case model), and they use efficient encoding and decoding algorithms. Most of the insight for this remarkable result goes back to a sem-inal paper by Shannon (1948). In that paper he gave the first examples of encoding and decoding functions that satisfied the first two goals, though they were not computationally efficient.
therefore not practical, but we can now see, with the benefit of hindsight, that ignoring the goal of efficient Shannon’s encoding and decoding functions were computability in order to gain some theoretical insight into the channels was extraordinarily fruitful. A general rule of thumb seems to operate: that the perfor-mance of the very best encoding and decoding functions can be matched arbitrarily closely by encoding and decoding functions that are also computationally efficient. This justifies considering the goal of efficiency separately from the other two goals.
3 The Existence of Good Encoding and Decoding Functions In this section we will describe results that demonstrate the existence of encoding and decoding functions that have an extremely good rate and reliability. In order todescribe these results, first proved by Shannon, it will be useful to consider two related notions introduced by Hamming in work that was essentially concurrent with that of Shannon. describing what makes one encoding function In order to understand these notions, let us start by E better or worse than another.
The task of the tion is to work out, when it receives a string decodingy , what the func- original message to working out what the encoded mess a gem was. Notice that this is equivalent E(m) was, since no two messages are encoded in the same way. The possible encoded messages are called that is, a codeword is a string of len gthn that arises ascodewords:

E(m) for some message m \in Σk.

881

fusing two codewords after errors have been intro-duced, and this depends only on the set of codewords, What we are worried about is the possibility of conand not on which codeword corresponds to which orig-inal message. Therefore, we adopt what at first seems a strange definition: an strings of len gthn in the alphabet error-correcting codeΣ (that is, any subset is any set of ofΣn). The strings in an error-correcting code are still called codewords.
This definition completely ignores the actual process of encoding of a message, but that is so that we can focus on the rate and the decoding error while ignoring computational efficiency. If we are given an encoding function E, then the corresponding error-correcting code is simply the set of all the code-words of E. Mathematically, this is just the image of the function What makes an error-correcting code good or bad? To E. answer this question, let us consider what happens ifthe alphabet is\\\{0, 1\\\} and the code contains two stringsxfer in precisely= (x1, x2, . . . , xdn places.
If errors are introduced with) and y = (y1, y2, . . . , yn) that dif- probability intoy is pdp(1, then the probability that - p()n)-d. Assuming that p <x is converted1, this prob- ability gets smaller asthe more likely the stringsd increases, so the smallerx and y are to be confused.2 d is, It seems preferable, therefore, that there should notbe too many pairs of strings in the code that differ in just a few places. A similar argument applies to larger alphabets as well. The above thoughts lead to a definition that is very natural in this context. Given an alphabet string sx = (x , x , . . .
, x ) and y = (y , yΣ , . . . , yand two) belonging to$Σn 1$, the2 Hamming distanc en betwee(n1)2 x and ny which is defined to be the number of coordinate sx = y . For example, let Σ = \\\{a, b, c, d\\\}i andfor letthe third and sixth places and are identical otherwise, n = i6. The strings i abccad and abdcab differ in so their Hamming distance is 2.
Our goal is to find an encoding function imizes the typical Hamming distance between pairs of E such that the associated code max - codewords. Shannon’s solution to this is an extremely simple application of the probabilistic method [IV.19 §3](/part - 04/extremal - and - probabilistic - combinatorics): he picks the encoding function at random. That is, for every mess a gem, the encoding E(m) is chosen entirely randomly from the setΣn, with all choices equally likely. Further more, for every message is independent of the encoding of every other mes - m, this choice sagem^ .
It is a good exercise in basic probability to see that such a choice almost always leads to a code 882 where the distances between codewords are on aver-age large. In fact, even the minimum distance between codewords is almost always large. However, we will not show this. Instead, we will argue that with high prob-ability this random choice leads to a “nearly optimal” encoding function, from the point of view of rate and reliability. First, let us consider what the decoding function ought to be.
In the absence of computational require - ments, it is not hard to say what the “optimal” decoding algorithm is. If you receive a sequence should choose the mess a gem that is most likely toz, then you have resulted in this sequence. For the modelwithp < 1 - 1/|Σ|, it is easily verified that this will beΣ-SC(p) the message toz, as measured by Hamming distance. (If the min - m for which the encoding E(m) is nearest imum distance is attained by both E(m) and E(m^ ), then one can make an arbitrary choice between them.) The condition onwhen the sequencep E(m)is important here.
It ensures that passes through the channel, the most likely output corresponding to any given term, out of the|Σ|different possibilities, is the same as the input. With out this condition, there would be no reason to expect there is a num berz to be close to C, depending only on the error proba-E(m). We shall argue that bility dom encoding function with rate smaller thanp and the size of the alphabet, such that for a ran-C, this decoding function recovers the original message with a high probability.
As an aside, Shannon also showed that for the same constant C, any attempt to communicate at rates greater than ability exponentially close to 1. Because of this result, C would lead to errors with prob- the constant channel. C is known as the Shannon capacity of the case of the binary alphabet choosing a random function Once again, for simplicity we shall consider just the\. \10}, from1\\$. In this case we are{0, 1}k to {0, 1}n, and we would like to show that, under suitable circumstances, the resulting code will almost certainly be very reliable.
In order to do this, we shall focus on a single message The first idea is a precise form ofm, and rely on two basic ideas.the law of large numbers the expected number of errors introduced into a code-[III.71 §4](/part-03/probability-distributions). If the error probability isp, then word the actual number of errors will almost certainly be very E(m) is pn, so, if n is large, then we expect that close to this, just as, if you toss a fair coin ten thousand times, you will be surprised if the number of heads is not close to five thousand. The result that expresses this formally is as follows.
VII.
 The Influence of Mathematics Claim.probability that the number of errors exceeds There exists a constantc > 0 such that the(p + )n is at most 2-c 2 n. ber of errors is less than The same can be said of the probability that the num-(p - )n, but we shall not use this result. Whenn is large, 2-c 2 n is extremely small, so the number of errors is almost certainly at most(p + )n. The number of errors equals the Hamming distance fromy , the output of the channel, to E(m), the code- word that was transmitted.
Therefore, the decoding function that chooses the codeword with smallest Hamming distance from E(m), provided that there is no messagey will almost certainly ch oosem^  such that E(m^ ) is closer to y than (p + )n.

almost certainly be the case, is that “Hamming balls are small.” Let The second idea, which allows us to say that this will z be a sequence in \\{0}, 1\\}n. Then the Ham- ming ball of radiu sr about z is the set of all sequences wth is set? Well, in order to specify a sequence with Hamming distance at mostr from z. How big isw with Hamming distance exactly specify the set ofd places whered fromw andz, it is enough toz differ. There arend ways of choosing this set, so the number of sequences at a distance of at most r isn0 + n1 + n2 + · · · + nr.

Ifconstant times$r = αn and α <^{n}$, because each term is at leas(t1)2, then this number is at most arn -r r = 1 -α α

times the one before. But

$nr = r$!(nn-!r )!.

If we now use approximation stirling’s formul an!= (n/e)n, then we find that this is[III.31](/part-03/the-gamma-function) or the looser about(1/α(1 - α))n, which is 2 H(α)n$, where H(α) = −α \log2 α - (1 - α) \log2(1 - α)$. (Note that H(α) is positive, because α and 1-α are less than 1 and therefore have negative logarithms.) The function uous and strictly increasing on the interval H is called the entropy function. It is contin-[0,1] with H(and therefore 20) = 0 and H(H(α)(n1)2) =is exponentially smaller than 21. So, if α <1 2$, then H(α) <2 1$, n:
this is what is meant by saying that the Hamming ball of radiusαn is small.

VII.6. Reliable Transmission of Information

ity that a single randomly chosen sequencein the Hamming ball of radius Let us setα to be p +  <^1^2(p. Then the probabil-+ )n about E(m^ )yliesis at most 2 H((p+2))n2-n. (The 2 is to compensate for slight inaccuracies in the above estimate for the sizeof the ball.) Since there are 2 k - 1 possibilities for m, the probability that one can be found for which E(m^ ) lies in the ball is at most 2$k2H(p + 2^{)}n2 - n$.
Therefore, ifk2-⩽nn(, which is exponentially small.1 - H(p + 2) - ), this probability is at most we can make Because we can choosek/n as close as we like to 1 to be as small as we like,- H(p) while still maintaining an exponentially small probability ofdecoding error. It turns out that the quantity 1- H(p) is the constantity of the binary symmetric channel. Thus, the capacity Cdiscussed earlier: the Shannon capacof the binary symmetric channel is always positive if p <1. general than the above example demonstrates.
For awide variety of channels, and for a wide variety of mod-Shannon’s theorem and proof are significantly more2 els of (probabilistic) error, his theory pins down the capacity of the channel and shows that reliable communication is possible if and only if the rate of the channel is less than its capacity. Shannon’s proof is a remarkable example of the use of the probabilis-tic method in the practice of engineering. Note, however, that the encoding and decoding algorithms are quite impractical.
The proof gives no clue about howto find an encoding function, though of course one can consider every encoding function{0, 1}n to check if it is good. However, even if such E:\\{0},1\\k \to a function is found, it may have no succinct descrip - tion, in which case the encoder and decoder have to store this encoding function as an exponentially long table in their memory.
Finally, the decoding algorithm seems to involve a brute-force search for the near-est codeword, a problem which seems to be the most serious obstacle to obtaining a computationally effi-cient version of Shannon’s theorem that can be used in practice. What the theorem definitely is a significant insight into the limitations and poten-does give us tial utility of the communication channel. With this in mind, we can set ourselves the right targets to strive for when we come to devise more practical encoding and decoding procedures.
In the next section we will show that it is possible to achieve a fixed rate that is bounded away from zero, to tolerate a constant fraction of errors, and to do both of these with efficient algorithms. 883 4 Efficient Encoding and Decoding Let us now turn to the task of designing encoding and decoding functions that can be calculated effi - ciently. Currently, there are at least two very different approaches to building such functions. We describe here an approach based on algebra over finite fields.
The alternative approach is based on the construction of expanding graphs [III.24](/part - 03/expanders), but we will not describe that here.

4.1 Codes for Large Alphabets Using Algebra

In this section we describe a simple way to get an encod-ing function E:$Σ^{k} \to Σ^{n}$, where Σis a finite [I.3 §2.2](/part-01/fundamental-definitions) with at leastn elements. (Recall that there are field finite fields withpt for a prime p qand a positive integer elements whenever qt.) These codes is of the form were introduced by Reed and Solomon (1960) and have since been called the Reed–Solomon codes. ofa message A Reed–Solomon code is specified by a sequence nd i st in ct field ele men tsm = (m , m , . . . , mα1, . . . , α). nn Σ. nk, we asso-Σ.
Given ciate with the message the polynomialm x + · · · + m (x0()k)-1. The encoding o(f1)k^-1 M(x)m=is sim-m0 + ply the sequence other words, to encode a sequence1 k E(m)-1 = M(α1)$, M(αm2)$, . . . , M(α, you treat then). In terms of the sequence as themial of de greek - 1 and write out the values that thisk coefficients of a polyno polynomial takes atα1, . . . , αn. this code, let us note that it is very succinctly repre-sented: all that is needed to specify it is a descrip-Before describing the error-correcting capability of tion of the fieldα , . . . , α .
It is easy to show that the number of addi-Σ and the sequence of n elements tions and multiplications needed to compute at most1 Ck^n for some constant C. (For example, to work M(α) is out 3α3 - α2 + 5α + 4, you start with 3, multiply byα, subtract 1, multiply by α, add 5, multiply by α, and add 4.) Therefore, the number of field operations needed to compute the entire encoding is bounded above bymore sophisticated and efficient algorithms are known Cnk, for some (different) constant C.
(In fact, for the encoding problem that take at most Cn(. og n)2 steps.)Now let us consider the error-correcting properties of the code. We start by showing that the encodingsof any two mess a gesm and m have a Hamming dis- tance of at least$n - (k - {}^{1} 1)$. To see this$, let2 M1(x) and M$ Now the difference2(x) be the polynomials associated withp(x) = M (x) - M (x)mhas degree1 and m2.1 2 884 at most k - 1, and it is not the zero polynomial (since Mk -1 and1 roots. This tells us that there are at most M2 are distinct), and therefore it has at mostk - 1 values ofα for which M1(α) = M2(α).
It follows that the Hamming distance between the sequences $E(m^{1}) = (M^{1}(α^{1})$, M1(α2), $. . . , M1(αn)) and$ E(m2) = (M2(α1)$, M2(α2)$, . . . , M2(αn)) is at leastn - k + 1. ming distance from at least one ofis greater than It follows that if1(n z- k)is any sequence, then its Ham-(since otherwise the distance E(m1) and E(m2) betwee nn - k).
Therefore, if the number of errors that occur E(m1)2 and E(m2) would have to be at most during transmission is at mostinal mess a gem is uniquely determined by the receive(d1)2(n - k), then the orig- sequenceis an efficient algorithm for working out whatz. What is much less obvious is that therem was, but, remarkably, it is possible to compute polynomial-time algorithm (inn), which we shall nowm with a describe. the numbers What must the decoding algorithm do? It is given$α^{1}$, . . . , αn and the received sequencezdegree1, . . .
, zkn-and is required to find a polynomial1 or less such that M(α ) = z for all but at M of$most 12$(n - k) values of i. If such a polynomial exists, i i then it is unique, as we have just seen, and its coeffi-cients will give the original mess a gem (if the number of errors is at most 12(n - k)). easier: one can determine the coefficients of a polyno-mial of degree If there were no errors, then our task would be muchk - 1 from k of its values by solving k simultaneous equations.
However, if some of the values we use are in correct, then we will end up with acompletely different polynomial, so this method is not easy to use for the problem we actually face. MTo over come this difficulty, let us imagine that exists and that the errors introduced into the sequence s ⩽1(n - M(αk). Then the polynomial1), . . . , M(αn) occur at B(x)i1=, . . . , i(x - sα, where) · · ·(xonly if - 2αi sx) has degree at mostis equal to α for som(e1)2 (n - jk). Let us setand is zero if and A(x()i)1 to equal at most$M(x)B(x)k - 1 + {}^{1}$.
Then(n -ik)j A(x)=1(nis a polynomial of degree+ k - 2), and for everyithen this is obvious, since we have A(αi)2 = zi B(αi)z2. (If there is no error at= M(α ), and if there isi, an error ati, then both sides are 0.()i)i

VII. The Influence of Mathematics

mialsof degree at most Conversely, suppose that we manage to find polyno-A(x), of degree at mostk - 1, such tha(t1)2(n A(α+ k -) =2)z, and B(αB(x)) for, every of degree at most$i$. Then R(x)1=(n A(x)+k--2)M(x)B(x), and R(αiis a polynomial) = 0 when eve(ri)i M(αthis happens for at leasti) = zi. Since there are at most2 n -1 (n - k()1)2 i=(n1-(nk)+ errors, k) val- ues ofthan its degree, from which it follows thati. Therefore, the number of roots o(f2)2 RR is identi-is bigger cally zero, so that$A(x) = M(x)B(x) \text{for every} x$.
From this we can determine A(x) and B(x) are nonzero, one can determine M: givenk values of x for whichk values of$M(x) = A(x)/B(x)$, and hence determine M. find polynomials properties. The It remains to show that we can indeed (efficiently)n A(x)constraints and B(x)A(αwith the required) = z B(α ) turn intoof A nandlinear constraints on the unknown coefficients$B$. Since B has1(n - k) +1 coefficients and(ii)i A$hasis(n1)2+(n1$.
Since the system of equations is homogeneous+k)coefficients, the total number of unknowns2 (that is, we obtain a solution if we take all unknowns tobe zero) and the number of unknowns is greater than the number of constraints, there must be a nontrivial solution: that is, a solution where A(x) and B(x) are not both the zero polynomial. More over, we can find such a solution by Gaussian elimination, which takes at most$Cn^{3} steps$. fact that two distinct low-degree polynomials can not be equal for too many values. We then exploit the rigid To summarize:
we construct a code by exploiting the algebraic structure of low-degree polynomials for the purposes of decoding. The main tool that allows us to do this is linear algebra and in particular the solving of systems of simultaneous equations.

4.2 Reducing the Size of the Alphabet Using

Good Codes

The ideas described in the previous section show ushow to build codes with efficient encoding and decoding algorithms, but they use relatively large alphabets. In this section we shall exploit these results to build binary codes. To begin with, let us consider a very obvious method of converting codes over large alphabets into codes over the binary alphabet$\\{0}$, 1\\\\\\\\\\\\\\\\\\\\\}. For simplicity, assume that we have a Reed–Solomon code over an alphabetof size 2 l for some integer l.
Then we can associate theΣ elements ofcase, we can regard the Reed–Solomon encoding func-Σ with binary strings of length l. In such a tion, which mapsΣk to Σn, as a function from \{0, 1. lk VII.6. Reliable Transmission of Information to\{0, 1. ln. (For instance, an element of Σk is a sequence of len gthk objects, each of which is a binary sequence ofl. Putting them together produces a single binary sequence of lengthtinct messages differ for at least kl.) Since the encodings of two dis - n - k + 1 elements ofΣ, they must also differ on at least n - k + 1 bits. alphabet.
However, fraction of This gives a fairly reasonable code over the binary$ln$: the ratio$n -(nk +-$1 is not as large as a fixedk + 1)/ln is less than 1/l, and since we need 2 l, the size of Σ, to be at least n, we find that this fraction is at most 1 to zero asntends to infinity. However, this can be fixed$/ \log2 n$, which tends in a simple way, as we shall see. two different elements of binary sequences that differ in just one bit.
However, The problem with the simple binary approach is thatΣ may be represented by the Hamming distance between two binary sequencesof length lis usually much larger: it is more likecl for some positive constant sent the elements ofΣc. Suppose that we could repre-as binary sequences of some length between any two of the sequences used was at least L in such a way that the Hamming distancesc L. This would allow us to improve our argument above:
if the encodings of two messages were different for atleastn - k + 1 elements of Σ, then they would have to differ on at leastn - k + 1, and this is a positive fraction ofc L(n - k + 1) bits rather than just Ln. What we are asking for is an encoding of the binary sequences of length a way that no two codewords are closer thanl as sequences of len gthc LL in suchto each other. But we know, from the previous section, that such an encoding exists, provided that L and c satisfy appropriate conditions:
for instance, it is possible tofind an encoding function that works with$L ⩽ 10l andc ⩾$So how do we use this? We start with a binary101 . se que ncem of length lk. As above, we associate with this a sequence of length encode this sequence using the Reed–Solomon code, k in the alphabet Σ. We then obtaining a sequence of length Next, we convert each term of this sequence into an in the alphabet Σ. binary sequence of length$l$.
And, finally, we encode each of these length L using a good encoding function, obtaining asn binary sequences as a sequence of a result a binary sequence of length this sequence through the channel, where errors may Ln. We then pass be introduced. The receiver then breaks the received sequence up into block to work out what binary sequence of len gthn blocks of length L, decodes eachl gave rise to it, and interprets that binary sequence as an

885

element ofofΣ. It then uses the Reed–Solomon decoding algo-Σ. This results in a sequence of n elements rithm to decode this sequence, producing a sequence ofk elements of Σ. Finally, this can be converted into a binary sequence of length lk. coding and decoding procedures that convert binary sequences of length We have said nothing about the efficiency of the en-l into ones of length L and back again, stating merely that they exist. Since efficiency is supposed to be our priority, this may seem rather strange:
do we not now face exactly the same problem that we were trying to solve in the first place? Luckily we do not, because although these encoding and decod-ing procedures may take exponentially long, they take exponentially long as a function ofmuch smaller thann. Indeed, L is proportional to log L, and L is muchn, from which it follows that 2 L is bounded above by a polynomial function ofone can afford procedures of exponential complexity$n$. This is a useful principle: provided that one only ever applies them to very short strings.
the code explicitly, we have demonstrated that there is an encoding and decoding algorithm that runs in Thus even though we have not managed to specify polynomial time and that corrects a constant fraction of errors. To complete this section, let us address the question of the probability of decoding error, which we have not yet discussed.
The technique described above, of composing encoding functions (and decoding functions), can also be used to improve the above code sothat the encoding and decoding still take place in polynomial time, but now the decoding error probability is exponentially small on the binary symmetric channel with parameterp, and the rate is arbitrarily close to the Shannon capacity, which is the theoretical maximum.(The idea is to compose a Reed–Solomon code that has rate close to 1 with a random inner code, and then toshow that with random errors most of the inner decoding steps decode correctly.
One then uses the outer decoding step to convert the “mostly correct decoding” to a “fully correct decoding.”) 5 Impact on Communication and Storage The mathematical theory of error-correcting codes has made a deep impact on the technologies for storage and communication of information, and we elaborate a little on this below. the biggest success story for error-correcting codes. Storage of information on digital media is probably

886

Most known forms of storage media, and in particu-lar standards for audio and data CDs and DVDs, prescribe error-correcting codes based on Reed–Solomon codes. Specifically, they are based on a code that maps F223 to F255, where F is the finite field with 256 elements. In audio CDs, codes are use to protect from minor scratches, though more serious scratches do lead256 256 256 to audible errors. In data CDs the error correction is stronger (with more redundancies), so that even seri-ous scratches do not lead to loss of data.
In all cases (CDs and DVDs) the readers for these devices use fast algorithms for decoding when reading the information on the media. Typically, these algorithms are based on the idea of the previous section, but are much faster implementations (in particular, an algorithm due to E. Berlekamp is widely used). Indeed, several CD read-ers owe their faster reading speed to faster decoding algorithms. Similarly, the increased storage capacity of DVDs (compared with CDs) is attributed in part to better error-correcting codes.
Indeed, error-correction technology played a crucial role in establishing the dominance of audio CDs, which store music digitally, over the traditional, and now almost extinct, gramo-phone records, which store music in continuous forms. Thus, mathematical advances in coding theory have played an influential role in this technology. effect on communication. Since the late 1960 s, error-Similarly, error-correcting codes have had a profound correcting codes (and decoding) have been used for communication from satellites to their base stations on Earth.
Of late, error-correcting codes are also being used in cellular phone communications and modems. Again, the most commonly used code at the time of the writing of this article is the Reed–Solomon code, though this situation has been changing rapidly since the dis-covery of a new class of codes called “turbo codes.” This new family of codes seems to offer significant resilience to random errors (more so than that offered by methods based on Reed–Solomon codes) and uses a simple and quick algorithm, even when the codes used have small block length.
These codes and the corresponding decoding algorithm have led to a resurgence of inter-est in codes constructed with the help of insights from graph theory [III.34]. Many of the good properties of turbo codes have been observed only empirically: that is, the codes seem to work very well in practice but it has not yet been proved rigorously that they do. Nevertheless, the observations have been so compelling that new standards for communication are starting to prescribe these codes.

VII. The Influence of Mathematics

codes used are based on ones that are studied in the mathematical literature, this should not be taken to Finally, it must be stressed that while many of the mean that they can be deployed immediately with out further design. For example, the Mariner spacecraft used not a Reed–Muller code but a variant of it designedto allow for synchronization between blocks. Similarly, the Reed–Solomon codes used in storage devices are carefully spread out over the disc, so as to allow the physical device to resemble more closely the model ofa code over a large alphabet.
Note that errors due to a scratch on the disc surface tend to ruin a large col-lection of bits in a small localized part of the disc. If all the data from a block were sitting in such a neigh-borhood, the entire block would be lost. So each block of 255 bytes of information is spread out all over the disc. On the other hand, the bytes themselves, which are elements ofproximity. So a scratch corrupting one bit out of these F256, are written as eight bits in close eight is also likely to corrupt others in the neighbor-hood.
However, this is all right from the perspective of the model that views the entire collection of eight bitsas a single element. In general, working out the right way to apply the theory of error correction to a given scenario is a major challenge, and many success stories would not have been success stories had it not been for some careful design choices. other in this arena. Mathematical successes, such asnew algorithms for decoding Reed–Solomon codes, Mathematics and engineering continue to feed each raise the challenge of how to adapt technology to exploit new algorithms.
Engineering successes, such as the discovery of turbo codes that perform extremely well, challenge mathematicians to come up with a formal model and analysis that can explain this success. And if such a model and analysis emerges, it is likely to lead to the discovery of new codes that might surpass the performance of turbo codes and lead to a new set of standards! 6 Bibliographic Notes The theory of reliable communication and storage of information owes much to the seminal works of Shannon (1948) and Hamming (1950), which formed the basis for much of this article.
The Reed–Solomon codes of section 4.1 are from Reed and Solomon (1960). their decoding algorithm originates in the work of Peters on (1960), though the algorithm given here is significantly simplified. The technique of composing codes is due to Forney (1966).

VII.7. Mathematics and Cryptography

variety of results. Some of these give better con struc-tions of codes with faster algorithms. Others provide Over the years, coding theory has amassed a wide theoretical upper limits on how well codes can perform. The theory uses an enormous variety of math-ematical tools, many of them more advanced than the ones described in this article. Most not able among them are algebraic geometry and graph theory, which are used to construct very good codes, and the theory of orthogonal polynomials, which is used to prove limits on parameters of codes, such as their rate and relia-bility.
Most of the highlights of this vast literature are covered in Pless and Huffman (1998).

Further Reading

Hamming, R. W. 1950. Error detecting and error correcting codes. Bell System Technical Journal 29:147–60. Forney Jr., G. D. 1966.MIT Press. Concatenated Codes. Cambridge, MA: Peters on, W. W. 1960. Encoding and error-correction pro-cedures for Bose–Chaudhuri codes. IEEE Transactions on Pless, V. S., and W. C. Huffman, eds. 1998.Information Theory Coding Theory, two volumes. Amsterdam: North-Holland.6:459–70. Handbook of Reed, I. S., and G. Solomon. 1960. Polynomial codes over certain finite fields. SIAM Journal of Applied Mathematics Shannon, C. E. 1948.
A mathematical theory of communica-8:300–4.tion. Bell System Technical Journal 27:379–423, 623–56. VII.7 Mathematics and Cryptography

Clifford Cocks

1 Introduction and History

Cryptography is the science of hiding the meaning or content of communications. The aim is that an adversary who sees a message only in its enciphered state can not make sense of or derive useful information from what is seen. On the other hand, the intended recip-ient must be able to decipher the true meaning.
For most of history cryptography has been an art practiced seriously only by a few—such as governments for mil-itary and diplomatic communications—for whom the consequences of unauthorized disclosure of informa-tion are damaging enough to justify the expense and in convenience of enciphering messages. Recently this has changed: one of the results of the information revolution has been the need for instant and secure com-munication for all on demand. Fortunately, mathematics has come to the rescue and provided theoretical

887

and algorithmic developments to meet this need. It has also provided entirely new possibilities, such as “digital signatures” (which will be discussed later). raphy isbe enciphered consists of a piece of English text. Before One of the oldest and most basic methods of cryptog-simple substitution. Suppose that a message to it is sent, the sender and recipient agree on a permutation of the twenty-six letters of the alphabet, which they keep private.
An enciphered message might then look something like ZPLKKWL MFUPP UFL XA EUXMFLP For very short messages this method is reasonably secure—it is just possible to work out the meaning of the above example by matching letter patterns to those commonly seen in English, but it is quite challenging! However, for longer messages, simply count-ing the frequencies of each letter and comparing those counts with the frequencies of letters in natural lan-guage will almost always reveal the hidden permutation sufficiently to allow the meaning to be easily recovered.
advent of mechanical encryption devices in the twenti-eth century, of which the German Enigma used during A major leap forward in cryptography came with the World War II is perhaps the most famous example. An account of the fascinating Enigma story and the role of the code breakers of Bletchley Park appears in Simon Singh’s excellent book on cryptography (Singh 1999).It is interesting that the principle on which Enigma operates is a development of the simple substitution method.
Each letter of the input message is enciphered exactly as a simple substitution, but with the addi-tional rule that the permutation controlling the substitution changes after every letter. A complex electromechanic al device controls the substitution process ina deterministic way. The recipient can decipher the message only if he or she can set up another device in exactly the same way as the originator. The informa-tion needed to do this is called the key. Making sure that keys are known only by the right people is called key management to gr ap hy (to be discussed later), key management was.
Until the advent of public-key crypa major in convenience and expense for anyone wanting to secure their communications. 2 Stream Ciphers and Linear Feedback Shift Registers Since the advent of computers, information has tended to be transmitted as binary data: that is, as a stream

888

ar ar-1 a1 Figure 1 Linear feedback shift register.

of 0 s and 1 s. For such data there is a rather different method of encipherment based on a device called the linear feedback shift register, or LFSR (see figure 1). The first step is to generate a random-looking sequenceof 0 s and 1 s in a deterministic way, and this is done by means of a recurrence formula, of which a simple example is

$xt = (xt)-3 + (xt)-4$.

Here, addition is mod 2, sox will be 1 if an odd num-t

ber of the terms wise. We must also specify the first four values of the$xt^{-}3$, (xt)-4 is 1, and it will be 0 other- sequence, so let us begin with 1000. The sequence then continues as follows: 100110101111000100110101111. . . .a1 More generally, one specifies some positive integers$, a^{2}$, . . . , ar, called feedback positions—the numbers 3 and 4 in the above example—and defines a sequenceby means of the recurrence formula $xt = xt^{-}a 1 + xt^{-}a 2 + · · · + xt^{-}a r$, where again the addition is mod 2.
random, but because there are only finitely many binary sequences of length A sequence produced in this way usually looks fairly a it must eventually repeat.r

Notice that, in our example, the sequence is periodic with period 15, which is actually the longest possi-ble period, since there are sixteen binary sequences of length 4, and after a moment’s thought one sees that the sequence 0000 cannot occur (or else the whole sequence up to then would have had to consist entirely of zeros).In general, the length of the sequence depends on properties of the polynomial P (x) = 1 + (xa)1 + (xa)2 + · · · + (xa)r over the just seen in the case field [I.3 §2.2](/part-01/fundamental-definitions)a F2=of two elements.
As we have4, the maximum possible rs equ en ce length is 2 ar - 1, and for this length to be achieved the polynomial F : that is, it must not factorize into smaller polyno-P (x) must be irreducible over mials. For example, the polynomial 12+ x4 + x5 is not

VII. The Influence of Mathematics

irreducible, because(1 + x + x3)(1 + x + x2) expands out to 1+ x + x + x2 + x2 + x3 + x3 + x4 + x5, which equals 1+x4 + x5 since 1 + 1 =0 in the field F2. quence to have the maximum length, but it does not guarantee it. For that we need a second condition: that Irreducibility is a necessary condition for the sethe polynomial isus take the polynomial primitive$x3$.
To see what this means, let+ x + 1 and calculate the remainder when, for the first few positive integerswe divide$x^{m} by x^{3} + x +$1 (with all coefficients in Fm)., When$x^{2}$, x +m1, goes from 1 to 7 we obtain the polynomialsx2 + x, x2 + x + 1, x2 + 1, 1. For instance,2 x, x6 = (x3 + x + 1)(x3 + x + 1) + x2 + 1, so the remainder on dividing$x^{6} by x^{3} + x + 1 is x^{2} + 1$. was when the polynomial Now the first time that we obtained the polynomial 1 m =x7, and 73 + x + 1 is primitive. In general, a= 23 - 1.
This shows that polynomialtime you obtain a remainder of 1 when you dividep(x) of degree dis primitive if the firs txm$by$ p(x) is when m = 2 d - 1. There are computationally efficient tests for determining whether a polynomial is irreducible and wheth-er it is primitive. The advantage of using a primitive polynomial as the basis of an LFSR is that, in the sequence it generates, no subsequence of length repeated until all nonzero sequences of length$a^{r} ahave^{r} is$ appeared exactly once.
ple idea would be to take the stream of bits gen-erated by an LFSR and add it term by term to the How is all this applied in cryptography? A sim message one is enciphering. For instance, if the lfsr generated a sequence that began 1001101 and the message was 0000111, then the encrypted message would begin 1001010. To decipher such a message, one could simply repeat the process: adding the two sequences 1001101 and 1001010 gives the original message 0000111.
For this to work, the recipient would need to know the details of the LFSR in order to be ableto generate the same sequence 1001101, so one might consider using the feedback positions (in this case 3 and 4) as the secret key. The above procedure is not good enough to be of practical use because there is an efficient algorithm, due to Berlekamp and Massey (1969), that can recover the feedback rule from the stream of bits it generates. It is better to use some predetermined nonlinear function of the successive sequences ofar bits in order to

VII.7. Mathematics and Cryptography

L R

F

Figure 2 Feistel round structure.

scramble further the sequence of bits produced by the LFSR. Even then, such procedures are simple enough that, with careful design, they can be applied to large amounts of data very quickly. 3 Block Ciphers and the Computer Age

3.1 Data Encryption Standard

When computers started to be used, an entirely different method of cryptography became practical: the block cipher. The first example of this was DES: the Data Encryption Standard (first published in 1977). DES was adopted as a standard in 1976 by the U.S.National Bureau of Standards (now the National Institute of Standards and Technology). This enciphers ablock of 64 bits at a time, with a key of length 56 bits. It has a particular structure, referred to as a(see figure 2). Feistel cipher you first divide it into two parts of 32 bits each, and call them This structure is as follows.
Given a block of 64 bits, L and R. Next, you take a subset of the 56 bits of the key, according to some predetermined rule, anduse this subset to define a nonlinear function F, again according to some predetermined rule, which takes 32 bit sequences to 32-bit sequences. You then replace the pair[L, R] by the pair [R ⊕ F(L), L].
(Here R ⊕ F(L) denotes the result of taking the mod-2 sum of R and F(L)Having done that, you repeat the process a num-one bit at a time.) ber of times, choosing a different nonlinear function F each time (but always deriving it in a predetermined way from the 56-bit key). A complete encryption by DES consists of 16 such rounds, together with some permutation of the bits of the input and output. as long as one knows the 56-bit key it is quite easyto reverse the encryption process. Given a round that One reason for using the Feistel structure is that performs the transformation

[L, R] \to [R ⊕ F(L), L], 889 one can invert it by means of the transformation [L$, R] \to [R, L ⊕ F(R)].$ This has the great advantage that it does not require us to invert F , so even if F is quite complicated the procedure can be easy to carry out. have been developed. Simply using the algorithm to encrypt each 64-bit block of data in turn is called ECBA number of what are called “modes of use” of DES (for mode is that if there is an exact 64-bit repeat in the data electronic codebook) mode. A disadvantage of this then this results in an exact 64-bit repeat in the cipher.
Here, each block of data is added mod 2 to the previ-ous block before being encrypted as above. In OFB, or Another mode is CBC, or cipher block chaining, mode. output feedback DES encipherment of the previous block. It is an easy, mode the block of data is added to the exercise to see how to decipher in CBC and OFB modes, and in practice these are the two most common modes of use of DES. 3.2 Advanced Encryption Standard The U.S.
National Institute of Standards and Tech-nology recently held a competition for a replacement for DES, to be called the Advanced Encryption standard with a variety of possible key lengths. Many compet-, or AES. This was to be a 128-wide block cipher ing designs were submitted and subjected to public scrutiny, and the winning entry was called Rijndael, after the designers Joan Daemen and Vincent Rijmen. The design is remarkable and elegant and makes use of interesting mathematical structures (Daeman and Rijmen 2002).
The 128 bits in each block are thought of as 16 bytes (a byte consists of eight bits), arrangedin a 4. imes 4 square. Each byte is then thought of as an ele- ment of F256, the field of order 256. Encryption consists of ten or more rounds (the exact number depending upon the key length); and each round mixes the data and the key. A round consists of a series of steps, typically as follows. First, each byte, regarded as an element ofthe finite field F , is replaced by its inverse in the field, except that 0 is left unchanged.
Each byte is then regarded as an element of the vector space of dimen - 256 sion 8 over the field formation is applied. Each row of the 4 F2 and an invertible linear trans-$\times 4 \text{square is}$ then rotated, by a different number of bytes for each row. Next, the values of each column of the square are taken to be the coefficients of a degree 3 polynomial over F and this is multiplied by a fixed polynomial 256 890 and reduced modulox4 + 1. Finally, the key for the round, which is derived linearly from the encryption key, is added modulo 2 to the 128 bits.
which makes decipherment straightforward. It is likely that AES will take over from DES as the most widely It can be seen that all of these steps are reversible, used block cipher. 4 One-Time Key The various encryption methods described above relyon the computational difficulty of recovering some secret that protects the enciphered data. There is one classic encryption method that does not rely on this property.
This is the “one-time key.” Imagine that the message to be enciphered is encoded as a sequence of bits (for example, the standard ASCII encoding that represents each character as eight bits). Suppose that ahead of time the sender and recipient have shared a sequence of random key bitsr , . . . , r at least as long as the message. Suppose that the message bits are$p^{1}$, p2, . . . , pn.1 nxin each bit. If the bitsi The enciphered message is then= pi + ri. Here, as usual, addition is mod 2 additionr are fully random, then knowingx1$, x2$, . . . , xn, wherei

the sequencex gives no information whatsoever abouti

the message sequence key. It is very secure as long as the key is used only once.$pi$. This system is called one-time However, it is impractical to use this method except in very specialized situations because of the need for sender and recipient to share and keep safe possibly large quantities of key material. 5 Public-Key Cryptography All of the examples of encryption methods that we have seen so far have had the following structure. two communicators agree on an algorithm or method for encryption.
The choice of method (e.g., simple substi-tution, AES, or one-time key) can be made public with out the security of the system being compromised. Thetwo communicators also agree on a secret key in the form required by the chosen encryption method. This key needs to be kept secure and never revealed to any adversary. The communicators encipher and decipher messages using the algorithm and secret key. This presents a major problem: how can the communicators securely share the secret key?
It would be inse-cure to exchange this over the same system that they will later use to send enciphered messages. Until so-called public-key methods were discovered this issue

VII. The Influence of Mathematics

limited the use of encryption to those organizations that could afford the physical security and separate communication channels necessary for distributing keys reliably. The following remarkable, counter intuitive proposition forms the basis of public-key cryptography: possible for two entities to communicate information it is in such a way that they start with no secret shared information; an adversary has access to all communications between them; at the end the entities have shared secret knowledge that the adversary is unable to determine. be.
Consider, for example, some one making a pur-chase over the Internet. Having identified a product one It is easy to see how useful such a capability could wishes to buy the next step is to send personal infor-mation such as credit card details to the vendor. With public-key cryptography it is possible to do this in a secure manner straightaway. structure of a solution was proposed by James Ellis in1969, How might public-key cryptography be possible? The1 with the first public description by Diffie and Hellman (1976).
The critical idea is to use a function that is hard to invert unless you have an “inverse key” that helps you to do so. More formally, a one-way function H is a mapping from a set told the value X to itself, with the property that if you arey = H(x) for some x \in  X, then it is computationally hard to determineis a secret value, z, say, used in creating the functionx. The inverse key H, with the property that if you knowz then it becomes computationally We can use this to solve the problem of secure key easy to recoverx from H(x). exchange as follows.
Let us suppose that Bob wishes tosend some data securely to Alice. (Particularly useful would be a shared secret that they can use later as akey for subsequent communications.) Alice begins by generating a one-way function H with an inverse keyzthe inverse key remains her personal secret, which she. She then communicates the function H to Bob, but reveals to no one—not even to Bob. Bob takes the datax that he wishes to send, computes H(x), and returns the result of his computation to Alice. Because Alice has the inverse keyz, she can reverse the function H and there by recoverx.
the communications between Alice and Bob. Then the Now suppose that an adversary manages to read all available at www.cesg.gov.uk/site/publications/media/possnse.pdf.1. See “The possibility of secure non-secret digital encryption,”

VII.7. Mathematics and Cryptography

adversary will know the function However, Alice has not communicated the inverse key H and the value H(x).z, so the adversary is faced with the computationally intractable problem of inverting successfully transmitted the secret H. Therefore, Bob hasx to Alice with out the adversary being able to work out what it is.
(Fora more precise idea of what computational intractability is and a further discussion of one-way functions, see computational complexity [IV.20](/part-04/computational-complexity), especially section 7.)It can be helpful to imagine the one-way function Has a padlock and the inverse key as the key that unlocks the padlock. Then if Alice wants to receive an enciphered message from Bob, she sends him her padlock, retaining the key. Bob locks (enciphers) the message into a box with the padlock, and returns it.
Only Alice, who is in possession of the padlock key, can unlock (decipher) the message. 5.1 RSA It is all very well to have such a framework, but it leaves open an obvious question: how can one produce a one-way function with an inverse key? The following method was published by Rivest, Shamir, and Adleman(1978).
It relies on the fact that it is relatively easy to find large prime numbers and multiply them to pro-duce a composite number, but it is much harder, if you are given that composite number, to determine its two prime factors. To create a one-way function by their method, Alice first finds two large prime numbers calculates the integer N = P Q and sends it to Bob, P and Q. She then together with another integer exponent.
The values N and ee are called the called the encryption public parameters knows what they are.because it does not matter if an adversary to send to Alice as a number moduloputes Bob then expresses the secret value H(x), which is defined to bex Nexmod. Next, he com-that he wishes N, that is, the remainder whenxe is divided by N. Bob sends H(x) to Alice. Upon receipt of Bob’s message, Alice needs to recover $x from x^{e} mod N$. This she can do by first calculating the numberdthat satisfies the equation

de ≡ 1 mod (P - 1)(Q - 1).

To do this efficiently, Alice can user i thm [III.22]. Notice, however, that this would not be euclid’s al go possible if she did not know the values offact, the ability to calculate the correct value of P andd Qcan. In

891

be shown to be equivalent to the ability to factorize The value of dis Alice’s private key (or “inverse key” in N. the terminology above): it is the secret that can undo the encryption function H. This is because H(x)d mod N can be shown to equalx. Indeed, the significance of the number(P - 1)(Q - 1) is that it equalsφ(N), the number of integers less than states that N and coprime toxφ(N) ≡ 1 mod N.Neuler’s theorem when eve rx is coprime[III.58](/part - 03/modular - arithmetic) tohas the form N.
Therefore, mφ(N)xmφ(N)+ 1, as we are assuming, then≡ 1 mod N as well, so if de H(x)d ≡ xde ≡ x mod N.
In other words, if you raise x to the powerd mod N you get back toe mod N and then raise that to the powerx. (An important point is that raising numbers to powers mod easy by the method of “repeated squaring.” This is dis - N is computationally cussed in While it has not been proved that the only way for computational number theory [IV.3 §2](/part - 04/computational - number - theory).) an adversary to defeat the RSA encryption system is tofactorize N, no other general attack has been found. This has created interest in finding improved factor-ization methods.
A number of new subexponential methods—elliptic curve factorization (Lenstra 1987), the multiple polynomial quadratic sieve (Silverman 1987), and the number field sieve (Lenstra and Lenstra1993)—have been discovered in the years since the RSA algorithm was found. See theory [IV.3 §3](/part - 04/computational - number - theory) for discussions of some of them.computational number 5.1.1 Implementation Details The security of the RSA system depends on the primes P and Q being large enough to make factorization hard. However, the larger they are, the slower the encryption process is.
Thus, there is a trade-off between security and the speed of encryption. A typical choice that is often made is to use primes that are each of 512 bits. tion exponent either For the deciphering method to work, the encryp-(P - 1) ore must have no factors in common with(Q - 1). This assumption was needed when we applied Euler’s theorem, and if it does not hold then the encryption function is not invertible. Values such as 17 or 216+ 1 are often used in prac- tice, because making computation needed to calculate the encrypted valuee small reduces the amount ofxe mod N.
(These two values of e are also well-suited to calculation by repeated squaring.)

5.2 Diffie–Hellman

Another approach to generating a shared secret was published by Whitfield Diffie and Martin Hellman. In

892

their protocol Alice and Bob jointly create a shared secret, which can then be used as the key for one of the conventional cryptographic systems such as AES.To do this, they agree on a large prime number P and a primitive ele me ntg such that (g P)-1 ≡g1 mod modulo P , but P , which means a number gm ≡ 1 mod P for anym < P - 1. randomly chosen between 1 andg Alice then creates her own private key= ga mod P and sends this to Bob. P - 1, and calculatesa, a number a

1 and Bob similarly creates his own private key P - 1 and calculates and sends gb =bgbetweenb mod P to Alice. gcalculates this asab Alice and Bob can now create the shared secret mod P . Alice calculates this asgb mod P . Note that all of these terms(ga)b mod P and Bob can be calculated in time logarithmic inrepeated squaring.a a and b throughg(gb)ab An adversary, however, would see only mod mod PP, and would also knowbe determined from this? One method is$g and g P^{a}$.
How could mod P and to solve what is called the This is the problem of calculating discrete logarithm problem a if you know P , g.,$and$ g^a mod P. For large P this appears to be a com- put at i on ally intractable problem. It is not known for certain whether there is a faster way for the adversary to calculategab mod P than computing discrete logarithms—this is called the Diffie–Hellman problem— but at present no better method is known.
general, but it is much easier if, as is usually the case, the prime It is not obvious how to find primitive elements in P has been constructed so as to ensure that the factorization of P - 1 is known. For instance, if P isof the form 2 Q +1, where Q is also a prime (such num- bers are called shown that for any Sophie Germain primes a, exactly one of a), then it can beand -a has the property that its Qth power is congruent to -1 mod P , and this one is a primitive element. In practice, one can find such primes by a process of trial and error:
for example, one can choose a number use randomized primality tests to see whether Q randomly and Q and 2 Q + 1 are prime. Assuming that, as every one believes, such pairs occur with the “expected” frequency, the probability of finding one on any given attempt is large enough for this approach to be feasible.

5.3 Other Groups

The Diffie–Hellman protocol can be expressed in the language of group theory [I.3 §2.1](/part-01/fundamental-definitions). Suppose we have

VII. The Influence of Mathematics

a group the group to be Abelian and will use “G and some element g \in  G. We will require+” to denote the group operation. (In the examples so far, the groups under consider at i on were multiplicative groups consisting of elements coprime to some integer using additive notation we are taking a “logarithmic”N, so by perspective.)To execute the protocol Alice computes some private integer that Alice can compute this sum ofa and computes and sendsa elements ofag to Bob. Note G in time of order logarithmic ina by successive doubling and adding.
(In the multiplicative groups considered earlier, “doubling” is squaring, “adding” is multiplying, and “multiplying by a” is raising to the powera.) computes and sends Similarly, Bob computes a private integer bg to Alice. b andabg Both Alice and Bob can calculate the shared value. An adversary will know only G, g, ag, and bg. cryptographic systems? The critical property is that the discrete logarithm problem in The question is: which groups can be used in practical Gmust be hard; in other words, given to determine Ga,.
g, and ag it should be a hard problem to graphic purposes is the additive group generated by points on an One type of group that has aroused interest for cryp-elliptic curve [III.21](/part - 03/elliptic - curves). An elliptic curve has an equation of the form

$y2 = x3 + ax + b$.

It is an interesting exercise to sketch this curve over the real numbers—the shape depends upon how many times the curve

$y = x^{3} + ax + b$

crosses thex-axis.

a group law It is possible to define an “addition rule” (often called) on the points of this curve, as follows. Given two points A and B on the curve, the straight line joining them must meet the curve in a third point, C say. This is because a straight line must meet a cubic in three places precisely. Define A+ B to be the mirror image of C in the It is obvious that Ax-axis (see figure 3).+ B = B +A from this definition. What is rather more surprising is that the associative law holds. That is, for any three points A, B, and C we have$((A + B) +C) = (A +(B + C))$.
There are some deep reasons why this is true, but of course it can be verifiedby just doing the algebra. from the set of points on an elliptic curve defined over To use this for cryptography the group is formed

VII.7. Mathematics and Cryptography

C

B

A

A + B

Figure 3 Addition of points on an elliptic curve. a finite field. The graphical image for the sum of two points is no longer valid, but the algebraic definition still holds, so addition still obeys the associative law. We need to add one further point to the set of points on the curve to function as the zero of the group: thisis the “point at infinity” on the curve. a curve defined over ments in the group is a prime number.
In fact it is For optimal security it turns out to be best to find Fp for which the number of ele- guaranteed—by a deep result on the theory of elliptic curves—that the number of points on a curve defined over(See the weil conjectures Fp will lie between p + 1[V.35](/part-05/the-weil-conjectures).)- 2 . qrt{p} and p + 1 + 2 . qrt{p}. The reason this group is used is that for general curves the discrete logarithm problem appears to be particularly hard.
If the group hasn elements and if we are given group elementsof steps needed to determineg anda, by the best algorithm sag, then the number that are currently known, is around a so-called birthday attack that allows one to solve this$\sqrt{n}$. Since there is problem in any group withn elements in around . qrt{n} computational steps, this means that the problem for elliptic curve groups is as hard as it can be. Therefore, whatever level of security you require, the public key isas short as it can be.
This is important when there are constraints on the number of bits that can be sent asit allows the protocol to be executed in the minimum possible time. 6 Digital Signatures As well as secure transmission of data, there is another very useful capability that is provided by public-key cryptography. That is the concept of a digital signature. A digital signature is a string of symbols that an author attaches to the end of a message that certifies the

893

authenticity of the message. In other words, it proves that the message was written by the attested author and that it has not been modified. Once the necessary frameworks are in place, this opens up the possibility of much legal business being conducted online. There are a number of ways that public-key methods can be used to create digital signatures. The one based on the RSA system is perhaps the simplest. Suppose Alice wants to sign documents. Just as she does for encryption, she generates two large prime numbers P and Q and calculates her public modulus N = P Q and her public exponente.
She also generates her private key—the deciphering exponent xde ≡ x mod N for any x. She will use the same param-d with the property that eters both for encryption and for the creation of digital signatures. messages know her Alice can assume that the recipients of her signed N and e values. In practice she may have these values themselves signed and certified by a trusted authority or organization that the prospective recipient of a signed message will recognize.
One other component of this system is an object called athe message to be signed, which may be rather long, one-way hash function, which takes as its input and outputs a number between 1 and$N - 1$. The impor- tant property that a hash function must have is that for any value ally hard to construct a messagey between 1 and Nx that hashes to thatit is computation- value. This is similar to a one-way function except thatwe are no longer assuming that for eachy there is exactly one tion should ideally also bex that maps to ycollision free.
However, the hash func-, which means that, even though there are many pairs of messages that hash to the same value, it is not easy to find any. Such hash functions need to be carefully designed, but there are some recognized standard hash functions (two of which are called MD5 and SHA-1). Suppose thatx is the message to be signed, and let apply the hash function tox. The digital signature that X be the output when you Alice appends to the message is$Y = X^{d} mod N$.
Observe that anyone in possession of Alice’s public key can verify the signature by following these steps. First, calculate the hashed value X of the message x, which is possible because the hash function is made public. Next, compute Z = Ye mod N, which can be done because the parameters N and e are also pub- lic. Finally, verify that such a signature, you have to find X equals Z . In order to fake Y with the prop- erty that Ye ≡ X mod N. That is, you must know how894 to calculate Xd, which is computationally intractable if you do not already knowd.
It is also possible to construct digital signatures using a public key based on discrete logarithms (Diffie–Hellman type) rather than on factorization (RSA type). The U.S. standards body has published such a proposal: the Digital Signature Standard (1994). 7 Some Current Research Topics Cryptography remains an active and fascinating are a for research—there are undoubtedly more results and ideas to be discovered.
For a good over view of current activity one should look at recent proceedings of the main conferences, such as Crypto, Eurocrypt, or Asiacrypt (these are published in the Springer series Lecture Notes in Computer Science). The comprehensive book on cryptography by Menezes, van Oorschott, and Vanstone (1996) is a good way to get up to speed on present theory. In this final section I out line just a few of the directions in which the subject is moving.

7.1 New Public-Key Methods

One important area of investigation is the search for new public-key methods and signature schemes. Recently some interesting new ideas have come from the use of Franklin 2001). These are maps pairings on elliptic curves (Boneh andw from pairs of points on the curve to either the finite field over which the curve is defined or an extension field. A pairingw is bilinear, in the sense that w(A+B, C) =w(A, C)w(B, C) and w(A, B + C) = w(A, B)w(A, C), where addition is the group operation defined on points of the curve and multiplication takes place in the field.
“identity-based cryptosystem.” Here, a user’s identity serves as his or her public key, which eliminates the One way that such a map can be used is to create an need for directories or other public-key infrastructure in order to store and propagate public keys. a curve, a pairing map In such a system, a central authority decides uponw, and a hash function that maps identities to points on the curve. All of this ismade public, but there is also a secret parameter, an integerx. Suppose that the hash function maps Alice’s identity to the point Alice’s private key A on the curve.
The authority calculatesx A and issues it to her when she registers, after making appropriate checks on her identity. Similarly, Bob would receive his private keyx B,

VII. The Influence of Mathematics

where identity. B is the point on the curve corresponding to his out any initial key exchange, using the common keyw(x A, B)Alice and Bob are now able to communicate with-= w(A, x B). The important point is that unlike other public-key systems this can be done with-out any need to share public keys. 7.2 Communication Protocols A second area of activity is the study of proposed pro - tocols, especially those likely to become international standards.
When public-key methods are to be used in practical communication the sequence of bits to be transmitted needs to be clearly defined, so that both communicating parties understand the same thing byeach bit sent. For example, if ann-bit number is transmitted, are the bits transmitted in increasing or decreasing order of significance? The rules or protocols are often enshrined in public standards, and it isimportant that they do not introduce any weakness into the system. duced in this way is one discovered by Coppersmith(1997) in a seminal paper.
He showed that in a low-An example of the sort of weakness that can be intro exponent RSA system (for example, one with encryp-tion exponent equal to 17) a weakness arises if too many of the bits of the number that is to be enciphered are set to publicly known values. This is something that is natural to want to do, if, as is often the case, a large public-key modulus is being used to transmit amuch shorter communication key. As a result of Coppersmith’s discovery such fields are nowadays usually padded out before they are encrypted, with bits that vary unpredictably.
7.3 Control of Information Using public-key methods, one can control very pre-cisely how information is released, shared, or generated. Research in this area is usually focused on finding elegant and efficient ways of achieving different sorts of control in a variety of situations.
As a simple example, we might want to create a secret that is shared between N people in such a way that if any K people combine their share (where the secret, but no information can be gained about the K < N) they can reconstruct secret by any smaller number than Another example of this type of control is a protocol K collaborating. that allows two participants to create an RSA modulus(a product of two primes) in such a way that neither VII.8. Mathematics and Economic Reasoning participant gets to know the primes that were usedto produce the modulus.
To decipher a message enciphered under this modulus the two participants haveto collaborate—neither can achieve this on their own (Cocks 1997). allows Alice and Bob to replicate tossing a coin, but to A third and more amusing example is a protocol that do it over the telephone. Obviously, it would not be sat-is factory for Alice to toss the coin and for Bob to make the call “heads” or “tails”—for how does Bob know that Alice is telling the truth about how the coin actually fell? This problem turns out to have a simple solu - tion. Alice and Bob choose large random sequences.
Alice then appends either a 1 or a 0 to her sequence and Bob does the same for his. Alice’s extra bit rep-resents the out come of the coin toss, and Bob’s represents his guess. Next, they send one-way hashes of their sequences (with the extra bits appended). At this point, because of the nature of one-way hashes, nei-ther has any idea what the other’s sequence is, so, for example, if Alice reveals her hashed sequence first, Bob cannot use this information to increase his chance ofguessing correctly. Alice and Bob then exchange the unhashed sequences to see whether Bob’s guess was correct.
If either does not trust the other, they can hash the other’s sequence to check that it really does give the right answer. Since it is hard to find a different sequence that gives the right answer, they can each be confident that the other has not cheated. More complicated proto-cols of this type have been designed—it is even possible to play poker remotely in this way. Further Reading Boneh, D., and M. Franklin. 2001. Identity-based encryp-tion from the Weil pairing. In Advances in Cryptology— CRYPTO 2001 ume 2139, pp. 213–29. New York: Springer..
Lecture Notes in Computer Science, vol Cocks, C. 1997.ters. Cryptography and Coding Split Knowledge Generation of RSA Parame-. Lecture Notes in Comput Coppersmith, D. 1997. Small solutions to polynomial equa-er Science, volume 1355, pp. 89–95. New York: Springer.tions, and low exponent RSA vulnerabilities. Journal of Daeman, J., and V. Rijmen. 2002.Cryptology AES—The Advanced Encryption Standard Series. New10(4):233–60. The Design of Rijndael. Data Encryption Standard. 1999. Federal Information Pro - York: Springer.cess ing Standards Publications, number 46 - 3. Diffie, W., and M. Hellman.
1976. New directions in cryp - tography. IEEE Transactions on Information Theory 22(6): 644–54. 895 Digital Signature Standard. 1994. Federal Information Pro-cess ing Standards Publications, number 186. Lenstra, A., and H. Lenstra Jr. 1993.the Number Field Sieve. Lecture Notes in Mathematics, The Development of Lenstra Jr., H. 1987. Factoring integers with elliptic curves.volume 1554. New York: Springer. Annals of Mathematics 126:649–73. Massey, J. 1969. Shift-register synthesis and BCH decoding. IEEE Transactions on Information Theory 15:122–27. Menezes, A., P. van Oorschott, and S. Vanstone.
1996.Applied Cryptography. Boca Raton, FL: CRC Press. Rivest, R., A. Shamir, and L. Adleman. 1978. A method for obtaining digital signatures and public-key cryptosystems. Machinery Communications of the Association for Computing21(2):120–26. Silverman, R. 1987. The multiple polynomial quadratic sieve. Mathematics of Computation 48:329–39. Singh, S. 1999. The Code Book. London: Fourth Estate. VII.8 Mathematics and Economic Reasoning Partha Dasgupta 1 Two Girls 1.1 Becky’s World Becky, who is ten years old, lives with her parents andan older brother Sam in a suburban town in America’s Midwest.
Becky’s father works in a law firm specializing in small business enterprises. Depending on the firm’s profits, his annual income varies some what, but it is rarely below 145 000. Becky’s parents met in col - lege. For a few years her mother worked in publishing, but when Sam was born she decided to concentrate on raising a family. Now that both Becky and Sam attend school, she does voluntary work in local education. The family live in a two-story house.
It has four bed - rooms, two bathrooms upstairs and a toilet downstairs, a large drawing - cum-dining room, a modern kitchen, and a family room in the basement. There is a small plot of land in the rear, which the family use for leisure activities. erty, Becky’s parents own stocks and bonds and have a savings account in the local branch of a national bank. Although they have a partial mortgage on their prop Becky’s father and his firm jointly contribute to his retirement pension. He also makes monthly payments into a scheme with the bank that will cover college education for Becky and Sam.
The family’s assets and their lives are insured. Becky’s parents often remark that, federal taxes being high, they have to be careful with 896 money; and they are. Nevertheless, they own two cars, the children attend camp each summer, and the family take a vacation together once camp is over. Becky’sp are nts also remark that her generation will be much more prosperous than they. Becky wants to save the environment and insists on biking to school each day. Her ambition is to become a doctor.
1.2 Desta’s World Desta, who is about ten years old, lives with her parents and five siblings in a village in subtropical, southwest Ethiopia. The family live in a two - room, grass-roofed mud hut. Desta’s father grows maize and tef on half a hectare of land that the government has awarded him. Desta’s older brother helps him to farm the land and care for the household’s livestock: a cow, a goat, anda few chickens. The small quantity of tef produced is sold so as to raise cash income, but the maize is largely consumed by the household as a staple.
Desta’s mother works a small plot next to their cottage, growing cabbage, onions, and also serves as a staple). In order to supplement house-enset (a year-round root crop that hold income, she brews a local drink made from maize. As she is also responsible for cooking, cleaning, and minding the infants, her work day usually lasts fourteen hours. Despite the long hours, it would not bepossible for her to complete the tasks on her own.
(As the ingredients are all raw, cooking alone takes five hours or more.) So Desta and her older sister help their mother with household chores and mind their younger siblings. Although a younger brother at tends the local school, neither Desta nor her older sister has ever been enrolled there. Her parents can neither read nor write, but they are numerate. Desta’s home has no electricity or running water. Around where they live, sources of water, land for graz-ing cattle, and the woodlands are communal property. They are shared by people in Desta’s village;
but the villagers do not allow outsiders to make use of them. Each day Desta’s mother and the girls fetch water, collect fuelwood, and pick berries and herbs from the local commons. Desta’s mother frequently observes that the time and effort needed to collect their daily needs has increased over the years. either credit or insurance. As funerals are expensive occasions, Desta’s father long ago joined a commu-There is no financial institution nearby to offer nity insurance scheme (iddir) to which he contributes monthly.
When Desta’s father purchased the cow they now own, he used the entire cash he had accumulated

VII. The Influence of Mathematics

and stored at home, but had to supplement that with funds borrowed from kinfolk, with a promise to repay the debt when he had the ability to do so. In turn, when they are in need, his kinfolk come to him for a loan, which he supplies if he is able to. Desta’s father says that such patterns of reciprocity he and those close tohim practice are part of their culture, reflecting their norms of social conduct. He also says that his sons arehis main assets, as they are the ones who will look after him and Desta’s mother in their old age.
differences in the cost of living between Ethiopia and Economic statisticians estimate that, adjusting for the United States, Desta’s family income is about 5000 per year, of which 1000 is attributable to the products they draw from the local commons. However, asrainfall varies from year to year, Desta’s family income fluctuates widely. In bad years, the grain they store athome gets depleted well before the next harvest. Food is then so scarce that they all grow weaker, the younger children especially so. It is only after harvest that they regain their weight and strength.
Periodic hunger and illnesses have meant that Desta and her siblings are some what stunted. Over the years Desta’s parents have lost two children in their in fancy, stricken by malaria in one case and diarrhea in the other. There have also been several miscarriages. to a farmer, like her father) when she reaches eighteen Desta knows that she will be married (in all likelihood and will then live on her husband’s land in a neighbor-ing village. She expects her life to be similar to that of her mother.
2 The Economist’s Agenda That the lives people are able to construct differ enor-mously across the globe is a commonplace. In our age of travel, it is even a common sight. That Becky and Desta face widely different futures is also somethingwe have come to expect, perhaps also to accept. Nevertheless, it may not be out of turn to imagine that thetwo girls are intrinsically very similar: they both enjoy eating, playing, and gossiping; they are close to their families; they like pretty things to wear; and they both have the capacity to be disappointed, get annoyed, be happy.
Their parents are also alike. They are knowl-edge able about the ways of their worlds. They also care about their families, finding ingenious ways to meet the recurring problem of producing income and allocating resources among family members—over time and allowing for unexpected contingencies. So, a promising

VII.8. Mathematics and Economic Reasoning

route for exploring the underlying causes behind their vastly different conditions of life would be to begin by observing that the constraints the families face are very different: that in some sense Desta’s family are far more restricted in what they are able to be and do than Becky’s. Economics in large measure tries to uncover the processes that influence how people’s lives come to be what they are. The context may be a household, a vil-lage, a district, a state, a country, or the whole world.
In its remaining measure, the discipline tries to iden-tify ways to influence those very processes so as to improve the prospects of those who are hugely constrained in what they can be and do.by which I mean the style of economics taught and Modern economics, practiced in today’s graduate schools, does the exer-cises from the ground up: from individuals, through the household, village, district, state, country, to the whole world. In varying degrees the millions of individ-ual decisions shape the eventualities people all face;
as both theory and evidence tell us that there are enor-mous numbers of unintended consequences of what we all do. But there is also a feedback, in that those consequences go on to shape what people subsequently cando and choose to do. For example, when Becky’s family drive their cars or use electricity, or when Desta’s fam-ily create compost or burn wood for cooking, they contribute to global carbon emissions.
Their contributions are no doubt negligible, but the millions of such tiny contributions cumulatively sum to a sizable amount, having consequences that people every where are likely to experience in different ways. To understand Becky’s and Desta’s lives, we need first of all to identify the prospects they face for trans-forming goods and services into further goods and services—now and in the future, under various con-tingencies.
Second, we need to uncover the character of their choices and the pathways by which the choices made by millions of households like Becky’sand Desta’s go to produce the prospects they all face. Third, and relatedly, we need to uncover the pathwaysby which the families came to inherit their current circumstances. studying history we could, should we feel bold, take the long view—from about the time agriculture came The last of these is the stuff of economic history.
In to be settled practice in the Fertile Crescent (roughly, Anatolia) some eleven thousand years ago—and try to explain why the many innovations and practices that have cumulatively contributed to the making of

897

Becky’s world either did not reach or did not take hold in Desta’s part of the world. (Diamond (1997) is an enquiry into this set of questions.) If we wanted a sharper account, we could study, say, the past six hundred years and ask how it is that, instead of the several regions in Eurasia that were economically promising in about 1400 that made it and helped to create Becky’s world, evenc.e., it was the unlikely northern Europe while by passing Desta’s. (Landes (1998) is an inquiry into that question.
Fogel (2004) explores the pathways by which Europe during the past three hundred years has escaped permanent hunger.) As modern economics is largely concerned with the first two sets ofenquiries, this article focuses on them. However, the methods that today’s economic historians deploy to answer their questions are not dissimilar to the ones I describe below to study contemporary lives. The meth-ods involve studying individual and collective choices in terms ofthe theories are then tested by studying data relat-maximization exercises. The predictions of ing to actual behavior.
Even the ethical foundations ofnational economic policies involve maximization exercises: the maximization of social well-being subject to constraints. (The treatise that codified this approach to economic reasoning was Samuelson (1947).) 3 The Household Maximization Problem Both Becky’s and Desta’s households are microecono-mies. Each subscribes to particular arrangements over who does what and when, recognizing that it faces constraints on what its members are capable of doing.
We imagine that both sets of parents have their fam-ilies’ well-being in mind and want to do as well as they can to protect and promote it.1 Of course, both Becky’s and Desta’s parents would have a wider notion of what constitutes their families than I have allowed here. Maintaining ties with kinfolk would be an important aspect of their lives, a matter I return to later. One also imagines that Becky’s and Desta’s parents are interested in their future grandchildren’s well-being.
But asthey recognize that their children will in turn care about their that doing the best for their children amounts tochildren, they are right to conclude by recursion doing the best for their grandchildren, for their great grandchildren, and so on, down the generations. native would be to suppose that household decisions are reached by negotiation between the various parties (see Dasgupta 1993, chap-1. As suggested by Mc Elroy and Horney in 1981, a realistic alter ter 11). Qualitatively, nothing much is lost in my assuming optimizing households here.

898

stituents: health, relationships, place in society, and satisfaction at work are but four. Economists and psy-Personal well-being is made up of a variety of co nc ho log is ts have identified ways to represent well-being as a numerical measure. To say that some one’s well being is greater in situation say that her well-being measure is numerically higher Y than in situation Z is to in Y than in Z. A family’s well-being is an aggregate of its members’ well-beings.
As goods and services are among the determinants of well-being (some important examples are food, shelter, clothing, and medical care), the problem that both Becky’s and Desta’s parents face is to determine, from among those allocations of goods and services that are feasible, the ones that are best for their households. However, both pairs of parents care not only about today, but also about the future. More over, the future is uncertain.
So when the parents think about which goods and services their households should consume, they are concerned not just with the goods and services themselves, but also with when they will be consumed (food today, food tomorrow, and soon) and what will happen in the case of various contingencies (food the day after tomorrow if rainfall turns out to be bad tomorrow, and so forth). Implicitly or explicitly, both sets of parents convert their experience and knowledge into probabilistic judgments.
Some of the probabilities they attach to contingencies are no doubt very subjective, but others, such as their predictions about the weather, are arrived at from long experience. In subsequent sections we shall study the way in which Becky’s and Desta’s parents allocate goods and services across time and contingencies. But here we shall keep the exposition simple and consider a model that is static and deterministic. That is, we shall pretend that the people live in a time less world, and that they are completely certain about all the information they need in order to make their decisions.
whom we label 1 can appropriately model the well-being of household Suppose that a certain household has,2, . . . , N. Let us think about how we N members, member is taken to be a real number that depends in some wayi. As has already been mentioned, well-being on the goods and services consumed and supplied by It is traditional to divide goods and services into thosei. consumed bers to represent quantities of the former and negative and those supplied, and to use positive num numbers for the latter. Imagine now that there are commodities in all.
Let Y (j) represent the quantity of Mi thejth commodity that is consumed or supplied by i.

VII. The Influence of Mathematics

By our convention, Y (j) > 0 if j is consumed by ii

(e.g., food eaten or clothing worn) and Y (j) < 0 if ji

is supplied by$Y = (Y (1)$, . . . , Yi (e.g., labor). Now consider the vector(M)). It denotes the quantities of all the goods and services consumed or supplied bya point in$i^{i} R^{M}$—the Euclidean space ofi M dimensions.i. Yi is We now let that supplying goods and services decreases$U^{i}(Y^{i}) \text{denote i}$’s well-being. Let us assume$i$’s well being, while consuming them increases it.
Because the goods that are supplied byi are measured as nega- tive quantities, we can justifiably assume that Ui(Yi) increases as any of its components The next step is to generalize the model to one Yi increases. that applies to an entire household. The individual well-beings of the members of the household can be collected together so that they themselves forman N-dimensional vector, (U (Y ), . . . , U (Y )). The household’s well-being is dependent in some way on1 1 NN this vector. That is, we say that the well-being of the household is$W (U^{1}(Y^{1})$, . . .
, UN (YN )), for some func- tion simply the W . (Utilitarian philosophers have argued that sum of the U .) We also make the natural W isi

assumption that W is an increasing function of each Ui

(which is certainly the case if W is the sum of the U ).i

thebe thought of as the matrix you obtain if you make Let NMY denote the sequence (-dimensional Euclidean space Y1, . . . , YNR).NMY . It can alsois a point in a table of the amounts of each commodity consumedor supplied by each member of the household. Now, it is clear that not every$Y$ in RNMcan actually occur: after all, the total amount of any given commodity (inthe whole world, say) is finite. So we assume that$Y$ belongs to a certain setof all potentially feasible Jvalues of, which we regard as the set Y.
Within J we iden- tify a smaller set, This is the set of values of$F$, of “actually feasible” values of Y from which the household Y. could in principle choose. It is smaller than constraints that the household faces, such as the max-J because of imum amount of income it can earn.hold’s feasible set.2 The decision faced by a household F is the house- is to choose its well-being YW (Ufrom the feasible set1(Y1), . . . , UN (YNF))so as to maximize. This is called the household maximization problem It is reasonable, and mathematically convenient, to.
assume that the sets bounded subsets of RNMJ , and that the well-being func-and F are both closed and tion W is continuous. Since every continuous function than looking just at2. Presently we will see why we need to distinguish F. J from F, rather

VII.8. Mathematics and Economic Reasoning

on a closed bounded set has a maximum, it follows that the household maximization problem has a solution. If, in addition, Wis differentiable, the theory of nonlinear programming mality conditions the household’s choice must satisfy.can be used to identify the opti If those conditions are both necessary and sufficient. The F is a convex set and W is a concave function of Y, lagrange multipliers [III.64](/part-03/optimization-and-lagrange-multipliers) associated with F can be interpreted asto the household of slightly relaxing the constraints.notional prices:
they reflect the worth modern economist’s way of studying choice. First, letus assume that Let us conduct an exercise to test the power of the W is a symmetric and concave function of the individual well-beings if W were the sum of the U^i). The symmetry assump-Ui (as would be the case tion means that if two individuals exchange their well-beings, then Wis unchanged; and concavity means, roughly speaking, that, other things being equal, as a ULet us suppose in addition that the household members i increases, the rate of increase of W does not rise. are identical:
that is, let us set all the functions U to bei

equal to a single function, a strictly concave function of the U , say. Assume also that Y , which means that U isi

the rate of increase of well-being declines as consump-tion increases. Finally, assume that the feasible set F is nonempty, convex, and symmetric. (Symmetry means that if some Y is feasible, and the vector Z is the same as$Y$ except that the consumptions of a pair of indi- vi duals in the household have been exchanged, then Z is also feasible.) From these assumptions it can be shown that members of the household would be treated equally: that is, W is maximized when they all receive the same bundle of goods and services.
esis that the function To see why, we should note that, typically, 60–75%At low levels of consumption, however, the hypoth-U is concave is unreasonable. of the daily energy intake of some one in nutritional balance goes toward maintenance, while the remaining 25–40% is expended in discretionary activities (work and leisure). The 60–75% is rather like a fixed cost: over the long run a person needs it as a minimum no matter what he or she does.
The simplest way to uncover the implications of such fixed costs is to continue to suppose that case, for example, if there is a fixed quantity of food F is convex (which is the for allocation among members of the household), but that U is a strictly convex function at low in takes of food and a strictly concave function there after. It is not hard to show that a poor household in such aworld will maximize its well-being by allocating food 899 unequally among its members, while a rich household can afford the luxury of equal treatment and will choose to distribute food equally.
Suppose, to takea very stylized example, that energy requirement for daily maintenance is 1500 kcal and that a householdof four can obtain at most 5000 kcal for consumption. Then equal sharing would mean that no one would have sufficient energy for any work, so it is better to share the food unequally. On the other hand, if the household is able to obtain more than 6000 kcal, it can share the food equally with out jeopardizing its future. There are empirical correlates of this finding.
When food is very scarce, the younger and weaker members of Desta’s household are given less to eat than the others, even after allowance is made for differences in their ages. In good times, though, Desta’s parents can afford to be egalitarian. In contrast, Becky’s household can always afford enough food. Her parents therefore allocate food equally every day. 4 Social Equilibrium Household transactions in Becky’s world are carried out mostly in markets. The terms of trade are the quoted market prices.
In developing a mathematical construction of social out comes, I continue to imagine, for sim - plicity, a static, deterministic world. Let P (⩾ 0) be the vector of market prices and let M (⩾ 0) be the vector of a household’s endowments of goods and services.(That is, for each commodityj, P (j) is the price of j andhas.) Recalling our convention that goods consumed M(j) is the amount of j that the household already are of positive sign and goods supplied are of nega-tive sign, define$X = Y$.
(Thus, X(j) = Y (j) is the total amount of commodity household.) Then P · X is the total price of goods con-i j that is consumed by thei sumed by the household, minus the total price of goods supplied, and P ·M is the total value of its endowments. The feasible set satisfy the “budget” constraint F is the set of household choices P · (X - M) ⩽ 0. Y that assets it supplies to the market is determined by mar-ket prices (Becky’s father’s salary, interest rates on The income that Becky’s household earns from the bank deposits, returns on shares owned).
Those prices in turn depend on the size and distribution of house-hold endowments of goods and services and on household needs and preferences. They depend too on the ability and willingness of institutions, such as private firms and the government, to make use of the rights they in turn have been awarded. These functional relationships explain why Becky’s father’s skills as a lawyer

900

(itself an asset, termed “human capital” by economists)would not be worth much in Desta’s village, even though they are much valued in the United States. In fact, it was a firm belief that lawyers would continueto prove valuable in the United States that encouraged Becky’s father to Although Desta’s household does operate in mar-be a lawyer. kets (when her father sells tef or her mother sells the liquor she has brewed), it under takes many trans ac-tions directly with nature; in the local commons and in farming, and in nonmarket relationships with oth-ers in the village.
Therefore the$F$that Desta’s household faces is not defined simply by a linear budget inequality, as in the idealized model we have con-struc ted to display Becky’s world, but also reflects the constraints that nature imposes, such as soil produc-tivity and rainfall, the assets it has access to, and the terms and conditions involving transactions with others in the village via nonmarket relationships, a mat-ter I come to later. The constraints imposed by nature are felt by Becky’s household too, but through market prices.
For example, should a drought lead to a fall in world cereal production, it would become noticeable to Becky’s household through the high price of cereal. Desta’s household, in contrast, would notice it directly from the reduced harvest from their field. Desta’s household assets include the family home, livestock, agricultural implements, and their half hectare of land. The skills Desta’s family members have accumulated in farming, managing livestock, and collecting resources from the local commons are partof their human capital.
Those skills do not command much return in the global marketplace, but they do shape the household’s feasible set family’s well-being. Desta’s parents learned those skills F and are vital to the from their parents and grandparents, just as Desta andher siblings have learned them from their parents and grandparents. Desta’s family can also be said to own a portion of the local commons: in effect, her household shares its ownership with others in the village.
Difficulties in reaching and enforcing agreement with neigh-bors over the use of the local commons are less severe than they are in the case of global commons, such as the atmosphere as a sink for carbon emissions. Thisis not only because the required negotiations involve far fewer people when the commons are local, but also because there is likely to be greater congruence of opinions and interests among the users. It helps too that the parties are able to observe whether the agreements they made over the use of local commons are being kept.

VII. The Influence of Mathematics

(See below in our discussion of insurance arrangements in Desta’s world.) affected by the choices that other people make: this Thus, the choices that are available to individuals are results in feedback. In a market economy, the feed-back is in large part transmitted in prices. In nonmarket economies the feedback is transmitted through the terms in which households are able to negotiate with one another. Let us try to model this situation mathematically. We start by imagining an economy of For ease of exposition, I shall suppose that a house-H households.
hold’s well-being can be expressed directly in terms ofits aggregate consumption of goods and services, disregarding how this consumption is distributed among the individual members. Let X denote the consump- tion vector in household tion), let J be the set of potentially feasible vector sh (with the usual sign conven-h X , and let(Wh)h(Xh) be h’s well-being.$h$ Within$h$’s potentially feasible set$J^{h} \text{of consump}-$ tion vectors lies the actual feasible setto model the feedback we shall explicitly recognize$F^{h}$. In order that holds.
That is, it is a function of the sequence Fh depends on the consumptions of other house-(X , . . . , Xthis sequence, which consists of every household’s$h^{-}1$, Xh+1, . . . , XH). To save space, we shall denote1 consumption vector except is a function (some times called a “correspondence”) h’s, by$X^{-h}$. Formally, Fh that takes objects of the form Household$h$’s economic problem is to choose its con-$X^{-h} \text{to subsets of} J^{h}$. sumption Xh from its feasible set Fh(X-h) in such a way as to maximize its well-being mum choice depends on$h$’s beliefs about$W^{h}(X^{h} X)$.
The opti-^- and theh

correspondence$F^{h}(X^{-h})$.

lar calculations. How can we unravel the feedbacks?One way would be to ask people to disclose their Meanwhile, all other households are making simibeliefs about the feedbacks. Fortunately, economists avoid that route. So as to anchor their investigation, economists study that are self-confirming. The idea is to identify states equilibrium beliefs; that is, beliefs of affairs where the choices people make on the basis of their beliefs about the feedbacks are precisely those that give rise to those very feedbacks. We call any such state of affairs a social equilibrium.
Formally, a sequence a social equilibrium((X*)1, . . . , XH^*if, for every) of household choices is call edh, the choice X^* of household choices of Xh maximizes the well-being in its feasible set F (X* W).h(Xh) over allh(hh)-h VII.8. Mathematics and Economic Reasoning rium exist? Classic papers by Nash in 1950 and Debreu in 1952 showed that, under a fairly general set of con-This raises an obvious question: does a social equ i lib ditions, it always does. Here is a set of conditions that Debreu identified.
Assume that each well-being function that for any potentially feasible choice Wh is continuous and quasi - concave X^ (which means in J , the set$of$ W X(h Xin^ ) Jis convex). Assume also that for every house - h for which Wh(Xh) is greater than or equal t(oh)h hold ofh J ) is nonempty, compact, and convex, and depends hh, the feasible set Fh (recall that this is a subset continuously on the choices holds.
The proof that under the above conditions ah X - h made by other house- social equilibrium always exists is a relatively straight-forward use of the kakutani fixed point theorem [V.11 §2](/part - 05/fixed - point - theorems), which is itself a generalization of Brouwer’sfixed point theorem. Alternative sets of sufficient conditions for the existence of social equilibria (which allow the feasible set been explored in recent years. Fh(X-h) to be nonconvex) have ket equilibrium tor In Becky’s world, a social equilibrium is called a P* (⩾ 0) and a consumption vector.
A market equilibrium is a price vec-X* for each mar- householdto the budget constrainth, such that X(P*()h){*}maximizes· (X - M W)h⩽(X0, and suc(hh)h) subject that the demands for goods and services across house-holds are feasible (i.e.,$(X - M^{h} ) ⩽^{h}0)$. That market equilibria are social equilibria, in the sense in which we hh have defined the latter term here, was demonstrated by Arrow and Debreu in 1954. Debreu (1959) is the definitive treatise on market equilibria. In that book, debreu followed the leads of Erik Lindahl and Kenneth J.
Arrow, by distinguishing goods and services not only in terms of their physical characteristics, but also in terms ofthe date and contingency in which they appear. Later in this article we shall expand the commodity space inthat way to study savings and insurance decisions in both Becky’s and Desta’s worlds. lib rium is just or collectively good.
More over, except for the most artificial examples, social equilibrium is One cannot automatically assume that a social equ i not unique—which means that a study of equilibria per se leaves open the question of which social equilibrium we should expect to observe. In order to probe that question, economists study disequilibrium behavior and analyze the stability properties of the result-ing dynamic processes. The basic idea is to hypothesize about the way people form beliefs about the way the world works, track the consequences of those patterns of learning, and check them against data. It

901

is reasonable to limit such a study by considering only those learning processes that converge to a social equilibrium in stationary environments. Initial beliefs would then dictate which equilibrium is reached in the long run (see, for example, Evans and Honkapohja2000). Since the study of disequilibria would lengthen this article greatly, we shall continue to study social equilibria here. 5 Public Policy Economists distinguish between what they call goods and public goods. For many goods, consumption private is rivalrous:
if you consume a bit more from a given sup-ply of such a good (e.g., food), others have that much less to consume. These are private goods. The way to assess their consumption through out the economy is to add up the amounts consumed by all individual house-holds; which is what we did in the previous section when arriving at the notion of a social equilibrium. Notall goods are like that, however. For example, the extent of national security on offer to you is the same as thaton offer to all households in your country. In a just society the law has that same property, as has the state:
not only is consumption not rivalrous, but in addition, no one can be prevented from availing himself or herself of the entire amount available in the economy. Public goods are goods of this second kind. One models the quantity of a public good as a number G, and the quan- tity equal Gh G. An example of a public good that has a global consumed by each household h is deemed to coverage is the Earth’s atmosphere: the whole world benefits from it jointly. vi duals, then problems arise.
For example, even though every one in a city would benefit from a cleaner, health-If the supply of public goods is left to private indiier environment, individuals have a strong incentiveto free-ride on others when it comes to paying for that cleaner environment. Samuelson showed in 1954 that such a situation resembles the prisoner’s dilemma: each party has a strategy that is best for him/her, regard less of what strategies the other parties choose, even though there is another set of strategies, one per party, that is better for everybody.
Under such circumstances, one usually needs public measures, such as taxes and subsidies, in order for it to be in the interest of private individuals to act in a way that implements the collectively preferred out comes. In other words, the dilemma can be expected to be resolved effectively notby markets but by politics. It is widely accepted in political theory that government should be charged with

902

imposing taxes, subsidies, and transfers, and should be engaged in supplying public goods. The government is also the natural agency to supply infrastructure, such as roads, ports, and electrical cables, requiring as theydo investments that are huge in comparison with individual in comes. We shall now extend our earlier model to include public goods and infrastructure, so that we can study the government’s economic task. aggregate of household well-beings. Thus, ifwell-being, we write it as Let us assume that social well-being is a numerical V (W , . . . , W ).
It is natural to V is social postulate that example of such a function V increases as any1 V is the one prescribed (WH)h increases. (One by utilitarian philosophy, namely, government chooses what quantities to supply of the W1 + · · · + WH.) The various public goods and infrastructure commodities. These numbers can be modeled by two vectors, which we will call G and I , respectively. The government also chooses to impose on each household fers Th of goods and services (for example, providingh certain trans- health care and charging income tax). Let us write for the sequence(T , . . . , T ).
Whether or not a partic-T ular choice of vectors1 G and^H I is actually feasible for the government will depend onbe the set of feasible pairs of vectors T, so we define(G, I )$, \text{given the} K^{T} to$ choice of Because we have introduced a new set of goods, we T . shall have to modify the household well-being functions by enlarging their domains. The obvious nota-tion to express this extra dependence is to write Wmo re over, h(Xh, G, Ih,’s feasible set Th) for the well-being of household F now also depends on Gh., I, and Th;
so we write the set of feasible household$h$ choices as To try to determine the optimum public policy,$F^{h}(G$, I, Th, X-h). imagine a two-stage game. The government has the first move, choosing T and then G and I from K . Households go second, reacting to decisions made$T$ by the government. Imagine that a social equilibrium$X^{*} = (X^{*}$, . . . , X*) is reached and that the equilibrium is unique.
(We assume that if there are multiple equ i lib-ria, the government can select among them by resort-1$H$ ing to public signals.) Clearly, this equilibrium$X^{*} \text{is a}$ function of government will anticipate it and choose G, I, and T . An intelligent and benevolent T , G, and I from social well-being KT in such a way as to maximize the resulting V (W (X*), $. . . , W (X^{*} ))$. The public policy problem we have just designed,1$H$ involving as it does a double optimization, is techni-cally very difficult. It transpires, for example, that even

VII. The Influence of Mathematics

in some of the simplest model economies one can imag-ine, F (G, I, T , X- ) is not convex. This means that the social equilibrium cannot be guaranteed to depend con-tinuous ly on$h G^{h}$, I, andh T , as was shown by Mirrlees in

1984. This in turn means that standard techniques arenot suitable for the government’s optimization problem. In fact, of course, even “double optimization” isa huge simplification. The government chooses; people respond by trading, producing, consuming; the government chooses again; people respond once again—andso forth in an unending series of moves and countermoves. Identifying the optimum public policy involves severe computational difficulties. 6 Matters of Trust:
Laws and Norms The previous examples demonstrate that a fund a men-tal problem facing people who would like to transact with one another concerns trust. For example, the extent to which parties trust one another shapes the sets F and KT. If the parties do not trust one another, what could have been mutually beneficial transactions will not take place. But what grounds does a person$h$ have for trusting some one to do what he promises todo under the terms of an agreement? Such grounds can exist if promises can be made credible.
Societies every where have constructed mechanisms to create credibil-ity of this kind, but in different ways. What the mechanisms have in common, however, is that individuals who fail to comply with agreements with out a good reason are punished. How does that common feature work? embodied in the law. The markets Becky’s family enters In Becky’s world the rules governing transactions are are supported by an elaborate legal structure (a pub-lic good). Becky’s father’s firm, for example, is a legal entity;
as are the financial institutions he deals with in order to accumulate his retirement pension, to save for Becky’s and Sam’s education, and so on. Even when some one in the family goes to the grocery store, the purchases (paid for with cash or by card) involve the law, which provides protection for both parties (the grocer, in case the cash is counterfeit or the card is void; the purchaser, in case the product turns out on inspection to be substandard). The law is enforced bythe coercive power of the state. Transactions involve legal contracts backed by anthe state.
It is because Becky’s family and the grocery external enforcer, namely, store’s owner are confident that the government hasthe ability and willingness to enforce contracts (i.e., to

VII.8. Mathematics and Economic Reasoning

continue to supply the public good in question) that they are willing to make transactions. What is the basis of that confidence? After all, the contemporary world has shown that there are states and there are states. Why should Becky’s family trust the government to carry out its tasks in an honest man-ner? A possible answer is that the government in her country worries about its i tive press in a democracy helps to sober the govern-reputation: a free and inquisment into believing that in competence or malfeasance would mean an end to its rule come the next election.
Notice how the argument involves a system of inter-locking beliefs about the abilities and intentions of others. The millions of households in Becky’s country trust their government (more or less!) to enforce contracts, because they know that government leaders know that not to enforce contracts efficiently would mean being thrown out of office. In their turn, each party to a contract trusts the other to refrain from reneging (again, more or less!), because each knows that the other knows that the government can be trusted to enforce con - tracts. And so on.
Trust is maintained by the threat of punishment (a fine, a jail term, dismissal, or what - ever) for anyone who breaks a contract. Once again, we are in the realm of equilibrium beliefs, held togetherby their own bootstraps. Mutual trust encourages people to seek out mutually beneficial transactions and engage in them. As the formal argument that supports the above claim is very similar to the one showing that social norms contain mechanisms for enforcing agreements, we turn to the place of social norms in people’s lives.
Although the law of contracts exists also in Desta’s country, her family cannot depend on it because the nearest courts are far from their village. More over, there are no lawyers in sight. As transport is enormously costly, economic life is shaped out side a for-mal legal system. In short, crucial public goods and infrastructure are either unavailable, or, at best, in short supply. But even though there is no external enforcer, Desta’s parents do make transactions with others.
Credit (not dissimilar to insurance in her village) involves saying, “I will lend to you now if you promise to repay me when you can.” Saving for funerals involves saying, “I agree to abide by the terms and conditions ofthe iddir.” And so on. But why should the parties have any confidence that the agreements will not be broken? mutually enforced Such confidence can be justified if agreements are. The basic idea is this: a credible 903 threat by members of a community that stiff sanc-tions will be imposed on anyone who breaks an agreement can deter every one from breaking it.
The problem is then to make the threat credible. In Desta’s world credibility is achieved by recourse to social norms ofbehavior. by members of a community. A rule of behavior (or By a social norm we mean a rule of behavior followed “strategy” in economic parlance) reads like, “I will do X if you do Y,” “I will do P if Q happens,” and so forth. For a rule of behavior to be a social norm, it must be in the interest of every one to act in accordance with the rule if all others act in accordance with it. Social norms are equilibrium rules of behavior.
We will now see how social norms work and how transactions based on them compare with market-based transactions. To do this wewill study insurance as a commodity. 7 Insurance To insure one self against a risk is to act in waysto reduce that risk. (Formally, a random variable [III.71 §4](/part - 03/probability - distributions) ̃able ̃ Yif there is a random variable  ̃X is said to be riskier than a random vari-Z with zero mean such that  ̃$X$has the same distribution as  ̃$Y + Z$ ̃.
In this case,  ̃“spread out.”) As long as it does not cost too much, risk-$X$and  ̃$Y$have the same mean but  ̃X is more averse households will want to reduce risk by purchas-ing insurance: in fact, avoiding risk would seem to be a universal urge. To formalize these notions, consider anisolated village, such as Desta’s. Suppose for simplicity that it contains$h$’s food consumption is H identical households. If household X (represented by a single real number), let us say that its well-being is We shall assume that$W (X^{h}) > 0 (\text{that is}$, more food W (Xh).
leads to greater well-being) and that more food you already have, the less you benefit from$h W (X^{h}) < 0 (the$ yet more). We shall confirm below that the second prop-erty of W , its strict concavity, implies, and is implied by, risk a version; but the basic reason is simple: if W is strictly concave, then you gain less when you are lucky than you lose when you are unlucky. food by a household to rs such as the weather, involves no effort.
Since the For simplicity, let us suppose that the production ofh, which is subject to chance fac- output is uncertain, we represent it by a random variable  ̃positive. We shall denote expectations by$X^{h}$, with expected value μ, which is assumed to be E. expected well-being is simply If a household$h$is completely self-sufficient, then its E(W (X ̃h)). However, the

904

strict concavity ofput this in words: Wh’s well-being at the average level of implies that W (\mu) > E(W (X ̃h)). To production is greater than the expectation of$h$’s well being if the production is random. This means that will prefer a sure level of consumption to a risky one$h$ with mean equal to that sure level. In short, averse. Define a numbe$\bar{r}\mu by W (\mu)$= E(W (X ̃h))is risk. S$\bar{o}\mu$ is the level of production that achieves the expected well-being. This will be less thanμ, and so \mu h- \m. ar{u} is a measure of the cost of the risk that a self-sufficient household bears.
Notice that the greater the “curvature” ofciated with  ̃W is, the greater the cost is of the risk asso-X . (A useful measure of curvature turns out to be measure when discussing intertemporal choices.) To$-XW^{h} (X)/W (X)$. We will make use of this see how households could gain by pooling their risks, let us write  ̃$X = \mu +$ ̃ε, where  ̃ε is a random vari- able with mean zero, variance$h^{h} σ^{2}$, and finite support.$h$ Suppose for simplicity that the random variables  ̃are identical (i.e., they do not depend onh).
Let theεh correlation coefficient of any two of these dis tr ibu-tions beρ.
It turns out that, as long as ρ < 1, house- holds can reduce their risks by agreeing to share their outputs. Suppose that households are able to observe one another’s outputs. Given that the random variables Xshare out the outputs equally. Under this scheme, ̃h are identical, the obvious insurance scheme is toh’s uncertain food consumption becomes the average of X̃, . . . , X ̃ , which is an improvement on self-sufficiency because that, with out an enforcement mechanism, the agree-1 HE(W ( X ̃ h^ /H)) > E(W (X̃h)).
The problem is ment to share will not stick, because once each house-hold knows how much food every household has produced, all but the unluckiest households will wish to renege. To see why, notice first that the luckiest households will renege because their outputs are above the average; but this means that the next luckiest set of households will renege because their outputs are above the reduced average; and so on, down to the unlucki-est households.
Since households know in advance that this will happen if there are no enforcement mecha - nisms, they will not enter the scheme in the first place: the only social equilibrium is pure self-sufficiency and there is no pooling of risk. Let us call the insurance game just described the stage game social equilibrium for the stage game, we shall now see. Although pure self-sufficiency is the only that the situation changes if the game is played repeatedly. To model this, let us use the letter time, and let us take time to be a nonnegative integer.t to denote VII.
The Influence of Mathematics (The game might, for instance, take place every year, with 0 standing for the current year.) Let us assume that the villagers face the same set of risks in each time period, and that the risk in one year is independent of the risks in all other years. Also assume that, in each period, once food outputs are realized, households decide independently of one another whether they will abide by the agreement to share their produce equally or whether they will renege on it. hold, it will typically be less important than present well - being.
To model this we introduce a positive Although future well-being is important to a house parameter discounts its future well - being. The assumption is that,δ, which measures how much a household when making calculations atits well-being at timet by a factort = 0, a household divides(1 + δ)t: that is, the importance decays by a certain fixed percentage at each time period.
We shall now show that, provided δcare enough about their future well - being), there is ais sufficiently small (i.e., provided that households social equilibrium in which households abide by the agreement to share their aggregate output equally. Let  ̃Yh(t) be the uncertain amount of food avail- able to household participating in the agreement, then  ̃h at time t. If all households are Yh(t) will be\mu will be+ ( ̃$ε\mu^{h})/H+$ ̃ε, and if there is no agreement, then it. At time t = 0 the total expected well-being of household$\infty E$ ̃$h + t h$, present and future$, is(W (Yh(t)))/(1 δ)$.
(To calculate this we took, for eacht0 and divided it byt ⩾ 0, the expected well-being of(1 + δ)t. Then we added theseh at time numbers up.) might adopt: it begins by participating in the insur-ance scheme and continues to participate so long as no Now consider the following simple strategy that$h$ household has reneged on the agreement; but it with-draws from the scheme from the date following the first violation of the agreement by some household. Game theorists have christened this the “grim strategy,” or simply grim, because of its unforgiving nature.
Let us see how grim could support the original agreement to share aggregate output equally at every date. (For a general account of repeated games and the variety of social norms that can sustain agreements, see Fudenberg and Maskin (1986).) households have chosen grim. Then Suppose that householdh believes that all otherh knows that none of the other households will be the first to defect. What should$h$do then? We will show that ifδ is small enough, h can do no better than play grim. As the same

VII.8. Mathematics and Economic Reasoning

reasoning would be applicable to all other households, we should conclude that, for small enough values ofδ, grim is an equilibrium strategy in the repeated game. But if all households play grim, then no household will ever defect. Grim can therefore function as a social norm for sustaining cooperation. Let us see how the argument works. The basic idea is simple. As all other households are assumed to be playing grim, household enjoy a one-period gain by defecting if its own outputh would exceeded the average output of all households.
But if defects in any period, all other households will defecth in all following periods (they are assumed to be playing grim, remember). Therefore,$h$’s own best option in all following periods will be to defect also, which means that subsequent to a single deviation bycome can be predicted to be pure self-sufficiency. So, h, the out- set against a one-period gain that household enjoy if its output exceeded the average output of allh would households is the loss it would suffer from the follow-ing date because of the breakdown of cooperation.
That loss exceeds the one-period gain ifδ is small enough. So, ifbut will adopt grim; implying that grim is an equ i lib-δis sufficiently small, householdh will not defect, rium strategy and equal sharing among households in every period is a social equilibrium. uation in which To formalize the above argument, we consider the sit-$h$’s incentive to defect is greatest. Let$A$ andof any household.
Then the maximum gain that house-B be the minimum and maximum possible outputs holdh could possibly enjoy from defecting at t = 0 arises ifh happens to produce B and all other house- holds happen to produce in this eventuality is(B + (HA. Since the average output- 1)A)/H, the one-period gain that householdh would enjoy from defecting is W (B) - W B + (HH- 1)A .

But each subsequent period (i.e., fromh knows that if it defects, the expected loss int = 1 onward) will bethe notation, let us write E(W ( X ̃h^ /H)) - E(W (EX(W ( ̃h)). In order to simplify X ̃$/H)) - E(W (X$ ̃)) as total L. household loss it will suffer from defecting ath can then calculate that the expect edh^  t = 0 ish Le xce eds the present gain from defecting, then house-. nfty1 (1 + δ)-t, which equals L/δ. If this future loss hold not want to defect ifh will not want to defect. In other words, h will Lδ > W (B) - W B + (HH- 1)A$905 or$3δ < L W (B) - W B + (HH- 1)A . (1)

But ifone-period gain from defection is the largest possible,$h$does not find it in its interest to defect when the it will certainly not want to defect in any other situa-tion. We conclude that if inequality (1) holds, then grim is an equilibrium strategy and equal sharing among households in every period is a resulting social equilibrium. Notice that, as we said, this will happen ifδ is sufficiently small. We usually reserve the term “society” to denote a collective that has managed to find a mutually bene-ficial equilibrium.
Notice, however, that another social equilibrium of the repeated game is each household for itself. If every one believed that all others would break the agreement from the start, then every one would break the agreement from the start. Noncooperation would involve each household selecting the strategy: renege on the agreement. Failure to cooperate could be due simply to a collection of unfortunate, selfconfirming beliefs, and nothing else. It is also easy toshow that noncooperation is the only social equilibrium of the repeated game if$3δ > L W (B) - W B + (HH - 1)A$. (2)

a community can slide from cooperative to noncoop-era tive behavior. For example, political instability (in We now have in hand a tool for understanding how the extreme, civil war) can mean that households are increasingly concerned that they will be forced to dis-perse from their village. This translates into an increase inis now bent on destroying communal institutions inδ. Similarly, if households fear that their government order to strengthen its own authority,δ will increase. But from (1) and (2) we know that ifc ient ly, then cooperation ceases.
The model thereforeδincreases suffioffers an explanation for why, in recent decades, coop-eration at the local level has declined in the unsettled regions of sub-Saharan Africa. Social norms work only when people have reasons to value the future benefits of cooperation. In the above analysis, we allowed for the possibility that, in each period, household risks were positively correlated. More over, the number of households in any village is typically not large. These are two reasons why Desta’s household is unable to attain anything like full insurance against the risk they face.
Becky’s parents, in contrast, have access to an elaborate set of insurance markets that pool the risks of hundreds of thousands

906

of households across the country (even the world, ifthe insurance company is a multinational). This helps to reduce individual risk more than Desta’s parents can, because, first, spatially distant risks are more likely to be uncorrelated, and, second, Becky’s parents can pool their risk with many more households.
With enough households and enough independence of their risk, law of large numbers [III.71 §4](/part-03/probability-distributions) practically guaran-the tees that equal sharing among those households will provide each one with the averagetage of markets, backed by the coercive power of theμ. This is an advan- state as an external enforcer: in a competitive market, insurance contracts are available, enabling people who do not know one another to do business through third parties, in this case the insurance companies.
low rainfall, will in fact be very similar for all house-holds in their village. Since the insurance they are able Many of the risks that Desta’s parents face, such as to obtain within their village is therefore very limited, they adopt additional risk-reducing strategies, such as diversifying their crops. Desta’s parents plant maize, tef, and enset (an inferior crop), with the hope that even if maize were to fail one year, down. That the local resource base in Desta’s village enset would not let them is communally owned probably also has something todo with a mutual desire to pool risks.
Woodlands are spatially nonhomogeneous ecosystems. In one year one group of plants bears fruit, in another year some other group does. If the woodland were divided into private parcels, each household would face a greater risk thanit would under communal ownership. The reduction in individual household risks owing to communal own-ership may be small, but as average in comes are very low, household benefits from communal ownership are large.
(For a fuller account of the management of local commons in poor countries, see Dasgupta (1993).) 8 The Reach of Transactions and the Division of Labor Payments in Becky’s world are made in money, expressed in U.S. dollars. Money would not be requiredin a world where every one was known to be utterly trustworthy, people did not incur computational costs, and transactions were cost less: simple IOUs, stipulating repayment in terms of specific good and services, would suffice in that world. However, we do not live in that world.
A debt in Becky’s world involves a contract specifying that the borrower is to receive a certain number of dollars and that he promises to repay the

VII. The Influence of Mathematics

lender dollars in accordance with an agreed schedule. When signing the contract the relevant parties entertain certain beliefs about the dollar’s future value in terms of goods and services. Those beliefs are in part based on their confidence in the U.S. government to manage the value of the dollar. Of course, the beliefs are based on many other things as well; but the important point remains that money’s value is maintained only because people believe it will be maintained (the classic refer-ence on this is Samuelson (1958)).
Similarly, if, for whatever reason, people feared that the value would not be maintained, then it would not be maintained. Currency crashes, such as the one that occurred in Weimar Germany in 1922–23, are an illustration of how a loss in confidence can be self - fulfilling. Bank runs share that feature, as do stock market bubbles and crashes. Toput it formally, there are multiple social equilibria, each supported by a set of self-fulfilling beliefs. mous.
Becky frequently does not know the salespeople in the department stores of her town’s shopping mall, The use of money enables transactions to be anonynor do they know Becky. When Becky’s parents borrow from their bank, the funds made available to them come from unknown depositors. Literally millions of trans ac-tions occur each day between people who have never met and will never meet in the future. The problem of creating trust is solved in Becky’s world by building confidence in the medium of exchange: money.
The value of money is maintained by the state, which has an incentive to maintain it because, as we saw earlier, it wishes not to destroy its reputation and be thrown out of office. In the absence of infrastructure, markets are unable to penetrate Desta’s village. Becky’s suburban town, bycontrast, is embedded in a gigantic world economy. Becky’s father is able to specialize as a lawyer only because he is assured that his income can be used to purchase food in the supermarket, water from the tap, and heat from cooking ovens and radiators.
Specializa-tion enables people to produce more in total than they would be able to if they were each required to diversify their activities. Adam Smith famously remarked that the division of labor is limited by the size of the mar - ket. Earlier we noted that Desta’s household does not specialize, but produces pretty much all of its daily requirements from a raw state. More over, the many transactions it enters into with others, being supportedby social norms, are of necessity personalized, thus limited.
There is a world of a difference between laws and social norms as the basis of economic activities. VII.8. Mathematics and Economic Reasoning 9 Borrowing, Saving, and Reproducing If you do not have insurance, then your consumption will depend heavily on various contingencies. Purchasing insurance helps to smooth out this dependence. We shall see presently that the human desire to smooth out the dependence on contingencies is related to the equally common desire to smooth out consumption across time: they are both a reflection of the strict concavity of the well-being function W.
The flow of income over a person’s lifetime tends not to be smooth, so people look for mechanisms, such as mortgages and pensions, that enable them to transfer consumption across time. For instance, Becky’s parents took out a mortgage on their house because at the time of pur-chase they did not have sufficient funds to finance it. The resulting debt decreased their future consumption, but it enabled them to buy the house at the time they did and there by raise current consumption. Becky’s parents also pay into a pension fund, which transfers present consumption to their retired future.
borrowing for current consumption transfers future consumption to the present; saving achieves the reverse. Since capital assets are productive, they can earn positive returns if they are put to good use. This is one reason why, in Becky’s world, borrowing involves having to pay inter - est, while saving and investing earn positive returns. Becky’s parents also make a considerable investment in their children’s education, but they do not expect tobe repaid for this. In Becky’s world, resources are transferred from parents to children. Children are a direct source of parental well - being;
they are not regarded as investment goods. ents face when they arrange transfers of resources across time is to imagine that they view themselves as A simple way to formulate the problem Becky’s par part of a dynasty. This means that, in reaching their consumption and saving decisions, they take explicit note not only of their own well-being and the well-being of Becky and Sam, but also of the well-being of their potential grandchildren, great grandchildren, and so on, down the generations. assume that time is a continuous variable.
At time To analyze the problem, it is notationally tidiest to t(which we take to be greater than or equal to 0), let K(t) denote household wealth and X(t) the consump- tion rate, which is some aggregate based on the market prices of what they consume. In practice, a house-hold will want to smooth its consumption across both time and contingencies, but in order to concentrate on

907

time we shall consider a deterministic model. Suppose that the market rate of return on investment is a positive constantat timet is K(t)r . This means that if household wealth, then the income it earns from that wealth atthe dynasty’s consumption options over time is thent is r K(t). The dynamical equation describing$d$ K(t)/dt = r K(t) - X(t). (3) The right-hand side of the equation is the difference between the dynasty’s investment income at time t (which isatt.
This amount is saved and invested, so it gives ther times its wealth at t) and its consumption rate of increase of the dynasty’s wealth attime is$t = 0 and K(0)$is the wealth that Becky’s parent st. The present have inherited from the past. Earlier, we assumed that the household allocates its consumption across contingencies by maximizing its expected well-being. The cor-responding quantity for allocating consumption across time is

. nfty W (X(t))(e-)δt dt, (4)

where, as before, we assume that ditions$W (X) > 0 and^{0} W (X) < W0$. The parameter satisfies the con-δ is once again a measure of the rate at which future well-being is discounted—owing to short sight edn ess, the possibility of dynastic extinction, and so on. the difference between this and the previousδ is that now we are considering a continuous model rather than a discrete one, but the decay is still assumed to be exponential. In Becky’s world the rate of return on invest-ment is large; that is, investment is very productive. So it makes empirical sense to suppose that$r > δ$.
We will see presently that this condition provides Becky’sp are nts with the incentive to accumulate wealth and pass it on to Becky and Sam, who in turn will accumu-late their wealth and pass that on, and so on. For simplicity, let us suppose that the “curvature” ofis$-XW (X)/W (X)$, is equal to a parameter Wα, whose, which value exceeds 1.3 As we saw earlier, strict concavity ofsumption than you lose from decreasing it by the same W means that you gain less from increasing con- amount. The strength of this effect is measured byα:
is a positive number) and arbitrary constants that arise when we integrate the curvature of3. This means that W has the form B (which can be of either sign) are the two B - AX^-^(α^-^1^), where A (which W to arrive at adopted for WA itself. We will see presently that the values that areand Bhave no bearing on the decisions that Becky’s parents will want to make; that is, Becky’s parents’ optimum decisionis independent of A and B. Notice that, as α > 1, W (X) is bounded above.
The above form is particularly useful in applied work, because in order to estimate W (X) from data on household consumption, one has to estimate only one parameter, behavior in the United States have revealed thatα. Empirical studies of savingαis in the range 2–4.

908

the larger it is, the greater the benefit of any smoothing you are able to do. Becky’s parents’ problem att = 0 is to maximize the quantity in (4) by making a suitable choice of the rate at which they consume their wealth (namely, X(t)), sub- ject to the condition (3), together with the conditions that K(t) and X(t) should not be negative.4 This is a problem in the calculus of variations [III.94](/part-03/variational-methods). But it is of a some what unusual form, in that the horizon is infinite and there is no boundary condition at infinity.
The reason for the latter is that Becky’s parents would ideally like to determine the level of assets that the dynasty ought to aim at in the long run; they donot think it is appropriate to specify it in advance. If we assume for the moment that a solution to the opti-mization problem exists, then it turns out that it must satisfy the Euler–Lagrange equation:

$α(dX(t)/dt) = (r - δ)X(t)$, t ⩾ 0. (5)

This equation is easily solved, and gives

X(t) = X(0)e((r-)δ)t/α. (6)

However, for this problem we are free to choose X(0). Koopmans showed in 1965 that mal if W^ (X(t))K(t)(e-)δt \to 0 as t X(t)→. nftyin (6) is opti-. It transpires that, for the model in hand, there is a value of which we shall write as X*(0), such that the condi - X(0), tion (3) and Koopmans’s asymptotic condition are sat-isfied by the function X(t) given in (6). This implies that X*(0)e((r-)δ)t/α is the unique optimum. Consump- tion grows at the percentage rate(r - δ)/α and dynas- tic wealth accumulates continually in order to make that rising consumption level possible.
All other things being equal, the larger the productivity of investment the higher the optimum rate of growth of consumption.r , By contrast, the larger the value ofof growth of consumption, since there is a greater wishα, the lower the rate to spread it out among the generations. Let us conduct a simple exercise with our finding. Suppose the annual market rate of return is 4% (i.e.,$r = 0$.04 per year)—a reasonable figure for the United States—thatδ is small, and that α = 2. Then we can conclude from (6) that optimum consumption will growat an annual rate of 2%;
meaning that it will double every thirty-five years—roughly, every generation. The Ramsey insisted that show that an optimum function4. This problem originated in a classic paper by Ramsey (1928).δ = 0 and devised an ingenious argument to X(t) exists despite the fact that the integral in (4) does not converge. For simplicity, I am assuming As W (X) is bounded above and r > 0 (meaning that it is feasible forδ > 0.X(t)allowed to rise fast enough.to grow indefinitely), we should expect (4) to converge if X(t) is

VII. The Influence of Mathematics

figure is close to the postwar growth experience in the United States. ent, since they are heavily constrained in their ability to transfer consumption across time. For example, they For Desta’s parents the calculations are very differ have no access to capital markets from which they can earn a positive return. Admittedly, they invest in their land (clearing weeds, leaving portions fallow, and soforth), but that is to prevent the productivity of the land from declining. More over, the only way they are ableto draw on the maize crop following each harvest is to store it.
Let us see how Desta’s household would ideally wish to consume that harvest over the annual cycle. ries. As rats and moisture are a potent combination, Let K(0) be the harvest, measured, say, in kilocalo- stocks depreciate. Ifsumption andγ the rate of depreciation of the maize X(t) is the planned rate of con- stock, then the stock attsatisfies the equation d K(t)/dt = −X(t) - γK(t). (7) Here, and K(t)γ is assumed to be positive and both nonnegative.
Let us imagine that Desta’s X(t) parents regard their household’s well-being over the1 year to belet-XW^  (X)/W0 W (X(t))^ (X) be equal to a number dt. As with Becky’s household,α > 1. Desta’s parents’ optimization exercise is to maximize1 W (X(t)) dt, subject to (7) and the condition that K(0 This is a straightforward problem in the calculus of1) ⩾ 0. variations. It can be shown that the optimum maize consumption declines over time at the rateγ/α. This explains why Desta’s family consume less and become physically weaker as the next harvest grows nearer.
But Desta’s parents have realized that the human body is amore productive bank. So the family consumes a good deal of maize during the months following each har-vest so as to accumulate body mass, but they draw on that reserve during the weeks before the next harvest, when maize reserves have been depleted. Across the years maize consumption assumes a sawtooth pattern.(Readers may wish to construct the model that incorporates the body as a store of energy:
see Dasgupta (1993)for details.) As Desta and her siblings contribute to daily household production, they are economically valuable assets. Her male siblings, however, offer a higher return to their parents, because the custom (itself a social equi - lib rium!) is for girls to leave home on marriage and for boys to inherit the family property and offer secu-rity to their parents in old age. Because of an absence VII.8. Mathematics and Economic Reasoning of capital markets and state pensions, male children are an essential form of investment.
The transfer of resources in Desta’s household, in contrast to Becky’s, will be from the children to their parents. relatively recently, in excess of 300 per 1000 births. So, parents had to aim at large families if they were The under-five mortality rate in Ethiopia was, until to have a reasonable chance of being looked after by amale child in their old age. But fertility is not entirely a private matter, since people are influenced by the choices of others.
This gives rise to a certain inertia in household behavior even under changing circum - st ances, which is why even though the under-five mortality rate has fallen in Ethiopia in recent decades, Desta has five siblings.5 High population growth has placed additional pressure on the local ecosystem, meaning that the local commons that used to be managed in a sustainable manner no longer are. That they arenot is reflected in Desta’s mother’s complaint that the daily time and effort required to collect from the local commons has increased in recent years.
10 Differences in Economic Life among Similar People In this article, I have used Becky’s and Desta’s experi-ences to show how it can be that the lives of essentially very similar people can become so different (for further elaboration, see Dasgupta (2004)). Desta’s life is one of poverty.
In her world people do not enjoy food security, do not own many assets, are stunted and wasted, do not live long (life expectancy at birth in Ethiopia is under fifty years), cannot read or write, are not empowered, cannot insure themselves well against crop failure orhousehold calamity, do not have control over their own lives, and live in unhealthy surroundings. The depriva-tions reinforce one another, so that the productivity of labor effort, ideas, physical capital, and of land and nat-ural resources are all very low and remain low.
The rate of return on investment is zero, perhaps even negative(as it is with the storage of maize). Desta’s life is filled with problems each day. explain fertility behavior. In the notation of the section on social equi-libria, we are to suppose that household5. See Dasgupta (1993) for the use of interdependent preferences to$h$’s well-being has the form Wbir ths in the household, and that the higher the fertility rate is amongh(Xh$, X^{-}h)$, where one of the components of Xh is the number of other households in the village, the larger the desired number of chil-dren inh.
The theory based on interdependent preferences interprets transitions from high to low fertility rates as bifurcations. Fertility rates are expected to decline even in Ethiopia. Interdependent preferences are currently being much studied by economists (see Durlauf and Young 2001).

909

ple, life expectancy at birth in the United States is Becky suffers from no such deprivation (for exam nearly eighty years). She faces what her society calls challenges. In her world, the productivity of labor effort, ideas, physical capital, and of land and natu-ral resources are all very high and continually increasing; success in meeting each challenge reinforces the prospects of success in meeting further challenges.
differences between Becky’s and Desta’s lives, there isa unified way to view them, and that mathematics is an We have seen, however, that, despite the enormous essential language for analyzing them. It is tempting topronounce that life’s essentials cannot be reduced to mere mathematics; but in fact mathematics is essen-tial to economic reasoning. It is essential because in economics we deal with quantifiable objects of vital interest to people. Acknowledgments.ceived much guidance from my colleague Pramila Krishnan. In describing Desta’s life, I have re Further Reading Dasgupta, P. 1993.tion.
Oxford: Clarendon Press. An Inquiry into Well-Being and Destitu World Bank Conference on Development Economics 2003: Accelerating Development. 2004. World poverty: causes and pathways. In, edited by F. Bourguignon and Annual B. Pleskovic, pp. 159–96. New York: World Bank and Oxford University Press. Debreu, G. 1959.Diamond, J. 1997.Theory of Value Guns, Germs and Steel: A Short History. New York: John Wiley. of Everybody for the Last 13,000 Years Windus. . London: Chatto & Durlauf, S. N., and H. Peyton Young, eds. 2001.Dynamics. Cambridge, MA: MIT Press. Social Evans, G., and S. Honkapohja.
2001.tions in Macroeconomics. Princeton, NJ: Princeton Univer-Learning and Expect a Fogel, R. W. 2004.sity Press. Death, 1700–2100: Europe, America, and the Third World The Escape from Hunger and Premature. Fudenberg, D., and E. Maskin. 1986. The folk theorem Cambridge: Cambridge University Press.in repeated games with discounting or with incomplete Landes, D. 1998.information. York: W. W. Norton. Econometrica The Wealth and Poverty of Nations54(3):533–54. . New Ramsey, F. P. 1928. A mathematical theory of saving.nomic Journal 38:543–49. Eco Samuelson, P. A. 1947.Cambridge, MA:
Harvard University Press. Foundations of Economic Analysis. out the social contrivance of money. Economy. 1958. An exact consumption loan model with or with - 66:1002–11. Journal of Political 910 VII.9 The Mathematics of Money Mark Joshi 1 Introduction The last twenty years have seen an explosive growth in the use of mathematics in finance. Mathematics has made its way into finance mainly via the application oftwo principles from economics: market efficiency and no arbitrage Market efficiency is the idea that the financial mar-. kets price every asset correctly.
There is no sense in which a share can be a “good buy,” because the market has already taken all available information into account. Instead, the only way that we have of distinguishing between two assets is their differing risk characteristics offer a high rate of growth but also a high probability. For example, a technology share might of losing a lot of money, while a U.K. or U.S. govern-ment bond would offer a much smaller rate of growth, but an extremely low probability of losing money.
Infact, the probability of loss is so small in the latter case that these instruments are generally regarded as being risk less. ply says that it is impossible to make money with out taking risk. It is some times called the “no free lunch”No arbitrage, the second fundamental principle, sim principle. In this context, “making money” is defined to mean making by investing in a risk less government bond.
A simple more money than could be obtained application of the principle of no arbitrage is that if one changes dollars into yen and then the yen into euros and then the euros back into dollars, then, apart from any transaction costs, one will finish with the same number of dollars that one started with. This forces a simple relationship between the three foreign exchange(FX) rates: FX,€ = FX^$\\\{$,\. \1(X\\\{,\\\})€. (1) Of course, occasional anomalies and exceptions to this relationship can occur, but these will be spotted by traders.
The exploitation of the resulting arbitrage opportunity will quickly move the exchange rates until the opportunity disappears. finance into four main areas. One can roughly divide the use of mathematics in Derivatives pricing. This is the use of mathematics to price value depends purely upon the behavior of another securities (i.e., financial instruments), whose

VII. The Influence of Mathematics

asset. The simplest example of such a security is acall option, which is the right, but not the obligation, to buy a share for a pre-agreed price, K, on some specified future date. The pre-agreed price is called the strike. The pricing of derivatives is heavily reliant Risk analysis and reduction.upon the principle of no arbitrage.has holdings and borrowings of assets; it needs to Any financial institution keep careful control of how much money it can lose from adverse market moves and to reduce these risks as necessary to keep within the owners’ desired risk profiles.
Portfolio optimization.will have notions of how much risk he wants to take Any investor in the markets and how much return he wants to generate, and most importantly of where he sees the trade-off between the two. There is, therefore, a theory of how to invest in shares in such a way as to maximize the return ata given level of risk. This theory relies greatly on the Statistical arbitrage.principle of market efficiency. Crudely put, this is using mathematics to predict price movements in the stock mar-ket, or indeed in any other market.
Statistical arbitrageurs laugh at the concept of market efficiency, and their objective is to exploit the inefficiencies in the market to make money. seen the greatest growth in recent years, and which has seen the most powerful application of advanced Of these four areas, it is derivatives pricing that has mathematics. 2 Derivatives Pricing

2.1 Black and Scholes

Many of the foundations of mathematical finance were laid down by Bachelier (1900) in his thesis; his mathematical study ofthat of Einstein (see Einstein (1985), which contains his brownian motion [IV.24](/part-04/stochastic-processes) preceded 1905 paper). However, his work was neglected for many years and the great breakthrough in derivatives pricing was made by Black and Scholes (1973). They showed that, under certain reasonable assumptions, it was pos-sible to use the principle of no arbitrage to guarantee a unique price for a call option.
The pricing of deriva-tives had ceased to be an economics problem and had become a mathematics problem. The result of Black and Scholes was deduced by extending the principle of no arbitrage to encompass the idea that an arbitrage could result not just from static

VII.9. The Mathematics of Money

holdings of securities, but also from continuously trad-ing them in a dynamic fashion depending upon their price movements. It is this principle of noarbitrage that underpins derivatives pricing. dynamic to use the language of probability theory. In order to properly formulate the principle, we have assets, the An arbitrage portfolio is a trading strategy in a collection of, such that (ii) the probability that the portfolio will have a nega-(i) initially the portfolio has a value of zero; tive value in the future is zero;
(iii) the probability that the portfolio will have a posi-tive value in the future is greater than zero. Note that we do not require the profit to be certain; we merely require that it is possible that money maybe made with no risk taken. (Recall that the notion of making money is by comparison with a government bond. The same is true of the “value” of a portfolio: itwill be considered positive in the future if its price has increased by more than that of a government bond.) but often with a general upward or downward ten-The prices of shares appear to fluctuate randomly, dency.
It is natural to model them by means of a Brown-ian motion with an extra “drift term.” This is what Black and Scholes did, except that it was the share price S = S that was assumed to follow a Brown-logarithm of thet

ian motion to make, because changes in prices behave multiplica-Wt with a drift. This is a natural assumption tively rather than additively. (For example, we measure inflation in terms of percentage increases.) They also assumed the existence of a risk less bond, B , grow-t

ing at a constant rate. To put these assumptions more formally:. og S = . og S0 + \mu t + σ Wt, (2)Bt = B0 er t. (3) Notice that the expectation of log S is log S0 + \mu t, so it changes at a rateσ is known as the\mu, which is called the volatility. The higher the volatility, drift. The term the greater the influence of the Brownian motion W , t and the more unpredictable the movements ofinvestor will want a large\mu and a small σ; however, S.
(An market efficiency ensures that such shares are rather rare.) Under additional assumptions such as that there are no transaction costs, that trading in a share does not affect its price, and that it is possible to trade continuously, Black and Scholes showed that if there is no dynamic arbitrage, then at timet, the price of a call 911 Black–Scholes price 10 60 68 76 84 92 Share price100 108 116 124 132 140 Figure 1100 for various maturities. The value decreases as maturity The Black–Scholes price of a call option struck at decreases, with the bottom line denoting a maturity of zero.
option, C(S, t), that expires at time T must be equal to$BS$(S$, t, r , σ , T ) = SΦ(d^{1}) - Ke^{-r} (T^{-t})Φ(d^{2})$, (4) with $d1 = \log (S/K) +σ(r\sqrt{T}+-σt2/2)(T - t) (5)$ and $d2 = \log (S/K) +σ(r\sqrt{T}--σt2/2)(T - t)$. (6) Here, mal random variable has value less thanΦ(x) denotes the probability that a standard nor-x. As x tends$to$. nfty , Φ(x) tends to 1, and as x tends to −$\infty$, Φ(x) tends to 0. If we letto. nfty  if S > K t(in which case log tend to T, we find that(S /K) >d10) and toand d2 tend−. nfty if. ax ST(S< KT-.
It follows that the price K, 0), which is the value of a call option at^TC(S, t) converges to expiry, just as one would expect. We illustrate this in figure 1. Tre sult that go far beyond the formula itself. The first and most important result is that the price is unique. There are a number of interesting aspects to this Using just the hypothesis that it is impossible to make a risk less profit, along with some natural and innocuous assumptions, we discover that there is only one possi-ble price for the option. This is a very strong conclusion.
It is not just the case that the option is a bad dealif traded at a different price: if a call option is bought for less or sold for more than the Black–Scholes price, then a risk less profit can be made. A second fact, which may seem rather paradoxical, is that Scholes formula. This means that the expected behav-\mu, the drift, does not appear any where in the Blackior of the share’s future mean price does not affect the 912 price of the call option; our beliefs about the probabil-ity that the option will be used do not affect its price.
Instead, it is the volatility of the share price that isall - important. As part of their proof, Black and Scholes showed that the call option price satisfied a certain partial differ-ential equation (PDE) now known as the Black–Scholes equation, or BS equation for short: . artial C. artial t + r S . artial C. artial S +1 2 σ2 S2 . artial. artial S2 C2 = r C. (7) This part of the proof did not rely on the derivative being a call option: there is in fact a large class of derivatives whose prices satisfy the BS equation, differing only in boundary conditions.
If one changes vari - ables, settingτ = T - t and X = . og S, then the BS equa- tion becomes extra first-order term which can easily be removed. This the heat equation [I.3 §5.4](/part - 01/fundamental - definitions) with an means that the value of an option behaves in a similar way to time-reversed heat: it diffuses and spreads out the farther back one gets from the option’s expiry andthe more uncertainty there is about the value of the share at time T . 2.2 Replication The fundamental idea underlying the Black–scholes proof and much of modern derivatives pricing is dynamic replication.
Suppose we have a derivative Y that pays an amount that depends on the value of the share at some set of timest < t < · · · < t , and suppose that the payout occurs at a certain time1 2 Tn ⩾ tn. This can be expressed in terms of a The value of Y will vary with the share price. If, in payoff function,$f (t1, . . . , tn)$. addition, we hold just the right number of the shares themselves, then a portfolio consisting of Y and the shares will be instantaneously immune to changes in the share price, i.e., its value will have zero rate of change with respect to the share price.
As the value of Y will vary with time and share price, we will need to con-tinuous ly buy and sell shares to maintain this neutrality to share-price movements. If we have sold a call option, then it turns out that we will have to buy when the share price goes up and sell when it goes down; so these transactions will cost us a certain amount of money. money was always the same and that it could be com - puted.
The sum of money is such that by investing it Black and Scholes’s proof showed that this sum of in shares and risk less bonds, one can end up with a portfolio precisely equal in value to the payoff of Y no matter what the share price did in between. VII. The Influence of Mathematics money, one would simply carry out the trading strategy from their proof and always end up ahead. Similarly, if Thus if one could sell Y for more than this sum of one can buy Y for less, one does the negative of the strategy and always ends up ahead.
Both of these are outlawed by the principle of no arbitrage, and a unique price is guaranteed. replicated is called The property that the payoff of any derivative can be market completeness. 2.3 Risk-Neutral Pricing A curious aspect of the Black–Scholes result, mentioned above, is that the price of a derivative does not depend upon the drift of the share price. This leads to an alternative approach to derivatives pricing theory called risk-neutral pricing. An arbitrage can be thought of as the ultimate unfair game: the player can only make money.
By contrast, a martingale [IV.24 §4](/part - 04/stochastic - processes) encapsulates the notion of a fair game: it is a random process whose expected future value is always equal to its current value. Clearly, an arbitrage portfolio can never bea martingale. So if we can arrange for everything to be a martingale, there can be no arbitrages, and the price of derivatives must be free of arbitrage. Unfortunately, this cannot be done because the price of the risk less bond grows at a constant rate, and istherefore certainly not a martingale.
However, we can carry out the idea for prices of assets when they are divided by the price of discounted prices: that is, for the risk less bond. In the real world, we do not expect discounted prices to be martingales. After all, why buy shares if their mean return is no better than that of a bond that carries no risk? Nevertheless, there is an ingenious way of introducing martingales into the analysis: by changing the probability measure [III.71 §2](/part - 03/probability - distributions) that one uses.
will see that it depends only on which events have zero probability and which have nonzero probability. Thus, If you look back at the definition of arbitrage, you it uses the probability measure in a rather incomplete way. In particular, if we use a different probability measure for which the sets of measure zero are the same, then the set of arbitrage portfolios will not change. Two measures with the same sets of measure zero are saidto be equivalent.
drift of a Brownian motion, then the measure that you derive from it will be equivalent to the measure you A theorem of Girsanov says that if you change the had before. This means that we can change the term A good value to choose turns out to be\mu = r -1σ2.\mu.

VII.9. The Mathematics of Money

With this value of\mu, one has E(S/Bt) = S/B0 (8) for anying point, it follows thatt, and since we can take any time as our start - S/B is a martingale. (The extrat - 1 2 σ2 in the drift comes from the concavity of the coor- dinate change to log - space.) This means that the expec-tation has been taken in such a way that shares do not carry any greater return, on average, than bonds. Normally, as we have mentioned, one would expect an investor to demand a greater return from a risky share than from a bond.
(An investor who does not demand such compensation is said to benow that we are measuring expectations differently, werisk neutral.) However, have managed to build an equivalent model in which this is no longer the case. This yields a way of finding arbitrage-free prices. First, pick a measure in which the discounted price pro-cesses of all the fundamental instruments, e.g., shares and bonds, are martingales. Second, set the discounted price process of derivatives to be the expectations of their payoff; this makes them into martingales by construction. arbitrage.
Of course, this merely shows that the price is non arbitrage able, rather than that it is the Everything is now a martingale and there can be noonly non arbitrage able price. However, work by Harrison and Kreps (1979) and by Harrison and Pliska (1981) shows that if a system of prices is non arbitrage able, then there must be an equivalent martingale measure. Thus the pricing problem is reduced to classifying the set of equivalent martingale measures. Market completeness corresponds to the pricing measure being unique.
technique that it is now typical to start a pricing prob - Risk-neutral evaluation has become such a pervasive lem by postulating risk-neutral dynamics for assets rather than real-world ones. Scholes replication approach, and the risk-neutral ex-pectation approach. In both cases, the real-world drift, We now have two techniques for pricing: the Black\mu, of the share price does not matter.
Not surpris - ingly, a theorem from pure mathematics, the Feynman–Kac theorem, joins the two approaches together by stating that certain second-order linear partial differ-ential equations can be solved by taking expectations of diffusive processes. 2.4 Beyond Black–Scholes For a number of reasons, the theory outlined above isnot the end of the story. There is considerable evidence 913 that the log of the share price does not follow a Brown-ian motion with drift. In particular, market crashes occur.
For example, in October 1987 the stock market fell by 30% in one day and financial institutions found that their replication strategies failed badly. Mathematically, a crash corresponds to a jump in the share price, and Brownian motion has the property that all paths are continuous. Thus the Black–Scholes model failed to capture an important feature of share-price evolution.
share but with differing strike prices often trade with A reflection of this failure is that options on the same different volatilities, despite the fact that the BS model suggests that all options should trade with the same volatility. The graph of volatility as a function of the strike price is normally in the shape of a smile, dis-playing the disbelief of traders in the Black–Scholes model. that the volatility is constant.
In practice, market activ-Another deficiency of the model is that it assumes ity varies in intensity and goes through some periods when share prices are much more volatile and others when they are much less so. Models must therefore be corrected to take account of the stochasticity of volatil - ity, and the prediction of volatility over the life of an option is an important part of its pricing. Such models are called stochastic volatility models. ments, one quickly discovers that they do not resem-ble a diffusion.
They appear to be more like a series If one examines the data on small-scale share moveof small jumps than a Brownian motion. However, if one rescales time so that it is based on the number of trades that have occurred rather than on calendar time, then the returns do become approximately normal. One way to generalize the Black–Scholes model isto introduce a second process that expresses trading time. An example of such a model is known as the variance gamma model processes has been applied to develop wider theories.
More generally, the theory of Lévy of price movements for shares and other assets. not retain the property of market completeness. They Most generalizations of the Black–Scholes model do therefore give rise to many prices for options rather than just one. 2.5 Exotic Options Many derivatives have quite complicated rules to deter-mine their payoffs.
For example, a barrier option can be exercised only if the share price does not go below a certain level at any time during the contract’s life, and an 914 Asian option of the share price over certain dates rather than on the pays a sum that depends on the average price at expiry. Or the derivative might depend uponseveral assets at once, such as, for example, the right to buy or sell a basket of shares for a certain price. It is easy to write down expressions for the value ofsuch derivatives in the Black–Scholes model, either via a PDE or as a risk-neutral expectation.
It is not so easy to evaluate these expressions. Much research is therefore devoted to developing efficient methods of pricing such options. In certain cases it is possible to develop analytic expressions. However, these tend to be the excep-tion rather than the rule, and this means that one must resort to numerical techniques. There is a wealth of methods for solving PDEs and these can be applied to derivatives-pricing problems. One difficulty in mathematical finance, however, is that the PDE can be very high dimensional.
For example, if one is trying to evaluate a credit product dependingon 100 assets, the PDE could be 100 dimensional. PDE methods are most effective for low-dimensional problems, and so research is devoted to trying to make them effective in a wider range of cases. Monte Carlo evaluation. The basis of this method is very One method that is less affected by dimensionality is simple: both intuitively and (via the law of large num - bers) mathematically, an expectation is the long-run average of a series of independent samples of a random variable for estimating X.
This immediately yields a numerical method E(f (X)). One simply takes many inde- pendent samples and computes their average. It follows from the Xi of X , calculates f (Xi) for each onecen- tral limit theorem draws is approximately distributed as a normal distri-[III.71 §5](/part-03/probability-distributions) that the error after$N$ bution with variance equal to(N-1()/){2} times the variance ofsion independent. If the variance off (X). The rate of convergence is therefore dimen-f (X) is large, it may still be rather slow, however.
Much effort is therefore devoted by financial mathematicians to developing methods of reducing the variance when one computes high-dimensional integrals.

2.6 Vanilla versus Exotics

Generally, a simple option to buy or sell an asset is known as aderivative is known as an vanilla option, where as a more complicated exotic option. An essential difference between the pricing of the two is that one can hedge an exotic option not just with the underlying share, but also by trading appropriately in the vanilla

VII. The Influence of Mathematics

options on that share. Typically, the price of a deriva-tive will depend not just on observable inputs, such as the share price and interest rates, but also on unob-servable parameters, such as the volatility of the share price or the frequency of market crashes, which can not be measured but only estimated. dependence upon these unobservable inputs.
A stan-dard way to do this is to trade vanilla options in such When trading exotic options, one wishes to reduce a way as to make the rate of change of the value of the portfolio with respect to such parameters equal to zero. A small misestimation of their value will then have little effect on the worth of the portfolio. This means that when one prices exotic options, one wishes not just to capture the dynamics of the underlying asset accurately but also to price all the vanilla options on that asset correctly.
In addition, the model will predict how the prices of vanilla options change when the share price changes. We want these predictions to be accurate. ever, one can modify it so that the volatility varies with The BS model takes volatility to be constant. Howthe share price and over time. One can choose how it varies in such a way that the model matches the market prices of all vanilla options. Such models are known as local volatility models or Dupire models.
Local volatility models were very popular for a while, but have become less so because they give a poor model for how the prices of vanilla options change over time. the models we mentioned in section 2.4 comes from the desire to produce a model that is computationally Much of the impetus behind the development of tractable, prices all vanilla options correctly, and pro-duces realistic dynamics for both the underlying assets and the vanilla options. This problem has still not been wholly solved.
There tends to be a trade-off between realistic dynamics and perfect matching of the vanilla options market. One compromise is to fit the market as well as possible using a realistic model and then to superimpose a local volatility model to remove the remaining errors. 3 Risk Management

3.1 Introduction

Once we have accepted that it is impossible to make money in finance with out taking risk, it becomes important to be able to measure and quantify risks. We wishto measure accurately how much risk we are taking and decide whether we are comfortable with that level of

VII.9. The Mathematics of Money

risk. For a given level of risk, we want to maximize our expected return. When considering a new transaction, we will want to examine how it affects our risk lev-els and returns. Certain transactions may even reduce our risk while increasing our returns if they cancel out other risk. (A risk that can be canceled out by other risks that have a tendency to move in the opposite direction is called diversifiable.) when dealing with portfolios of derivatives, which are The control of risk becomes particularly important often of zero value initially but which can very quickly change value.
Placing a limit on the value of the contracts held is therefore not of much use, and controls based on deal sizes are complicated by the fact that often many derivatives contracts largely cancel each other out; it is the control. residual risk that one wishes to

3.2 Value-at-Risk

One method of limiting an institution’s risks in deriva-tives trading is to place a limit on the amount it can lose with a given probability over a specified period oftime. For instance, one might consider the losses at a 1% level over ten days, or at a 5% level over one day. This value is called Value-at-Risk or VAR. model of how the portfolio of derivatives might change in value over the time period. This requires a model To compute VAR one has to build up a probabilistic of how all the underlying assets can move.
Given this model, one then builds up the distribution of possible profits and losses over the given time period. Once one has this distribution one simply reads off the desired percentile. computation are quite different from those for deriva-tives pricing. Typically, a VAR computation is done over The issues involved in modeling the changes for VAR a very short time period, such as one or ten days, unlike the pricing of an option, which deals with a long time frame. Also, one is not interested in the typical path for VAR, but instead one focuses on the extreme moves.
In addition, since it is the VAR of an entire portfolio that matters, one has to develop an accurate model of the underlying assets’ joint distributions: the movement of one underlying asset could magnify the price movement of another, or it could act as a hedge. abilistic model for computing VAR. The first, the histor-ical approach, is to record all the daily changes over There are two main approaches to developing a prob some time period, for example two years, and then

915

assume that the set of changes tomorrow will be identi-cal to one of the sets of changes we have recorded. If we assign equal probability to each of those changes, thenwe get an approximation to the profit and loss distribution, from which we can read off the desired percentile. Note that as we are using a day’s change for all assets simultaneously, we automatically get an approximation to the joint distribution of all the asset prices. A second approach is to assume that asset price movements come from some well-known class of dis-tributions.
For example, we could assume that the logs of the asset price movements are jointly normal. We would then use historical data to estimate the volatilities and the correlations between the various prices. The main difficulty with this approach is obtaining robust estimates of the correlations given a limited amount of data. 4 Portfolio Optimization

4.1 Introduction

The job of a fund manager is to maximize the return on the money invested while minimizing the risk. If we assume that markets are efficient, then there is no point in trying to pick shares that we believe to be under val-ued as we have assumed that they do not exist. A corollary is that just as no shares are good buys, no shares are bad buys. In any case, over half the shares in the market are owned via funds and therefore under the control of fund managers. Therefore, the average fund manager cannot expect to out perform the market.
It may seem that this does not leave much for fund managers to do, but in fact it leaves two things. (i) They can attempt to control the amount of risk they are taking. (ii) For a given level of risk, they can maximize their expected return. joint distribution of asset prices over the longer term, and a quantifiable notion of risk. To do these things requires an accurate model of the

4.2 The Capital Asset Pricing Model

Portfolio theory has been in its modern form for longer than derivatives pricing. As an area, it relies less on stochastic calculus and more on economics. We briefly review the key ideas. The best-known model for modeling portfolio returns is theing model (or CAPM), which was introduced in the capital asset pric-

916

1950 s by Sharpe (see Sharpe 1964), and is still ubiqui-tous. Sharpe’s model built on earlier work of Markowitz (1952). what portfolio of assets, generally shares, an investor should hold in order to maximize returns at a given The fundamental problem in this area is to assess level of risk. The theory requires assumptions to bemade about the joint distribution of share returns, e.g., joint normality, and/or about the risk preferences ofinvestors, e.g., that they only care about the mean and variance of returns.
that every investor should hold a multiple of the “mar-ket portfolio,” which is essentially a portfolio consist-Under these assumptions, the CAPM yields the result ing of everything traded in appropriate quantities to achieve maximum diversification, together with a certain amount of the risk-free asset. The relative amounts are determined by the investor’s risk preferences. tween diversifiable risk and undiversifiable risk.
while investors are compensated for taking undiversifiable, A consequence of the model is the distinction beor systematic, risk via higher expected returns, diversifiable risk does not carry a risk premium. This is because one can cancel out diversifiable risk by holding appropriate combinations of other assets.
Therefore, ifit carried a risk premium, investors could receive extra return with out taking any risk. Much of the current research in this area is directed at trying to find more accurate models for the joint dis-tribution of returns, and at finding techniques that estimate the parameters of such returns. A related problem is the “equity premium puzzle,” which is that the excess return on investing in shares is much higher than the model predicts for reasonable levels of risk a version.
5 Statistical Arbitrage We only briefly mention statistical arbitrage as it is a rapidly changing area that is shrouded in secrecy. The fundamental idea in this area is to squeeze information out of asset price movements that the market hasnot already acted on. It therefore contradicts the principle of market efficiency, which says that all available information is already encoded in the market price. One explanation is that it is the action of taking such arbitrages that makes the market efficient.

Further Reading

Bachelier, L. 1900.Gauthier-Villars. La Théorie de la Spéculation. Paris:

VII. The Influence of Mathematics

Black, F., and M. Scholes. 1973. The valuation of options and corporate liabilities. Journal of Political Economy 81: Einstein, A. 1985.637–54.ian Movement. New York: Dover. Investigations on the Theory of the Brown Harrison, J. M., and D. M. Kreps. 1979. Martingales and arbitrage in multi-period securities markets. Journal of Harrison, J. M., and S. R. Pliska. 1981. Martingales and Economic theory stochastic integration in the theory of continuous trading.20:381–408. Markowitz, H. 1952. Portfolio selection. Stochastic Processes and Applications7:77–99. 11:215–60.Journal of Finance Sharpe, W.
1964. Capital asset prices: a theory of market equilibrium under conditions of risk. Journal of Finance 19:425–42. VII.10 Mathematical Statistics

Persi Diaconis

1 Introduction

Suppose you want to measure something: your height, or the velocity of an airplane for example. You take repeated measurements like to combine them into a final estimate. An obviousx1, x2, . . . , xn and you would way of doing this is to use the· · · + x )/n. However, modern statisticians use many sample mean (x1 + x2 + other estimators, such as the median or then trimmed mean10% of the measurements and take the average of what(where you throw away the largest and smallest is left). Mathematical statistics helps us to decide when one estimate is preferable to another.
For example, it is intuitively clear that throwing away a random half ofthe data and averaging the rest is foolish, but setting up a framework that shows this clearly turns out to bea serious enterprise. One benefit of the under taking is the discovery that the mean turns out to be inferior to nonintuitive “shrinkage estimators” even when the data are drawn from a natural as the bell-shaped curve (that is, are probability distribution normally[III.71](/part-03/probability-distributions) as distributed To get an idea of why the mean may not always give[III.71
§5](/part-03/probability-distributions)). you the most useful estimate, consider the following situation. You have a collection of a hundred coins and you would like to estimate their biases. That is, you would like to estimate a sequence of a hundred numbers, where thenth number θn is the probability that the pose that you flip each coin five times and note down nth coin will come up heads when it is flipped. Suphow many times it shows heads. What should your esti-mate be for the sequence$(θ^{1}$, . . . , θ100)? If you use the

VII.10. Mathematical Statistics

means, then your guess for times thenth coin shows heads, divided by 5. How-θn will be the number of ever, if you do this, then you are likely to get some very anomalous results. For instance, if all the coins happen to be unbiased, then the probability that any given coin shows up heads five times is 1 to guess that around three of the coins have biases of/32, so you are likely

1. So you will be guessing that if you flip those coins five hundred times then they will come up heads every single time. proposed in order to deal with this obvious problem. However, one must be careful: if a coin comes up Many alternative methods of estimation have been heads five times it could be thatto 1. What reason is there to believe that a differentθi really is equal method of estimation is not in fact taking us further from the truth? Here is a second example, drawn from work of Bradley Efron, this time concerning a situation from real life.
Table 1 shows the batting averages of eighteen baseball players. The first column shows the pro-portion of “hits” for each player in their first forty-five times at bat, and the second column shows the proportion of hits at the end of the season. Consider the task of predicting the second column given only the first column. Once again, the obvious approach is touse the average. In other words, one would simply use the first column as a predict or of the second column. The third column is obtained by a shrinkage estimator:
more precisely, it takes a number$y$in the first column and replaces it by 0$.265 + 0$.212(y - 0.265). The number 0 so the shrinkage estimator is replacing each entry in.265 is the average of the entries in the first column, the first column by one that is about five times closer to the average. (How the number 0.212 is chosen will be explained later.) If you look at the table, you will see that the shrinkage estimators in the third column are better predictors of the second column in almost every case, and certainly on average.
Indeed, the sum of squared differences between the James–Stein estimator and the truth divided by the sum of squared differences between the usual estimator and the truth is 0.29. That is a threefold improvement. ment and a clear sense in which the new estimatoris There is beautiful mathematics behind this improve-always better than the average. We describe the framework, ideas, and extensions of this example asan introduction to the mathematics of statistics. tween probability and statistics. In probability theory, Before beginning, it will be useful to distinguish be-

917

Table 1 major league players in 1970.Batting averages for eighteen number Player after 45 remainder James–Stein remaining average Batting at bats of season average Batting estimator at bats 12 0.4000.378 0.3460.298 0.2930.289 367426 34 0.3560.333 0.2760.221 0.2840.279 521276 56 0.3110.311 0.2730.270 0.2750.275 418467 78 0.2890.267 0.2630.210 0.2700.265 586138 10119 0.2440.2440.222 0.2690.2300.264 0.2610.2610.256 510200277 1213 0.2220.222 0.2560.304 0.2560.256 270434 1415 0.2220.222 0.2640.226 0.2560.256 538186 1617 0.2000.178 0.2850.319 0.2510.247 558405 18 0.156 0.200 0.242 70 one begins with
a set X (for the moment taken to be finite) and a collection of number sx ∈ X, which are positive and sum to one. This function P (x), one for each P (x) is called a probability distribution. The basic prob- lem of probability is this. You are given the probability distribution P (x) and a subset A ⊂ X, and you must compute or approximate the sum of P (x) for x in AP (A). (In probabilistic terms, each, which is defined to be x has a probability P (x) of being chosen, and P (A) is the probability that mul at i on hides wonderful mathematical problems.
Forx belongs to A.) This simple for- example, X might be the set of all sequences of pluses and minuses of length 100 (e.g.,+−−++−−−−−· · · ), and each pattern might be equally likely, in which case P (x) = 1/2100 for every sequence x. Finally, A might be the set of sequences such that for every positive integerk ⩽ 100 the number of +symbols in the firstk places is larger than the number of-symbols in the firstk places. This is a mathematical model for the following probability problem:
if you and a friend flip a fair coin a hundred times, then what is the chance that your friend is always ahead? One might expect this chance to bevery small. It turns out, however, to be about 1 , though verifying this is a far from trivial exercise. (Our poor12 intuitions about chance fluctuations have been used to 918 explain road rage: suppose you choose one of two lines at a toll booth. As you wait, you notice whether your line or the other has made more progress.
We feel it should all balance out, but the calculations above show that a fair proportion of the time you are always behind—and frustrated!) 2 The Basic Problem of Statistics Statistics is a kind of opposite of probability. In statis - tics, we are given a collection of probability distributions Pθ(x), indexed by some parameter θ. We see just one family (whichx and are required to guess which member of theθ) was used to generate x.
For example, let us keep X as the sequence of pluses and minuses of length 100, but this time let obtaining the sequencex if the probability of a plus is P^θ(x) be the chance ofθin the sequence chosen independently. Here 0 and the probability of a minus is 1 - θ, with all terms⩽ θ ⩽ 1, andthe number of times “Pθ(x) is easily seen to be+” appears in the sequenceθS (1 - θ)T, wherex Sandis T = 100 - Sis the number of times “-” appears. This is a mathematical model for the following enterprise. You have a biased coin with a probability up heads, but you do not knowθ.
You flip the coin aθ of turning hundred times, and are required to estimateθ based on the out come of the flips. In general, for eachx ∈ X, we want to find a guess, which we denote by ˆis, we want to come up with a function ˆθ(x), for the parameterθ, which willθ. That be defined on the observation space X. Such functions are called estimators. The above simple formulation hides a wealth of complexity, since both the observa-tion space X and the space Θ of possible parameters may be infinite, or even infinite dimensional.
For example, in nonparametric statistics, the set of all probability distributions onΘ is often taken as X. All of the usual problems of statistics—design of experiments, testing hypotheses, prediction, and many others—fit into this framework. We will stick with the imagery of estimation. dient is needed: you have to know what it means toget the right answer. This is formalized through the To evaluate and compare estimators, one more ingrenotion of a loss function L(θ, θ(x))ˆ . One can think of this in practical terms:
wrong guesses have financial consequences, and the loss function is a measure of how much it will cost ifθ is the true value of the parameter but the statistician’s guess is ˆwidely used choice is the squared errorθ(x)(θ. The most- θ(x))ˆ 2,

VII. The Influence of Mathematics

butants are also used. The|θ - θ(x)ˆ| or |θ - θ(x)ˆrisk function|/θ and many other vari - R(θ,θ)ˆ measures the expected loss ifestimator ˆθ is used. That is,θ is the true parameter and the R(θ, θ)ˆ= L(θ, θ(x))Pˆθ(dx).

Here, the right-hand side is notation for the average value of L(θ, θ(x))ˆ ifx is chosen randomly accord- ing to the probability distribution would like to choose estimators that will make the risk$P^{θ}$. In general, one function as small as possible. 3 Admissibility and Stein’s Paradox We now have the basic ingredients: a family$P^{θ}(x) and$ a loss functionif there is a better estimator$L$. An estimator ˆθ*θ, in the sense thatis called in admissible R(θ, θ*) < R(θ, θ)ˆ for allθ.

In other words, the expected loss with$θ^{*} \text{is less than}$ the expected loss with ˆθ, whatever the true value of θ. tion Given our assumptions (the model L) it seems silly to use an in admissible estimator. Pθ and loss func- However, one of the great achievements of mathemat-ical statistics is Charles Stein’s proof that the usual least-squares estimator, which does not at first glance seem silly at all, is in admissible in natural problems. Here is that story. Consider the basic measurement model

$X^{i} = θ + i$, 1 ⩽ i ⩽ n.

Herebe estimated, and Xi is the ith measurement, is measurement error. The classi-θ is the quantity toi

cal assumptions are that the measurement errors are independently and normally distributed: that is, they are distributed according to the bell-shaped, or Gaussian, curve ethe language we introduced earlier, the measurement$- {}^{x2/2}/\sqrt{2π}$, −$\infty$ < x < . nfty . In terms of space$X is R^{n}$, the parameter space Θ is R, and the observation sity P (x) =x. xp =[-((x1)1, (xn)2(x, . . . , x- θ)n)2]/(has probability den-. qrt{2π})n. The usual estimator is the mean: that is, ifthen one takes ˆθ θ(x)2 to b(e1)i (x + · · · +x = x(x)/n1, . . . , x.
It hasn), been known for a long time that if the loss function L(θ, θ(x))ˆ is defined to be$(θ - {}^{1} θ(x))$ˆ 2, then the mean$n$ is an admissible estimator. It has many other optimal properties as well (for example, it is the best linear unbiased estimator, and it is minimax—a property that will be defined later in this article). VII.10. Mathematical Statistics ters, vat i ons, Now suppose that we wish to estimateθ1 and X , . . . , Xθ2, say. This time we have two sets of obser - and Y , . . . , Y , with Xtwo = parame-θ + andpendent and normally distributed, as above.
The loss Yj = 1 θ2 + nηj.
The error(s1()m)i and ηi j are inde-1 i function(θ - θˆ(x))L((θ2 1+θ2(θ), (θˆ$- {}^{1}(x)θ$ˆ(y))θˆ2(y)))2: that is, you add up theis now defined to be squared errors from the two parts. Again, the mean ofthe1 X and the mean of th(e1()2)2 Y make up an admissible estimator for$i (θ^{1}$, θ2).iθ2 Consider the same setup with three parameters,, θ3. Again, Xi = θ1 + i$, Yj = θ2 + ηj$, Zk = θ3 +θδ1 k, are independent and all the error terms are normally distributed. Stein’s surprising result is that for three (or more) parameters the estimator

θˆ1(x) = (x1 + · · · + xn)/n,θˆ2$(y) = (y^{1} + · · · + y^{m})/m$,θˆ3$(z) = (z^{1} + · · · + z^{l})/l$

ister in all cases. For example, if in admissible: there are other estimators that do bet-p is the number of parameters (and$p ⩾ 3)$, then the James–Stein estimator is defined to be θˆJS$= 1 - p -θ$ˆ 2$+θ$.ˆ Here we are using the notation X+ to denote the max- imum ofof all the averages and$X$and 0;θ stands for the vectorθˆ is notation for(θ((θ1)2, . . . , θ+ · · · +p )θ2 p)1^/2.R(θ, estimator ˆThe James–Stein estimator satisfies the inequalityθˆJS$) < R(θ$,θis indeed in admissible.
The James–Steinθ)ˆ for allθ, and therefore the usual estimator shrinks the classical estimator toward zero. The amount of shrinkage is small ifθˆ 2 is large and appreciable forθˆ 2 near zero. Now the problem as we have described it is invariant under translation, so if we can improve the classical estimate by shrinking toward zero, then we must be able to improve it by shrinking toward any other point. This seems very strange at first, but one can obtain some insight into the phenomenon by considering the following in formal description ofthe estimator. It makes an a priori guess$θ^{0} at θ$.
(This guess was zero above.) If the usual estimator ˆto the guess, in the sense thatθˆ is small, then itθ is close moves ˆleaves ˆθθalone. Thus, although the estimator moves the toward the guess. If ˆθ is far from the guess, it classical estimator toward an arbitrary guess, it does so only if there are reasons to believe that the guess is agood one. With four or more parameters the data can in

919

fact be used to suggest which pointθ0 one should use as the initial guess. In the example of table 1, there are eighteen parameters, and the initial guess$θ^{0} \text{was the}$ constant vector with all its eighteen coordinates equal to the average 0 for the shrinking is equal to 1.265. The number 0- 16/ .212 that was usedθ -θ . (Note that for this choice ofof the parameters that make up$θ^{0}$, θ -θ0 is the standard deviationθ.)0 elegant blend of harmonic function theory and tricky The mathematics used to prove inadmissibility is an calculus. The proof itself has had many ramifications:
it gave rise to what is called “Stein’s method” in probability theory—this is a method for proving things like the central limit theorem for complex dependent problems. The mathematics is “robust,” since it is applicable to nonnormal error distributions, a variety of different loss functions, and estimation problems far from the measurement model. It is routinely used in problems where many param-The result has had enormous practical application. eters have to be simultaneously estimated.
examples include national laboratories’ estimates of the percentage of defectives when they are looking at many differ-ent products at once, and the simultaneous estimate of census under counts for each of the fifty states in the United States. The apparent robustness of the method is very useful for such applications: even though the James–Stein estimator was derived for the bell-shaped curve, it seems to work well, with out special assumptions, in problems where its assumptions hold only roughly. Consider the baseball players above, for example. Adaptations and variations abound.
Two popu-lar ones are called empirical Bayes estimates (now widely used in genomics) and hierarchical modeling(now widely used in the assessment of education). solved. For example, the James–Stein estimator is itself The mathematical problems are far from completely in admissible. (It can be shown that any admissible esti-mator in a normal measurement problem is an analytic function of the observations.
The James–stein estimator is, however, clearly not analytic because it involves the non differentiable functionit is known that there is little practical improvementx \to x+.) While possible, the search for an admissible estimator that is always better than the James–Stein estimator is a tantalizing research problem. ical statistics is to understand which statistical prob-Another active area of research in modern mathemat lems give rise to Stein’s paradox. For example, although

920

at the beginning of this essay we discussed some inade-quacies of the usual maximum-likelihood estimator for estimating the biases of a hundred coins, it turns out that that estimator is admissible! In fact, the maximum-likelihood estimator is admissible for any problem with finite state spaces. 4 Bayesian Statistics the ingredient to the family Bayesian approach to statistics adds one further P and loss function L. This is known as a prior probability distributionθ π(θ), which gives different weights to different values of the param-eterθ.
There are many ways of generating a prior dis- tribution: it may quantify the working scientists’ best guess atθ; it may be derived from previous studies or estimates; or it may just be a convenient way to generate estimators. Once the prior distribution has been specified, the observation$x$and Bayes’s the-π(θ) orem combine to give ahere denotedπ(θ|x). Intuitively, ifposterior distri but io nx is your observa-for θ, tion, thenπ(θ|x) measures how likely it is that θ was the parameter, given that the parameter was generated from the probability distributionπ.
The mean value$of$θ with respect to the posterior distribution π(θ|x) gives a Bayes estimator:

θˆBayes$(x) = θ π(θ|x)$.

For the squared–error loss function, all Bayes estimators are admissible, and, in the converse direction, any admissible estimator is a limit of Bayes estimators. (However, not every limit of Bayes estimators is admis-sible: indeed, the average, which we have seen to be in admissible, is a limit of Bayes rules.) The point forthe present discussion is this.
In a wide variety of practical variations of the measurement problem—things like regression analysis or the estimation of correlation matrices—it is relatively straightforward to write down sensible Bayes estimators that incorporate available prior knowledge. These estimators include close cousins of the James–Stein estimator, but they are more general, and allow it to be routinely extended to almost any statistical problem. Because of the high-dimensional integrals involved, Bayes estimates can be difficult to compute.
One ofthe great advances in this area is the use of computer simulation algorithms, called variously Monte Carlo or Gibbs samplers, to compute useful Markov chain approximations to Bayes estimators. The whole pack-age—provable superiority, easy adaptability, and ease

VII. The Influence of Mathematics

of computation—has made this Bayesian version of statistics a practical success. 5 A Bit More Theory Mathematical statistics makes good use of a wide range of mathematics: fairly esoteric analysis, logic, combi-natorics, algebraic topology, and differential geometry all play a role. Here is an application of group theory. Let us return to the basic setup of a sample space X, a family of probability distributions function L(θ, θ(x))ˆ . It is natural to consider how the$P^{θ}(x)$, and a loss estimator changes when you change the units of the problem:
from pounds to grams, or from centimeters to inches, say. Will this have a significant impact on the mathematics? One would expect not, but if we want to think about this question precisely then it is useful to consider a group G of transformations of X. For exam- ple, linear changes of units correspond to the group, which consists of transformations of the form affine x \to  ax + b. The family Pθ(x) is said to be invariant under distribution G if for each element Pθ(xg) is equal to a distri but i ong of G the transformed P^ ̄(x) for some othe. ar{r}distributionsθ in Θ.
For example, the family of normalθ- (x - θ1)2. xp$2$θ2 2, −$\infty$ < θ1 < . nfty $, 0 < θ2 < \infty$, 2πθ2 2 is invariant underx to ax + b, then after some easy manipulations youax+btr a ns formations: if you change can rewrite the resulting modified formula in the form. xp [-(x - φ )2/2φ2]/ 2πφ2 for some new parame- tersif ˆ$θ(xg)φ^{1} and= θ(x)$φ1 2. An estimator ˆ. This is a formal way of saying that2 2θ is called equivariant if you change the data from one unit to another, then the estimate transforms as it should.
For example, suppose your data are temperatures presented in centi-grade and you want an answer in Fahrenheit. If your estimator is equivariant, then it will make no difference whether you first apply the estimator and then convert the answer into Fahrenheit or first convert all the data into Fahrenheit and then apply the estimator. The multivariate normal problem underlying Stein’s paradox is invariant under a variety of groups, includ-ing thep-dimensional group of Euclidian motions (rota- tions and translations).
However, the James–Stein esti-mator is not equivariant, since, as we have already discussed, it depends on the choice of origin. This isnot necessarily bad, but it is certainly thought provoking. If you ask a working scientist if they want a

VII.11. Mathematics and Medical Statistics

“most accurate” estimator, they will say “of course.”If you ask if they insist on equivariance, “of course” will follow as well. One way of expressing Stein’s para-dox is the statement that the two desiderata—accuracy and invariance—are places where mathematics and statistics part company.in compatible. This is one of many Deciding whether mathematically optimal procedures are “sensible” is important and hard to mathematize. θover allˆ Here is a second use of group theory. An estimatoris calledθ. Minimax corresponds to playing things safe:
minimax if it minimizes the maximum risk you have optimal behavior (that is, the least possible risk) in the worst case. Finding minimax estimators in natural problems is hard, honest work. For example, the vector of means is a minimax estimator in normal location problems. The work is easier if the problem is invariant under a group. Then one can first search for best invariant estimators. Invariance often reduces things to a straightforward calculus problem. Now the question arises of whether an estimator that is minimax among invariant estimators is minimax among all estimators.
A celebrated theorem of Hurt and Stein says “yes” if the group involved is nice (e.g., Abelian or com-pact or amenable). Determining whether the best invariant estimator is minimax when the group is not nice isa challenging open problem in mathematical statistics. And it is not just a mathematical curiosity. For exam-ple, the following problem is very natural, and invariant under the group of invertible matrices: given a sample from the multivariate normal distribution, estimate its correlation matrix. In this case, the group is not nice and good estimates are not known.
6 Conclusion The point of this article is to show how mathematics enters and enriches statistics. To be sure, there are parts of statistics that are hard to mathematize: graph-ical displays of data are an example. Further, much of modern statistical practice is driven by the com-puter. There is no longer any need to restrict attention to tractable families of probability distributions. Com-plex and more realistic models can be used. This gives rise to the subject of statistical computing.
None the-less, every once in a while some one has to think about what the computer one innovative procedure works better than another.should do and determine whether Then, mathematics holds its own. Indeed, mathematizing modern statistical practice is a challenging, reward-ing enterprise, of which Stein’s estimator is a current

921

highlight. This endeavor gives us something to aim forand helps us to calibrate our day-to-day achievements.

Further Reading

Berger, J. O. 1985.Analysis, 2 nd edn. New York: Springer. Statistical Decision Theory and Bayesian Lehmann, E. L., and G. Casella. 2003.mation. New York: Springer. Theory of Point Esti Lehmann, E. L., and J. P. Romano. 2005.Hypotheses. New York: Springer. Testing Statistical Schervish, M. 1996. Theory of Statistics. New York: Springer. VII.11 Mathematics and Medical

Statistics

David J. Spiegelhalter

1 Introduction

There are many ways in which mathematics has been applied in medicine: for example, the use of differ en-tial equations in ph arm ac ok in et i cs and models for epidemics in populations; andof biological signals. Here we are concerned with med-fourier analysis [III.27](/part-03/the-fourier-transform) ical statistics, by which we mean collecting data about individuals and using it to draw conclusions about the development and treatment of disease. This definition may appear to be rather restrictive, but it includes all of the following:
randomized clinical trials of thera-pies, evaluating interventions such as screening programs, comparing health out comes in different populations and institutions, describing and comparing the survival of groups of individuals, and modeling the way in which a disease develops, both naturally and whenit is influenced by an intervention.
In this article we are not concerned with eases occur and how they spread, although most of the epidemiology, the study of why dis formal ideas described here can be applied to it. After a brief historical introduction, we shall summarize the varied approaches to probabilistic model-ing in medical statistics. We shall then illustrate each one in turn using data about the survival of a sample of patients with lymphoma, showing how alternative “philosophical” perspectives lead directly to different methods of analysis.
Through out, we shall give an indication of the mathematical background to what can appear to be a conceptually untidy subject. 2 A Historical Perspective One of the first uses of probability theory in the late seventeenth century was in the development of

922

“life-tables” of mortality in order to decide premiums for annuities, and Charles Babbage’s work on life-tables in 1824 helped motivate him to design his “difference engine” (although it was not until 1859 that Scheutz’s implementation of the engine finally calculated a life-table). However, statistical analysis of medical data was a matter of arithmetic rather than mathematics until the growth of the “biometric” school founded by Francis Galton and Karl Pearson at the end of the nineteenth century.
This group introduced the use of families of probability distributions tions, as well as concepts of correlation and regression[III.71](/part-03/probability-distributions) to describe pop ula in anthropology, biology, and eugenics. Meanwhile, agriculture and genetics motivated Fisher’s huge contributions in the theory of likelihood (see below) and significance testing.
Postwar statistical developments were influenced by industrial applications and a U.S.-led increase in mathematical rigor, but from around the 1970 s medical research, particularly concerning randomized trials and survival analysis, has been a major methodological driver in statistics. attempts to put statistical inference on a sound foun-dational or axiomatic basis, but no consensus could be For around thirty years after 1945 there were many reached.
This has given rise to a widespread ecu men i-cal perspective which makes use of a mix of statistical “philosophies” which we shall illustrate below. The some what uncomfortable lack of an axiomatic basis can make statistical work deeply unattractive to many mathematicians, but it provides a great stimulus to those engaged in the area. 3 Models In this context, by a model we mean a mathematical description of a probability distribution for oneor more currently uncertain quantities.
Such a quantity might, for example, be the out come of a patient who is treated with a particular drug, or the future survival time of a patient with cancer. We can iden-tify four broad approaches to modeling—these brief descriptions make use of terms that will be covered properly in later sections. (i) Aleaves unspecified the precise form for the proba-nonparametric or “model-free” approach that (ii) Ability distributions of interest.full parametric model in which a specific form is assumed for each probability distribution, which depends on a limited number of unknown parameters.

VII. The Influence of Mathematics

(iii) Athe model is parametrized, while the rest is left semi-parametric approach in which only part of unspecified. (iv) Aparametric model specified, but an additional Bayesian approach in which not only is a full “prior” distribution is provided for the parame-ters. These are not absolute distinctions: for example, some apparently “model-free” procedures may turn out to match procedures that are derived under certain para-metric assumptions. Another complicating factor is the multiplicity of possible aims of a statistical analysis.
These may include • estimating unknown parameters, such as the mean reduction in blood pressure when giving a certaindose of a certain drug to a defined population; • predicting future quantities, such as the number of•people with AIDS in a country in ten years’ time; testing a hypothesis, such as whether a particular drug improves survival for a particular class of patents, or equivalently assessing the “null •hypothesis” that it has no effect; making decisions, such as whether to provide a particular treatment in a health care system.
A common aspect of these objectives is that any conclu-sion should be accompanied by some form of assessment of the potential for an error having been made, and any estimate or prediction should have an asso-ciated expression of uncertainty. It is this concern for “second-order” properties that distinguishes a statis-tical “inference” based on probability theory from a purely algorithmic approach to producing conclusions from data.
4 The Nonparametric or“Model-Free” Approach Now let us introduce a running example that will be used to illustrate the various approaches. Matthews and Farewell (1985) report data on sixtyfour patients from Seattle’s Fred Hutchinson Cancer Research Center who had been diagnosed with advanced-stage non-Hodgkin’s lymphoma: for each patient the information comprises their follow-up time since diag-nosis, whether their follow-up ended in death, whether they presented with clinical symptoms, their stage of disease (stage IV or not), and whether a large abdominal VII.11.
Mathematics and Medical Statistics mass (greater than 10 cm) was present. Such informa-tion has many uses. For example, we may wish to look at the general distribution of survival times, or assess which factors most influence survival, or provide a new patient with an estimate of their chance of surviving, say, five years. This is, of course, too small and limited a data set to draw firm conclusions, but it allows us to illustrate the different mathematical tools that can be used. We need to introduce a few technical terms.
Patients who are still alive at the end of data collection, or have been lost to follow - up, are said to have their survival times “censored”: all we know is that they survived beyond the last time that any data was recorded about them. We also tend to call times of death “failure” times, since the forms of analysis do not just apply to death.(This term also reflects the close connection between this area and reliability theory.) “actuarial,” using the life-table techniques mentioned previously.
Survival times are grouped into intervals The original approach to such survival data was such as years, and simple estimates are made of one’schance of dying in an interval given that one was alive at the start of it. Historically, this probability was known as the “force of mortality,” but now it is usually called the describing large populations.hazard. A simple approach like this may be fine for It was not until Kaplan and Meier (1958) that this procedure was refined to take into account the exact rather than the grouped survival times:
with over thirty thousand citations, their paper is one of the most refer-enced papers in all of science. Figure 1 shows so-called Kaplan–Meier curves for the groups of patients with(n = 31) and with out (n = 33) clinical symptoms at diagnosis. These curves represent estimates of the underlying survival functionas the probability that a typical patient will survive until, whose value at a timet is thought of that time. The obvious way of producing such a curve is simply to let its value at timet be the proportion of the initial sample that is still alive.
However, this does not quite work, because of the censored patients. So instead, if a patient dies at timet, and if, just before time value of the curve is multiplied byt, there are m patients still in the sample, then the(m - 1)/m; and if a patient is censored then the value stays the same. (The tick marks on the curves show the censored survival times.) The set of patients alive just before timet is called thebe 1/m.
(We are assuming that two people do not die risk set and the hazard at t is estimated to 923 1.0 0.8 With out symptoms 0.6 0.4 With symptoms Proportion surviving 0.2 0 1 Years since diagnosis2 3 4 5 6 7 Figure 1 lymphoma patients with and with out clinical symptoms at Kaplan–Meier nonparametric survival curves for diagnosis.
at the same time, but it is easy to drop that assumption and make appropriate adjustments.) curve has any particular functional form, we do needto make the qualitative assumption that the censoring Although we do not assume that the actual survival mechanism is independent of the survival time. (For example, it is important that those who are about to die are not for some reason preferentially removed from the study.) We also need to provide error bounds on the curves: these can be based on a variance formula devel-oped by Major Greenwood in 1926.
(“Major” was his name rather than a title, one of the few characteristics he shared with Count Basie and Duke Ellington.) ical construct, and not something that one can directly observe. One can think of it as the survival experience The “true underlying survival curve” is a theoretthat would be observed in a vast population of patients, or equivalently the expected survival for a new individual drawn at random from that population. As well as estimating these curves for the two groups of patients, we may wish to test hypotheses about them.
A typical one would be that the true underlying survival curves in the two groups are precisely the same. traditionally such “null” hypotheses are denoted H , and the tradi- tional way to test them is to determine how unlikely itis that we would observe two Kaplan–Meier curves that0 are so far apart if H0 were true. One can construct a summary measure, known as aif the observed curves are very different.
For example, test statistic, that is large one possibility is to contrast the observed number of deaths in those with symptoms ( O = 20) with the num- ber one would have expected if$H^{0} \text{were true} (E = 11$.9).

924

Under the null hypothesis it turns out there is onlya 0.2% chance of observing such a high discrepancy between O and E, which casts considerable doubt on the null hypothesis in this case. When constructing intervals around estimates and testing hypotheses we require approximate probabil-ity distributions for our estimates and test statistics. From a mathematical perspective the important theory therefore concerns large-sample distributions of functions of random variables, largely developed in the early twentieth century.
Theories for optimal hypothesis testing were developed by Neyman and Pearson inthe 1930 s: the idea is to maximize the “power” of a test to detect a difference, while at the same time mak-ing sure that the probability of wrongly rejecting a null hypothesis is less than some acceptable threshold such as 5% or 1%. This approach still finds a role in the design of randomized clinical trials.
5 Full Parametric Models Clearly we do not actually believe that deaths can only occur at the previously observed survival times shown in the Kaplan–Meier curve, so it seems reasonable to investigate a fairly simple functional form for the true survival function. That is, we assume that the survival function belongs to some natural class of functions, each of which can be fully parametrized by a small number of parameters, collectively denoted byθ. It is θ that we are trying to discover (or rather estimate with areasonable degree of confidence).
If we can do so, then the model is fully specified and we can even extrapolate a certain amount beyond the observed data. We first relate the survival function and the hazard, and then illustrate how observed data can be used to estimatein a simple example.θ ability density details, this essentially corresponds to assuming that We assume that an unknown survival time has a prob-p(t|θ); with out getting into technical p(tt to|θ)t +ddttis the probability of dying in a small interval.
Then the survival function, given a particu- lar value ofwe denote it byθ, is the probability of surviving beyond S(t|θ). To calculate it, we integrate thet: probability density over all times greater thanis,$t$. That S(t|θ) =t . nfty p(x|θ) dx = 1 -0 t p(x|θ) dx. From this and[I.3 §5.5](/part-01/fundamental-definitions) it follows that the fundamental theorem of calcu-$p(t|θ) = −d S(t|θ)/dt$. The lus hazard functionh(t|θ) dt is the risk of death in the

VII. The Influence of Mathematics

small inter valt to t +dt, conditional on having survived to time find thatt. Using the laws of elementary probability weh(t|θ) = p(t|θ)/S(t|θ). vival function with mean survival time probability of surviving beyond time For example, suppose we assume an exponential sur - t is S(tθ, so that the|θ) = e - t/θ. The density isp(t|θ) = e - t/θ/θ. Therefore, the haz- ard function is a constant$h(t|θ) = 1/θ$, so that 1/θ represents the mortality rate per unit of time.
For instance, were the mean postdiagnosis survival to be θ = 1000 days, an exponential model would imply a constant 1/1000 risk of dying each day, regard less of how long the patient had already survived after diagnosis. More complex parametric survival functions allow hazard functions that increase, decrease, or have other shapes. cept oftion When it comes to estimatingp(tlikelihood|θ) but considers it as a function of. This takes the probability dis tr ibu-θwe need Fisher’s con-θ rather than plausible values oft, and hence for observedθthat “support” the data.
The rought allows us to examine idea is that we multiply together the probabilities (or probability densities) of the observed events, assuming the value ofsored failure times make different contributions to thisθ. In survival analysis, observed and cen - product: an observed timea censored time contribute st S(tc on tr i but es|θ). If, for example, wep(t|θ), while assume that the survival function is exponential, thenan observed failure time contributes$p(t|θ) = e^{-t}/θ /θ$, and a censored time contributes$S(t|θ) = e^{-t}/θ$.
Thus, in this case the likelihood is L(θ) =i \in Obs θ-1 e -t i /θi \in Cens (e-t()i)/θ = θ-n O e-T /θ. Here “Obs” and “Cens” indicate the sets of observed and censored failure times. We denote their sizes by$n$ and time$n^{C}$, respectively, and we denote the total follow-upt by T . For the group of thirty-one patients O presenting with symptoms we have68.3 years: figure 2 shows both the likelihood and its$i^{i} n^{O} = 20 and T =$ logarithm

$LL(θ) = −T /θ - n^{O} \log θ$.

unlabeled since only relative likelihood is important. A We note that the vertical axis for the likelihood is maximum-likelihood estimate (MLE) ˆθfinds parameter values that maximize this likelihood or equiva-lently the log-likelihood. Taking derivatives of LL(θ) and equating to 0 reveals that ˆwhich is the total follow-up time divided by the number$θ = T /n Obs = 3$.4 years,

VII.11. Mathematics and Medical Statistics

(a) (b)0−1

Likelihood−2

Log-likelihood

−3

2 Mean survival3 4 5 6 2 Mean survival3 4 5 6

Figure 2 timeθ for lymphoma patients presenting with clinical Likelihood and log-likelihood for mean survival symptoms. of failures. Intervals around MLEs may be derived bydirectly examining the likelihood function, or by making a quadratic approximation around the maximum of the log-likelihood. loosely, we have carried out a form of curve fitting by selecting the exponential curves that maximize the Figure 3 shows the fitted exponential survival curves: probability of the observed data.
Visual inspection sug-gests the fit may be improved by investigating a more flexible family of curves such as the Weibull dis tr ibu-tion (a distribution widely used in reliability theory): to compare how well two models fit the data, one can compare their maximized likelihoods. dation for most current work in medical statistics, Fisher’s concept of likelihood has been the foun and indeed statistics in general.
From a mathemat i-cal perspective there has been extensive development relating the large-sample distributions of MLEs to the second derivative of the log-likelihood around its max-imum, which forms the basis for most of the outputs of statistical packages. Unfortunately, it is not neces-sarily straightforward to scale up the theory to deal with multi dimensional parameters. First, as likelihoods become more complex and contain increasing numbers of parameters, the technical problems of maximization increase.
Second, the recurring difficulty with likelihood theory remains that of “nuisance parameters,” in which a part of the model is of no particular interest and yet needs to be accounted for. No generic theory has been developed, and instead there is a some-

925

1.0

0.8 With out symptoms

0.6

0.4 With symptoms

Proportion surviving 0.2

0 1 Years since diagnosis2 3 4 5 6 7

Figure 3 curves for lymphoma patients. Fitted exponential survival what bewildering variety of adaptations of standard likelihood to specific circumstances, such as conditional likelihood, quasi-likelihood, pseudo-likelihood, extended likelihood, hierarchical likelihood, marginal likelihood, profile likelihood, and so on. Below weconsider one extremely popular development, that of partial likelihood and the Cox model.
6 A Semi-Parametric Approach Clinical trials in cancer therapy were a major moti vat-ing force in developing survival analysis—in particular, trials to assess the influence of a treatment on survival while taking account of other possible risk factors. Inour simple lymphoma data set we have three risk factors, but in more realistic examples there will be many more.
Fortunately, Cox (1972) showed that it was possible both to test hypotheses and to estimate the influ-ence of possible risk factors, with out having to go the whole way and specify the full survival function on the basis of possibly limited data. hazard function of the form The Cox regression model is based on assuming a

h(t|θ) = h0(t)(eβ)·x.

Hereh0(t) is a baseline hazard function and β is typ- ically a column vector of regression coefficients that measure the influence of a vector of risk factorsx on the hazard. (The expressionβ · x denotes the scalar product ofresponds to the hazard function of an individual whoseβ and x.) The baseline hazard function cor- risk factor vector is$x = 0$, since then (eβ)·x = 1. More generally, we see that an increase of one unit in a factor

926

x will multiply the hazard by a factor eβj, for whichj

reason this is known as the “proportional hazards”regression model. It is possible to specify a parametric form for si ble to estimate the terms ofh0(t), but remarkably it turns out to be pos-β with out specifying the form of the tion immediately before a particular failure time. Again$h^{0}$, if we are willing to consider the situa- we construct a risk set, and the chance of a particular patient failing, given the knowledge that some one in the risk set fails, provides a term in a likelihood.
Thisis known as a “partial” likelihood since it ignores any possible information in the times between failures. When we fit this model to the lymphoma data we find that our estimate ofis 1.2: easier to interpret is its exponent eβ for the patients with symptoms^1^.^2 = 3.3, which is the proportional increase in hazard associated with presenting with symptoms.
We can estimate error bounds of 1.5–7.3 around this estimate, so we can be confident that the risk of a patient who presents with symptoms will die at any stage following diagnosis is substantially higher than that of a patient who does not present with symptoms, all other factors in the model being kept constant. A huge literature has arisen from this model, dealing with errors around estimates, different censoring patterns, tied failure times, estimating the baseline survival, and so on.
Large-sample properties were rigorously established only after the method came into routine use, and have made extensive use of the theory of stochastic counting processes: see, for example, Andersen et al. (1992). These powerful mathematical tools have enabled the theory to be expanded to deal with the general analysis of sequences of events, while allowing for censoring and multiple risk factors that may depend on time. Cox’s 1972 paper has over twenty thousand citations, and its importance to medicine is reflected in his having been awarded the 1990 Kettering Prize and Gold Medal for Cancer
Research. 7 Bayesian Analysis Bayes’s theorem It states that, for two random quantities is a basic result in probability theory. t and θ$, p(θ|t) = p(t|θ)p(θ)/p(t)$.

In itself this is a very simple fact, but when parameters in a model, the use of this theorem rep-θ represents resents a different philosophy of statistical modeling. The major step in using Bayes’s theorem for inference is in considering parameters as random variables

VII. The Influence of Mathematics

[III.71 §4](/part-03/probability-distributions) with probability distributions and therefore making probabilistic statements about them. For example, in the Bayesian framework one could express one’s uncertainty about a survival curve by saying that onehad assessed that the probability that the mean survival time was greater than three years was 0 make such an assessment, one can combine a “prior”.90.
To distribution tive plausibility of different values ofp(θ) (a distribution representing the rela-θ before you look at the data) with a likelihoodp(t|θ) (how likely you were to observe the data then use Bayes’s theorem to provide a “posterior” dis-t with that value of θ) and tributionp(θ|t) (a distribution representing the rela- tive plausibility of different values ofθ after you look at the data).Put in this way Bayesian analysis appears to be a simple application of probability theory, and for any given choice of prior distribution that is exactly what it is.
But how do you choose the prior distribution?You could use evidence external to the current study, or even your own personal judgment. There is also anextensive literature on attempts to produce a toolkit of “objective” priors to use in different situations. Inpractice you need to specify the prior distribution in a way that is convincing to others, and this is where the subtlety arises.
of lymphoma had suggested that mean survival times As a simple example, suppose that previous studies of patients presenting with clinical symptoms probably lie between three and six years, with values of around four years being most plausible. Then it seems reason-able not to ignore such evidence when drawing conclusions for future patients, but rather to combine it with the evidence from the thirty-one patients in the current study. We could represent this external evidenceby a prior distribution forθwith the form given in figure 4.
When combined with the likelihood (taken from figure 2(a)), this gives rise to the posterior distribution shown. For this calculation, the functional form of the prior is assumed to be that of the distribution, which happens to make the mathemat-inverse-Gamma ics of dealing with exponential likelihoods particularly straightforward, but such simplifications are not necessary if one is using simulation methods for deriving posterior distributions. It can be seen from figure 4 that the external evidence has increased the plausibility of higher survival times. By integrating the posterior
distribution above three years, we find that the posterior probability that the mean survival is greater than three years is 0.90.

VII.11. Mathematics and Medical Statistics

Likelihood Prior

Posterior

2 3 Mean survival4 5 6

Figure 4 mean survival time Prior, likelihood, and posterior distributions forθ for patients presenting with symp- toms. The posterior distribution is a formal compromise between the likelihood, which summarizes the evidence in the data alone, and the prior distribution, which sum-marizes external evidence that suggested longer survival times.
Likelihoods in Bayesian models need to be fully parametric, although semi-parametric models such as the Cox model can be approximated by high-dimensional functions of nuisance parameters, which then need tobe integrated out of the posterior distributions. Difficulties with evaluating such integrals held up realistic applications of Bayesian analysis for many years, but now developments in simulation approaches such as Markov chain Monte Carlo (MCMC) methods have led toa startling growth in practical Bayesian analyses.
Mathematical work in Bayesian analysis has mainly focused on theories of objective priors, large-sample properties of posterior distributions, and dealing with hugely mul-tivariate problems and the necessary high-dimensional integrals. 8 Discussion The preceding sections have given some idea of the tangled conceptual issues that under lie even routine medical statistical analysis. We need to distinguish a number of different roles for mathematics in medical statistics—the following are a few examples. Individual applications:
here the use of mathematics is generally quite limited, since extensive use ismade of software packages, which can fit a wide variety of models. In nonstandard problems, algebraic ornumerical maximization of likelihoods may be necessary, or developing MCMC algorithms for numerical integration.

927

Derivation of generic methods: implemented in software. This is perhaps the most these can then be widespread mathematical work, which requires ex-tensive use of probability theory on functions of random variables, particularly using large-sample arguments. Proof of properties of methods: most sophisticated mathematics, which concerns this requires the topics such as the convergence of estimators, or the behavior of Bayesian methods under different circumstances.
Medical applications continue to be a driving force in the development of new methods of statistical analysis, partly because of new sources of high-dimensional data from areas such as bioinformatics, imaging, and performance monitoring, but also because of the increas-ing willingness of health policy makers to use complex models: this has the consequence of focusing atten-tion on analytic methods and the design of studies for checking, challenging, and refining such models.
Nevertheless, it may appear that rather limited mathematical tools are required in medical statistics, even for those engaged in methodological research. This is compensated for by the fascinating and continuing debate over the underlying philosophy of even the most common statistical tools, and the consequent variety of approaches to apparently simple problems. Much ofthis debate is hidden from the routine user.
Regarding the appropriate role of mathematical theory in statis-tics, we can do no better than quote David Cox in his 1981 Presidential Address to the Royal Statistical Society (Cox 1981): Lord Rayleigh defined applied mathematics as being concerned with quantitative investigation of the real world “neither seeking nor evading mathematical dif-ficul ties.” This describes rather precisely the delicate relation that ideally should hold between mathemat-ics and statistics. Much fine work in statistics involves minimal mathematics;
some bad work in statistics getsby because of its apparent mathematical content. Yet it would be harmful for the development of the sub-ject for there to be widespread an anti-mathematical attitude, a fear of powerful mathematics appropriately deployed.

Further Reading

Andersen, P. K., O. Borgan, R. Gill, and N. Keiding. 1992.Statistical Models Based on Counting Processes. New York: Springer.

928

Cox, D. R. 1972. Theory and general principle in statistics. Journal of the Royal Statistical Society A 144:289–97. cussion).187–220.. 1981. Regression models and life-tables (with dis-Journal of the Royal Statistical Society B 34: Kaplan, E. L., and P. Meier. 1958. Nonparametric estimation from incomplete observations. Journal of the American Matthews, D. E., and V. T. Farewell. 1985.Statistical association standing Medical Statistics53:457–81.. Basel: Karger. Using and Under VII.12 Analysis, Mathematical and

Philosophical

John P. Burgess

1 The Analytic Tradition in Philosophy

Philosophical problems are never solved for the same reason that treasonous conspiracies never succeed: as successful conspiracies are never called “treason,” so solved problems are no longer called “philosophy.” Philosophy, which once included almost every subject in the university (every subject in which the highest degree is Ph. D.), has thus been shrunk by success. The greatest shrinkage occurred during the seventeenth and eighteenth centuries, when natural philosophy became natural science.
Philosophers of the period, all intensely interested in the emergence of the new science, differed over issues of scientific method. Philos-ophy had always been understood to differ from, for instance, theology, by restricting itself to the methods of reasoned argument and the evidence of experience, with out appeal to authority, tradition, revelation, or faith. But philosophers of the era of the scientific revolution disagreed about the comparative importance of reason and experience.
ingly divided into the rationalists, or the party of rea-son, and the empiricists, or the party of experience.
The In introductory histories, philosophers are ac co rd former, mainly from Continental Europe, were domi-nant in the seventeenth century, while the latter, mainly from the British Isles, predominated in the eighteenth. The rationalists, who included the mathematicians descartes the apparent ability of pure thought—logical deduction[VI.11](/part-06/ren-descartes-15961650) and leibniz [VI.15](/part-06/gottfried-wilhelm-leibniz-16461716), were impressed by from self-evident postulates—to achieve, as it seemed to do in geometry, substantive results with worldly applications;
and they were tempted to adopt similar methods in other areas. Spinoza even wrote his

VII. The Influence of Mathematics

Ethics his to ric peak of the influence of mathematics on phi-in the style of euclid’s [VI.2](/part-06/euclid-ca) Elements, a world los op hy. The empiricists, who included that acute critic of the calculus, Berkeley, recognized that in physics one cannot proceed as the rationalists wished to. The principles of physics are not self-evident, but must be conjectured from and tested against systematic observation and controlled experiment. What puzzled lead-ing empiricists such as Locke and Hume was how pure thought was able to succeed into in geometry.
Thus, while for rationalists mathemat-any area, as it seemed ics was a source of source of a problem methods. , for empiricists it was the fered by Kant, whose system attempted a synthesis of rationalism with empiricism. On the one hand, Kant An influential formulation of that problem was of claimed, geometry and arithmetic are a priori rather than a posteriori, by which he meant that they are knowable in advance of experience rather than depen-denton it.
On the other hand, they are synthetic rather than analytic, which is to say that they are more than mere logical consequences of the definitions of concepts, statements whose denials would amount to con-tradictions in terms. Philosophy of mathematics, today a smallish specialty within philosophy of science, itself a smallish specialty within epistemology or the theory of knowledge, played a much more important role for Kant, who in his own summary of his system gave pride of place to the question, “How is pure mathematics possible?” as the first case of the question, “How is syn-thetic a
priori
knowledge possible?” Kant’s proposed solution was based on the insight that our knowledge must be shaped as much by the nature of ourselves, the knowers, as by that of what is known. Kant concluded that space, the subject matter of geometry, and time, according to him the ultimate subject matter of arithmetic, were not features of things as they are in themselves, but rather of things as we must perceive and experience them, given the nature of our sensibility.
Synthetic a priori knowledge is ultimately knowledge, knowledge of the forms that we supply, self - and into which reality independent of us pours con-tent. This distinction between phenomena, or things as we experience them, and noumena, things beyond our experience, about which we can wonder but never know, was central to Kant’s entire system, his ethics as much as his metaphysics. Such is the history, painted in quick strokes and with a broad brush, of early modern philosophy. After Kant, the story no longer has as clear a plotline.

VII.12. Analysis, Mathematical and Philosophical System building continued for another generation, down to Hegel. But eventually, and inevitably, his system collapsed under its own weight, and in the ensuing reaction philosophers went off in all direc-tions. Out side academia, striking figures sporadically appeared on the borders of philosophy and literature, notably Nietzsche. Meanwhile, academic philosophy, rather like Victorian architecture, experienced a num-ber of revivals, of which the Kantian was the most prominent.
But even as neo-Kantianism prevailed inthe schools, the Kantian conception of mathematics was under attack. First, though the development of consistent non-Euclidean geometries in itself only confirms Kant’s claim that geometry is synthetic, those who developed alternatives to Euclid were quickly led to question whether Euclidean geometry is really a pri-ori, as Kant had claimed.
gauss [VI.26](/part-06/carl-friedrich-gauss-17771855) had already concluded that geometry is a posteriori, or, as he putit, of the same status as mechanics, and riemann [VI.49] argued at greater length that an examination of the hypotheses that lie at the foundation of geometry must lead us into the domain of the neighboring science of physics.
Second, while few doubted Kant’sclaim that arithmetic is a priori, a challenge arose to the claim that it is synthetic in the work of frege [VI.56](/part-06/gottlob-frege-18481925) and (slightly later, but largely indepen-gottlob dently)a derivation of arithmetic from logic along with anbertrand russell [VI.71](/part-06/bertrand-arthur-william-russell-18721970), who both attempted appropriate definition of number. deserved to be, despite the publicity given it by Rus-Frege’s work long remained less well-known than it sell once he became aware of it himself.
As a result, Frege, though very influential at present, is more a precursor of the tradition in philosophy within which he stands than a founder, the founders being rather Rus-sell and his contemporary and colleague G. E. Moore. That pair began by rebelling against the philosophy of their teachers, a late nineteenth-century aberration called absolute idealism, a kind of Hegel revival; butit soon became apparent that the rebels were aiming at more than just a return to the traditional empiri-cism of British philosophy from Bacon to Mill.
Meanwhile, Edmund Husserl was developing the first formof what was to become the great rival to the Russell Moore tradition in twentieth-century philosophy. Like Frege, Husserl had begun his career with work in the philosophy of arithmetic, work of which Frege himself had taken notice, and no one in the early twentieth century expected that Husserl’s and Frege’s heirs would,

929

within a generation, split into two non communicating lines of descent. The two lines of development or traditions are oddly named, with a stylistic label, “analytic,” for one, anda geographical label, “Continental,” for the other. This odd labeling reflects the historical fact that the prin-cipal representatives of the analytic style in Continental Europe (Ludwig Wittgenstein, Rudolf Carnap, and others) were forced to go into exile in the Englishspeaking world in the 1930 s, as a result of the process generally known as the Nazification (but celebrated by Husserl’s estranged student Martin
Heidegger asthe “self-affirmation”) of the German university. This physical separation—more than Heidegger’s break with his teacher, hostility toward science, rebarbative prose style, or loathsome politics—created a split that no one could have anticipated twenty years earlier. ers on each side tend to read and cite only prede-With the years the gap has widened, as later writcessors on that side. Indeed, the divide has extended backwards in time.
For while Borges has said that in literature great writers create their own predecessors, in philosophy even not-so-great writers can do so, and the two twentieth-century traditions came to see differ-ent nineteenth-century figures as leading up to themselves, thus extending the division between them right back to the death of Kant (with Hegel rather than Heidegger being identified as the first distinctively Conti-nental philosopher).
The gap between the reading lists of students in the two traditions has become so large that nowadays for a student trained in one to take up the other is virtually to switch disciplines. The word “tradition,” rather than “school” or “movement,” is used advisedly, for each tradition has con-tained several movements, as well as individuals who defy classification by school. It would be a serious mis-take to suppose that there is any doctrine or method on either side of the analytic/Continental divide that all philosophers on that side uphold.
In particular, analytic philosophy should not be confused with log-ical positivism, a Viennese–American school defunct for more than half a century, nor should continental philosophy be confused with existentialism, a literary philosophical movement out of fashion in Paris for nearly as long. Logical positivism and existentialism were indeed varieties of analytic and Continental phi-losophy respectively, and perhaps the most prominent varieties half a century or so ago; but each was even then far from being the only variety. In assessing the

930

influence of mathematics on philosophy in the twen-tieth century, one must take into account divisions within each tradition as much as the division between the two traditions. It may be true that since the early work of Husserl there has been comparatively little contact between mathematics and philosophy on the Continental side, though the label “structuralist” is broad enough to take in both the mathematics of various anthropological and linguistic doctrines that bourbaki [VI.96](/part-06/nicolas-bourbaki-1935) and the became influential in France after the eclipse of exis-tentialism;
but it is also true that the direct influence of mathematical ways of thinking on many individuals and groups within the analytic tradition has been negligible. Thus, just as there are distinguishable German and French subtraditions within the Contin en-tal tradition, so within the analytic tradition one may distinguish a more technically oriented subtradition, including Frege (who was himself a professor of math-ematics), Russell (who as an undergraduate had concentrated on mathematics before turning to philoso-phy), and the logical positivists (who had mostly been trained as theoretical
physicists), from a nontechnical or antitechnical subtradition, including Moore, Witt gen-stein, the so-called ordinary-language school of midcentury Oxford, and others. (Wittgenstein even went sofar as to claim that mathematicians always make bad philosophers, a sweeping judgment condemning many right back to Thales andthe immediate target was Russell.) However, there has pythagoras [VI.1](/part-06/pythagoras-ca), though been very much more communication and influence back and forth between the two subtraditions within each tradition than between the two traditions.
the influence of mathematics after the period of the founders has been occasional and sporadic, and has Even among the more technical analytic philosophers come mostly from areas such as mathematical logic, computability theory, probability and statistics, game theory, and mathematical economics (as in the workof the philosopher–economist Amartya Sen), which are rather far from the core of pure mathematics as math-ematicians see it.
Thus it is hard to imagine the solution to any of the Millennium Prize Problems (except perhaps the P vs NP problem, the one question com-ing from theoretical computer science rather than core mathematics) having measurable impact even among the most susceptible analytic philosophers. In contrast to this limited mathematics, resulting from its effect on the thought direct influence, the in direct influence of of the early figures Frege and Russell, has been over-

VII. The Influence of Mathematics

whelming even among the less technically oriented ana-lytic philosophers. The branches of mathematics that influenced Frege and Russell were geometry and algebra and, above all, the third great branch of core mathe-matics, “analysis,” in the mathematical rather than the philosophical sense, the branch beginning with differ-ential and integral calculus. (Frege and Russell were not influenced it, and mathematical analysis was a key influence on itsby mathematical logic:
rather, they created creation.) 2 Mathematical Analysis and Frege’s New Logic Let us turn, then, to consider the state of mathemat-ical analysis in the days of Frege and Russell, beginning our account with a quick look back at the situationca. 1800. As rich as its results were, and as powerful its applications, mathematics at the beginning of the nineteenth century was concerned with but a few structures: the natural, rational, real, and complex num-ber systems; and the Euclidean and projective spaces of dimensions one, two, and three.
All that changed quickly when the work of Gauss, hamilton [VI.37](/part-06/william-rowan-hamilton-18051865), and others introduced the first non-Euclidean spaces and first noncommutative algebras, after which a pro li fer-ation of new mathematical structures rapidly ensued.
this rigorizing generalizing tendency, since the proliferation of novelties tendency went hand in hand with a persuaded mathematicians that they needed to adheremore strictly than had become customary to the ancient ideal of rigor, according to which all new results in mathematics are to be logically deduced from previous results, and ultimately from a list of explicit axioms. For with out rigor, intuitions derived from familiarity with more traditional structures might easily be unconsciously transferred to new situations where they areno longer appropriate.
not only in geometry and algebra, but also in mathe-mat ical analysis. Generalization in mathematical analy-Generalization and rigorization went hand in hand sis took place in two directions. The eighteenth-century notion of “function” had been that of an operation applying to one or more real numbers as inputs or“arguments” and yielding a real number as output or “value,”f (x) = . in  xaccording to a certain formula+ . os  x or f (x, y) = x2 + y2. On the one, such as hand, nineteenth-century mathematicians generalized by dropping the requirement of an explicit formula.
Onthe other hand, Cauchy, Riemann, and others extended

VII.12. Analysis, Mathematical and Philosophical the notion to allow as arguments not only real num-bers but also complex numbers, that is, numbers of the form the “imaginary” square root ofa + bi, where a and b are real numbers and i is-1. on two levels.
First, for each theorem it had to be clearly stated just what special properties were being assumed Rigorization in mathematical analysis also took place for the functions to which the result was supposed to apply, since special properties such as definability by a formula (or continuity or differentiabili ty) were no longer being built into the highly general notionof function itself;
more over, the relevant properties themselves had to be clearly defined (leading to the so-called of such concepts as “continuity” and “different iabil-weierstrass [VI.44](/part-06/karl-weierstrass-18151897) epsilon–delta definitions ity” in freshman calculus), since, asremarked, until one has rigor in one’s definitions onepoincaré [VI.61](/part-06/jules-henri-poincar-18541912) cannot have rigor in one’s theorems.
Second, the properties assumed for the numbers to which the functions apply had also to be clarified and stated explicitly as axioms, with the properties of complex numbers being derived by logical definition and deduction from properties of real numbers (by Hamilton), which themselves in turn were derived from properties of rational num-bers (by dedekind [VI.50](/part-06/julius-wilhelm-richard-dedekind-18311916) and cantor [VI.54](/part-06/georg-cantor-18451918)), which themselves in turn were derived from properties of the system of natural numbers 0, 1, 2, and so on.
Here Frege wished to press still further, and to do what Kant had said could not be done, and derive the properties of the natural numbers themselves from pure logic. For this purpose he needed to become more self-conscious about logic than even the most rigorist mathematicians: he needed not merely to adhere implicitly to the rules and standards of logical def-inition and deduction, but also to analyze explicitly those very rules and standards themselves.
Such self-conscious analysis of definition and deduction was a topic that had, since antiquity, traditionally belonged to philosophy rather than mathematics. Frege needed to carry out a revolution in this philosophical subject, one that would bring it much closer to mathe-matics, and would bring progress to a field that Kant had described as having advanced not a step beyond the state in which it was left by its founder, Aristotle.(The description is slightly exaggerated, but essentially correct, in that each step forward in the two millen-nia after Aristotle had been followed by a step back.)
It was Frege’s new logic, detached from its original roleas part of a special project in foundations of arithmetic and applied to quite diverse subject matters, that was

931

to become the single most important general instru-ment for philosophical analysis in the twentieth century. Indeed, to a large degree philosophical analysis simply than mathematical notions, carried out with the aid ofis the logical analysis of philosophical rather Frege’s broad new logic, or still broader extensions ofit introduced by his successors. It was by the creation of this general instrument of a new logic, rather than the specialized application he made of it to the philosophy of mathematics, that Frege became the grand-father of analytic philosophy.
And the novelty in Frege’s logic was directly inspired by novel developments in mathematical analysis, as he himself emphasized. In an article entitled “Function and concept,” Frege describes the broadening of the notion of function as follows (in the translation by Peter Geach and Max Black): Now how has the reference of the word “function” been extended by the progress of science? We can distinguish two directions in which this has happened.
In the first place, the field of mathematical operations that serve for constructing functions has been extended. Be sides addition, multiplication, exponentiation, and their converses, the various means of transition to the limit have been introduced—to be sure, with out people’s being always clearly aware that they were thus adopting something essentially new.
People have even gone further still, and have actually been obliged to resort to ordinary language, because the symbolic language of Analysis failed, e.g., when they were speakingof a function whose value is 1 for rational and 0 for irrational arguments. [This is a famous example oflet [VI.36].] Secondly, the field of possible arguments di rich and values for functions has been extended by the admission of complex numbers. In conjunction with this, the sense of the expressions “sum,” “product,” etc.had to be defined more widely.
Frege adds at the end, “In both directions I go still further.” For it was the broadening of the notion of function by mathematicians that provided Frege with the clue he needed to develop a logic broader than Aristotle’s. by Frege’s logic, one must understand something of Aristotle’s.
Though it is a pretty poor achievement if Before one can appreciate the advance represented it is considered as the best the human race could do in this area in a couple of thousand years, it is a bril-liant one when considered as the work of a single individual in the course of a career devoted to many other projects. For Aristotle created from nothing the science of logic, whose aim is to distinguish valid from in valid

932

inferences of conclusions from premises. Here an inference is valid if its form alone, regard less of the material truth or falsehood of premises and conclusions, guarantees that sion is true. Equivalently, the inference is valid if in allif the premises are true, then the co nc lu inferences of the same form in which the premises are true, the conclusion is true.
Thus, to adapt an example of Lewis Carroll, the inference from “I believe what-ever I say” to “I say whatever I believe” is not valid, because there are inferences of identical form in which the premise is true and the conclusion false, such as the inference from “I see whatever I eat” to “I eat whatever I see.” ited range of forms of potential premises and conclu-The scope of Aristotle’s logic is limited by the lim sions he recognizes. In fact, he recognized only four:
the universal affirmative “All A’s are B’s,” the universal negative“Some A’s are B’s,” and the“No A’s are B’s,” the particular negative particular affirmative“Some A’s are not B’s” or “Not all A’s are B’s.” The premise“I believe whatever I say” amounts to “All things that I say are things that I believe,” and hence is a univer-sal affirmative.
The invalidity of the inference in the Lewis Carroll example exemplifies the invalidity of the inference from “All A’s are B’s” to “All B’s are A’s.” The validity of the inference from the two premises“All Greeks are human beings” and “All human beings are mortal” to the conclusion “All Greeks are mortal”exemplifies the validity of the inference from “All A’s are B’s” and “All B’s are C’s” to “All A’s are C’s,” tradi-tion ally called the “syllogism in Barbara,” for reasons that need not concern us here.
Aristotle’s logic was inpart inspired by the practice of deduction in philosophical debate (“dialectic”) and in part by the practice of deduction in mathematical theorem-proving (“demon-st ration”), and he offers in his Posterior Analytics an account of a deductive science that is presumed to be based on the practice of the contemporary geometer Eudoxus, in the same sense and to the same degree in which his account in the Poetics of tragedy is based on the practice of the contemporary playwright Euripi-des.
But, in fact, Aristotle’s logic is inadequate for the analysis of mathematicians’ actual arguments, because he makes no provision for forms of argument involving the valid argument from “All squares are rectangles”relations. He cannot, for instance, analyze properly to “Anyone who draws a square draws a rectangle,”because he has no way of representing adequately the form of the conclusion.

VII. The Influence of Mathematics

tory logic text, you will find instructions on how torepresent symbolically the forms of arguments involv-By contrast, if you open any present-day introducing relations. The example just given would appear textbook-style as follows:

$∀x(Square(x) \to Rectangle(x))$

∴∀y(∃x(Square(x)& Draws(y, x)) \to∃x(Rectangle(x)& Draws(y, x))).

In words this would amount to the following. For everyx, if x is a square, then x is a rectangle. Therefore, for everyy, if there is an x such that x is a square and rectangle andy draws xy, then there exists an draws x. (Thus “$\to$” means “ifx such that. . .x, thenis a. . .,” “∀” means “for every,” and “∃” means “there is.”) This style of logical analysis is the invention of Frege.
kind of function, a function that (generalizing the math-ematical notion in one direction) need not be given by Underlying it is a notion of a “concept” as a special any kind ofalizing the mathematical notion in another direction)mathematical description, and that (gener need not have as arguments any kind of concept for Frege is a function whose argument or argu-numbers. A ments may be any objects at all, and whose values are Truth and Falsehood.
Thus, the concept Wise applied to the argument Socrates produces the value Truth, since Socrates is wise (at least to the extent of recog-nizing that he lacked perfect wisdom), while the concept Immortal applied to Socrates produces Falsehood, since Socrates was not immortal but died of drinking hemlock. Frege is able to handle relations because he follows the mathematical analysts who allowed func-tions of two or more arguments.
Thus the two-argument concept or relation Taught applied to Socrates and Plato, in that order, produces Truth, since Socrates taught Plato, while applied to Plato and Socrates, in order, produces Falsehood, since Plato did not teach that Socrates. Aristotle’s simple “All A’s are B’s” becomes, for Frege, the more complex “For all object sx, if A(x), then B(x).” At the price of such extra complexity, he is able to logically analyze arguments turning onrelations, as Aristotle was not.
of the concepts Animal and Rational in the sense of “language-using.” In present-day textbook notation Aristotle analyzed the concept Human Being in terms (writing “↔” for “if and only if”), this would be Human(x) ↔ Animal(x)& Rational(x). to analyze the notion of Mother (respectively, Father)But Aristotle, with no theory of relations, was unable

VII.12. Analysis, Mathematical and Philosophical in terms of Female (respectively, Male) and Parent. For Frege, Mother is analyzed as follows: Mother(x) ↔ Female(x)&∃y Parent(x, y). A mother is a female who is some one’s parent, and analogously for a father. Frege was even able to ana-lyze the concept Ancestor in terms of the concept Parent, though this analysis is beyond the scope of the present sketch.
Later philosophical analysis would have been unthinkable with out Frege’s broadening of logi-cal analysis beyond Aristotle’s, and Frege rightly saw his broadening of logical analysis as a direct extrapo-lation from the nineteenth-century mathematical analysts’ broadening of the notion of function they had inherited from their eighteenth-century predecessors. Russell’s Theory of Descriptions3 Mathematical Analysis and Like Frege, Russell found in mathematics both a source of problems and a source of methods.
For the purposes of a specialized investigation of problems inthe philosophy of mathematics, he created an instrument, his theory of descriptions, and a more general method, that of contextual definition, which his successors took up and applied to many other problem areas. Indeed, it was not merely Russell’s successors who applied these ideas to areas out side philosophy of mathematics, since Russell himself did so in his first publications on the subject.
Thus it is not apparent from Russell’s still widely read “On denoting,” pub-lished in 1905 and even today a key item on the syllabus of students of analytic philosophy, that the theory of descriptions originated in the course of studies in foundations and philosophy of mathematics. Rather, this isa fact mentioned in Russell’s au to biographical writings and known to historians of twentieth-century philoso-phy.
The degree to which the method of contextual definition, which the theory of descriptions exemplifies, was inspired by the nineteenth-century rigorization of analysis is perhaps not sufficiently appreciated even by such specialists. A principal puzzle Russell addresses in “On denoting” is that of so-called negative existentials, such as“The king of France does not exist.” In superficial grammatical form this statement resembles “The queen of England does not agree,” and to that extent it appears to involve picking out an object (in this case, a person), and then attributing a property to him
(or her, as the case may be). Thus it seems that in order to say that some one or something does not exist, one must assume

933

that in some sense there is such a person or thing, to whom or which the property of nonexistence may be ascribed. Russell cites Alexius Meinong (a student of Husserl’s teacher Franz Brentano) as a philosopher committed to such a view. For Meinong had a theory of “objects beyond being and non being,” exemplified by The Golden Mountain and The Round Square. But as Scott Soames reveals, in histhe Twentieth Century, volume I:
Philosophical Analysis in The Dawn of Analysis the first days of his and Moore’s joint rebellion against, Russell himself had briefly held a similar view in absolute idealism.
It was through the development of his theory of descriptions that Russell was able to free himself from anything like commitment to Meinongian “objects.” According to that theory, to say that a Golden Mountain exists is to say that there is something that is both golden and a mountain:∃x(Golden(x)& Mountain(x)). To say that the Golden Mountain exists is to say that there is one thing that is both golden and a mountain and no other such thing:

∃x(Golden(x)& Mountain(x)

&∼∃y(Golden(y)& Mountain(y)&y ≠ x)).

(Here “∼” represents “it is not the case that.”) This is logically equivalent to saying there is something such that a thing is both golden and a mountain if and onlyif it is identical with that thing: ∃x∀y(Golden(y)& Mountain(y) ↔ y = x). To say that the Golden Mountain does not exist is simply to deny this: ∼∃x∀y(Golden(y)& Mountain$(y) ↔ y = x)$. To say that the king of France is bald is, similarly, tosay that there is something such that a thing is king of France if and only if it is identical with that thing, and that thing is bald: $∃x(∀y(King - of - France(y) ↔ y = x)$& Bald(x)).
sell’s theory, whose main point should be clear from these few examples: when the logical form is properly This is not the place to go into the subtleties of Rus analyzed, using the new logic, the phrase “the Golden Mountain” or “the present king of France” disappears. With it vanishes any appearance that we must acknow-ledge such an “object” as the Golden Mountain or king of France even in order to deny that any such object exists. The examples illustrate in miniature two lessons:

934

first, that the logical form of a statement may differ sig-nificantly from its grammatical form, and that recognition of this difference may be the key to solving or dissolving a philosophical problem; second, that the correct logical analysis of a word or phrase may involve an explanation not of what that word or phrase taken by itself ta in ing the word or phrase means, but rather of what mean. Such an explanation whole sentences conis what is meant by ation that does not provide an analysis of the word or contextual definition:
a de fin i phrase standing alone, but rather provides an analysisof contexts in which it appears. cal form, and his claim that the former may be system-at ically misleading, was to prove immensely influen-Russell’s distinction between grammatical and logitial, even among nontechnically oriented philosophers, such as the Oxford ordinary-language school, who saw no need to use special symbols to represent logical forms, and objected to details of Russell’s specific application of the distinction in his theory of descrip-tions.
But Russell’s notion of contextual definition is one implicit already in the practice of Weierstrass and other leaders of the nineteenth-century rigorization of analysis, and familiar to Russell from his undergradu-ate mathematical studies, so that even the antitechnical ordinary-language school of philosophical analysts are being influenced at one remove (and, so to speak, in spite of themselves) by mathematical analysis. Contextual definition was the tool the rigorizers used to dispel the mysteries surrounding the notions of infinitesimals and infinities in the calculus.
The followers of Leibniz had, for instance, written dfor the derivative of a functionf (x), where in df (x)/x wasdx supposed to represent an “infinitesimal” change in the argument, and dchange$f (x + dx)f (x)- f (x)$a corresponding “infinitesimal”in the value when the argument changes fromx to x + dx.
(Leibniz claimed that this was all just a figure of speech, but his followers seem to have taken it literally.) These infinitesimals could be treated as nonzero in some circumstances—in particular, one could divide by them, as one cannot divide by zero—and yet treated as zero and neglected in other circumstances. Thus the derivative of the function$f$ (x) = x2 was computed as follows: d$f (x) = f (x + dx) - f (x) = (x + dx)2 - x2$ dx 2 x dx +d(xdx)2 dx= dx = 2 x + dx = 2 x.

Here dand zero at the last step—the kind of procedure thatx is treated as nonzero at the next-to-last step,

VII. The Influence of Mathematics

outraged critics like Berkeley. In the course of the nineteenth-century rigorization, the infinitesimals were banished: what was provided was not a direct expla-nation of the meaning of df (x) or dx, taken sepa- rately, but rather an explanation of the meaning ofcontexts containing such expressions, taken as wholes.
The apparent form of di nf in it es i ma ls df (x) and df (x)/x was explained away, thedx as a quotient of true form being of an operation of differentiation d$(d/dx)f (x)$, indicating the application/dx applied to a function Similarly, such an expression as limf (x)$. 1/x =$. nfty, or “the limit of 1 explained as a whole/x as , with out requiring any explana-xgoes to zero is infinity,” was$x \to^{0}$ tion of “$\infty$” or “infinity” taken separately. The details, which now appear in any freshman calculus textbook, need not detain us.
What is important historically is that the notion of contextual definition employed in Russell’s theory of descriptions was an idea that would have been familiar to him as a student of mathematics. To acknowledge this is, need less to say, not to deny that there is a certain genius involved in extracting such an idea from its original context of mathematical analysis and employing it to resolve philosophical puzzles.
To acknowledge the germs of Russell’s ideas in ideas of Weierstrass is merely to indicate more pre-cisely what kind of genius Russell, like Frege before him, was bringing to bear on philosophical issues: akind of philosophical genius in formed by knowledge of mathematics. 4 Philosophical Analysis and Analytic Philosophy Anyone who acquires a new tool is in some danger ofbehaving like the proverbial man with a hammer to whom everything seems to be a nail.
There is no deny-ing that some of the first people to apply the new methods of Frege and Russell were over enthusiastic about what such methods could accomplish. Russell himself, having established to his own satisfaction that math-ematics could be reduced to pure logic once one had a sufficiently rich and powerful logic, went on to conclude that every science apart from mathematics could be reduced to logical compounds of statements about immediate sensory impressions—“sense data” as they were called.
The logical positivists reached a similar conclusion, and were ready to ban any statement that did not admit such a reduction, from the assertions of Hegelian or absolute idealist metaphysicians on, as a“pseudo-statement,” or mere nonsense.

VII.13. Mathematics and Music

ence, even the parts concerned with theoretical enti-ties not directly observable (such as quarks and black Conscientious attempts to work out just how sciholes in the science of today), could be reduced log-ically to statements about sense data, or at least to statements about everyday observable objects (suchas meter readings), failed.
Hence the positivists were forced to acknowledge that their program could not succeed, and (since they did not wish to dismiss large parts of modern science as mere pseudo-statements) that their standards of meaningfulness were too rigid. But as Soames emphasizes, this very acknowledgment of failure was a kind of success, because few if any philosophical schools before the positivists had even stated their aims with sufficient clarity to make it possible to see that they were unachievable.
The new log-ical resources provided by Frege and Russell had both tempted the positivists to conjecture more than they could prove their conjecture was impossible.and made it clear to them that proof of methods gradually came to be better understood. Rus-sell’s theory of descriptions had been hailed by his With experience the scope and limits of the new student F. P. Ramsey as “a paradigm of philosophi-cal analysis,” which indeed it is.
But it came to be appreciated that the kind of application Russell madeto the issue of negative existentials, where a philosophical problem was completely dissolved by philosophical analysis, would seldom be possible. Analy-sis, in general, is only a preliminary, a process that makes it clearer what the real problems are, and nota panacea, exposing all apparent problems as mere pseudo-problems. has been replaced by dedication:
recognition of the lim-itations of Frege’s and Russell’s methods has led not As analytic philosophy has developed, enthusiasm to the abandonment of the goal of clarity, which wasthe underlying motive of the great pioneering figures, but rather to firmer adherence to it.
Today, when onecan read large tracts of philosophy in the analytic tradition with out encountering a single explicit analysis, let al one one expressed in special logical symbolism, one still finds almost every where a clarity of prose style that instantly distinguishes writing in this tradition from the writings of Continental philosophers (to say nothing of the Continental iz ing philosophasters to be found in certain humanities departments in universities in the English-speaking world).
This clarity—found, to besure, already in the mathematician–philosopher Descartes, the first truly modern philosopher, but lost in

935

many of his successors—is the ultimate influence and legacy which the pioneers of analytic philosophy transmitted from mathematics to their philosophical heirs.

Further Reading

I recommend eth Century (Princeton, NJ: Princeton University Press, Philosophical Analysis in the Twenti2003) by Scott Soames for those wish ing to read more about this subject. Each of the two volumes of this work contains substantial lists of primary and secondary sources at the end of each of its several parts. VII.13 Mathematics and Music

Catherine Nolan

1 Introduction and Historical Over view

Music is the pleasure the human mind experiences from counting with out being aware that it is counting. This intriguing remark of leibniz [VI.15](/part-06/gottfried-wilhelm-leibniz-16461716), from a 1712 letter to fellow mathematician[VI.17](/part-06/christian-goldbach-16901764), suggests a serious connection between math-christian goldbach ematics and music, two subjects—one a science, the other an art—that may at first seem very different from each other.
Leibniz was perhaps thinking of the long-standing historical and intellectual association of the two disciplines that date back to the time ofras [VI.1](/part-06/pythagoras-ca), when the subject of music was part of an pyth a go elaborate classification scheme of knowledge in the mathematical sciences. This scheme became known in the Middle Ages as thethe four disciplines of arithmetic, music (harmonics), quadrivium, and consisted of geometry, and astronomy.
In the Pythagorean world-view, these subjects were interlinked, since in one way or another they were all concerned with simple ratios. Music was merely the aural manifestation of a more universal harmony, which was likewise expressed by rela-tion ships between numbers, geometrical magnitudes, or the motions of celestial bodies.
Harmonic conso-nance of musical intervals resulted from simple ratios of the first four natural numbers, 1:1 (the unison), 2:1(the octave), 3:2 (the perfect fifth), and 4:3 (the perfect fourth), and was demonstrated empirically by the ratios of lengths of vibrating strings on the ancient instrument the monochord.1 Beginning with the Scientific not artistic, purposes. It consisted of a single string stretched between two fixed bridges. A movable bridge between the fixed bridges was1.
The monochord was an instrument designed for demonstration, used to adjust the length of the string as it was plucked to produce sound, there by altering the pitch of the sound.

936

Revolution of the seventeenth century, theories of tun-ing and temperament of musical intervals required more advanced mathematical ideas as well, such as logarithms and decimal expansions. Musical composition has been inspired by mathematical techniques through out its history, although mathematically inspired compositional techniques are associated mainly with music of the twentieth, and now twenty-first, centuries. A striking early example appears in the section on melody in a monumental treatise on music, entitled37), by the mathematician Marin Mersenne.
Mersenne Harmonie universelle (1636 applied simple (from today’s perspective) combinatorial techniques to the distribution and organization of notes in melodies. For example, he calculated the number of different arrangements or permutations of notes, for eachn between 1 and 22 (twenty-two notesn delimiting the range of three octaves). The answer is of course musical staves all 720 (6!) permutations of the six notes n!, but in his zeal to illustrate this, he notated on of the minor hexachord (A, B, C, D, E, F), occupying afull twelve pages of Harmonie universelle.
He went on to explore more complicated problems such as determining the number of melodies of a certain number of notes selected from a larger number, or determining the number of arrangements of finite collections of notes containing certain numbers of repetitions of one or more notes. He illustrated some of his findings with combinations of letters as well as musical nota - tion, there by showing that the music was incidental to the problems, which were in essence purely combinato - rial.
Such exercises, while seemingly of little practical or aesthetic value, at least demonstrated the great musi-cal diversity that was in principle available with only a limited set of resources. The polymath Mersenne was a composer and practicing musician as well as a mathematician, and his fas-cination with applying a relatively new mathematical technique to music composition showed a level of inter-est in abstract connections between mathematics and music that is shared by many music theorists, and to a lesser degree by performing musicians and nonspecialist music enthusiasts.
The patterns of music, in particular pitch and rhythm, lend themselves well to mathe-mat ical description, and some of them are amenable to algebraic reasoning. In particular, the system of twelve equal-tempered notes is naturally modeled using modular arithmetic binatorial arguments, was used in the music theory of[III.58](/part - 03/modular - arithmetic), and this, together with comthe twentieth century. In this article we survey the asso-

VII. The Influence of Mathematics

ciation of mathematics and music from its concrete representation in sound itself, through its manifestation in the working materials of composers, and finally to its explanatory power in abstract music theory. 2 Tuning and Temperament The most obvious relationships between mathematics and music appear in acoustics, the science of musical sound, and particularly in the analysis of the inter-vals between pairs of pitches.
With the development of polyphonic music in the Renaissance period, the Pythagorean conception of consonance based on the simple ratios of the integers from 1 to 4 eventually came into conflict with musical practice. The acousti-cally pure perfect consonances of Pythagorean tuning were well-suited for medieval parallel organum,2 but in the fifteenth and sixteenth centuries use was increas-ingly made of the so-called imperfect consonances, that is, major and minor thirds and their octave inversions, minor and major sixths.
In Pythagorean tuning, intervals are derived by successions of perfect fifths, sothe corresponding frequency ratios are powers of 3. In conventional Western music, twelve perfect fifths in succession, C–G–D–A–E–B–F*–C$*$–G$*$–D$*$–A$*$–E$*$–B$*$, are2 supposed to equal seven octaves (Cnot work in Pythagorean tuning, since$= B^{*}()$, but this does^3 )^12 does not equal 2 fifths will never result in a whole number of octaves.7. Indeed, a succession of Pythagorean perfect2 As it happens, twelve Pythagorean perfect fifths give an interval slightly larger than seven octaves.
The dif-ference is a small interval known as the Pythagorean comma, which corresponds to a ratio of( {}32)12/27, which is about 1.013643. of successive single pitches. The problems associated with it start to arise when pitches sound simultane-Pythagorean tuning was originally conceived in terms ously. While Pythagorean fifths between simultaneous pitches sound pleasing with their simple 3:2 ratios, Pythagorean thirds and sixths have much more complex ratios that sound harsh to Western ears.
These came to be replaced by the simple ratios ofnation, which are ratios of quite small whole numbers.just into These ratios were considered “natural” because they adding a voice (or voices) to an existing plainchant melody (firmus2. Organum). In its original form, the added voice proceeded in parallelis the earliest form of musical polyphony, and involved cantus motion to the plainchant melody at the interval of a perfect fourth or fifth. VII.13.
Mathematics and Music Notes CDEFGABC Intervals(ratios) 98 109 1615 98 109 98 1615 Figure 1 scale tuned in just intonation. Successive intervals in a major reflect the ratios of the natural over tone series.3 The Pythagorean major third, which has the relatively com-plex ratio of$(3 )4/22$, or81 , was replaced by the slightly smaller major third of just intonation, which has the much simpler ratio 5:4. The difference between these2 64 two intervals is known as the corresponds to the ratio 81:80, or 1.0125.
Likewise, syntonic comma, which the Pythagorean minor third has ratio 32:27, and so isslightly smaller than the minor third of just intonation, which has ratio 6:5. The difference is again a syntonic comma. The Pythagorean major and minor sixths, the octave inversions of the thirds, also differ from their just counterparts by a syntonic comma. Suppose that you want to build a C-major scale in just intonation. You can do it as follows. Start with C and define each other note by the ratio of its frequency to that of C.
The subdominant and dominant, that is, F and G, have ratios 4:3 and 3:2, respectively. From these three notes one can build major triads in the ratios 4:5:6. So E, for instance, which belongs to the major triad that starts with C, has ratio 5:4. Similarly, A has ratio 5:3, since it is in a ratio 5:4 with F. With this kind of calculation, one ends up with the scale shown in figure 1, where the fractions now represent the fre-quency ratios between successive notes. The smaller whole tone (10:9) between notes D and E creates into-nation problems for the supertonic triad, D–F–A.
While the minor triads on E and A (the mediant and submediant) produce the proportion 10:12:15, the minor triad on D is out of tune. Its fifth, D–A, is a syntonic comma flat, as is its third, D–F, which is in fact a pythagorean minor third. intervals offered a practical solution to the problems inherent in just intonation by distributing the syntonic Tempering (increasing or decreasing) the size of comma among the major thirds or the perfect fifths of the scale, there by compromising the purity of one interval to preserve the purity of another.
This pracof the fundamental pitch, and the first six partials generate the inter-vals of the major triad. For instance, the first six partials of the over - 3. The partials of the over tone series are multiples of the frequency tone series of a fundamental pitch C are C (1:1)$, C (2:1)$, G (3:1)$, C (4$:1)$, E (5$:1)$, G (6$:1). 937 tice became known as meant one temperament. Vari-ous systems of meant one temperament were put forward in the sixteenth and seventeenth centuries for the tuning of keyboard instruments, the most common of which was quarter-comma meant one temperament.
Inthis system the perfect fifth is lowered by a quarter of a syntonic comma so that the major thirds have the pure ratio 5:4. is that, while modulation to closely related keys sounds pleasing, modulation to more remote keys sounds out A perpetual problem with meant one temperaments of tune. The system of equal temperament, in which the syntonic comma is distributed evenly among all twelve semitones of the octave, gradually became adopted because it removed the limitations on keys for modulation.
The discrepancies between just and equal-tempered intervals are small and easily accepted by most listeners. The ratio of an equal-tempered semi-tone is 12$\sqrt{2}$, or 1.05946 . . .; by comparison, a just semitone, with ratio 16:15, is 1 equal-tempered perfect fifth, seven semitones, is.06666 . . . . The ratio of an12. qrt{2}7 or fifth, with ratio 3:2, is of course 1.5. In equal tempera - 12$\sqrt{128}$, which is 1.498307 . . .
, where as a just perfect ment, one starts from a reference such as the note A, which is usually taken to have frequency 440 Hz.4 All other notes have frequencies of the form 440 wheren is the number of semitones between the note(12. qrt{2})n, in question and the reference note A.
In equal tempera - ment, enharmonic notes such as C* and D+ are acousti-cally identical—that is, they share the same frequency. Equal temperament was well-suited for the kind of music that was written from the eighteenth century onward, with its much greater range of modulations and chromatic harmonic vocabulary. The unit of the cent was defined by A. J. Ellis as the ratio between two pitches separated by one hundredth of an equal-tempered semitone, and became the most commonly used unit for measuring and comparing intervals.5 The octave consists, therefore, of 1200 cents.
Ifdistance in cents between the corresponding pitches isa and b are two frequencies, then the given by the formula notice that ifa = 2 bn then one does indeed get the = 1200 log2(a/b). (As a check, answern = 1200.) cycles per second (abbreviated as “cps”). More commonly, the number of cycles per second is identified in units called4. The frequency of a pitch is a measurement of the number of hertz (abbreviated as “Hz”), named after the physicist Heinrich Rudolf Hertz.5.
Ellis’s account of the cent appeared in his appendix to the eminent nineteenth-century physicist Hermann von Helmholtz’s Sensations of Tone (1870; English edn., 1875). On the 938 the octave into more than twelve parts were proposed Microtonal systems based on the equal division of and realized by some composers in the twentieth cen - tury, but they have not become widely used in Western music. However, the idea of dividing the octave into equal parts has become fundamental. It means that the notes used are naturally modeled by integers.
Ifone regards two notes an octave apart as “the same,” which makes good musical sense, then one is dividing all notes into twelve The natural model for these is arithmetic modulo 12.equivalence classes [I.2 §2.3](/part - 01/language - and - grammar). As we shall see later, the symmetries of the group ofintegers mod 12 are of great musical significance. 3 Mathematics and Music Composition The association of number and music in acoustics wasthe result of scientific discovery. Number and music have also been associated through invention and creativity in music composition.
Fundamental aspects ofthe temporal organization of music reflect simple proportional relationships. The basic durational values in Western music notation are the whole note ( ), half note ( ), quarter note ( ), eighth note ( ), etc. These are related to each other by simple multiples or fractions—all powers of 2—and these relationships are reflected in the metric organization of musical time into bars with the same number of beats.
Bars or measures are indicated by time signatures such as the3, or 4 ( ), where beats (the in these examples) are simple meters 24, typically subdivided into two, or the46, 9 , or4 12 , in which beats (the . in these examples)compound meters are subdivided into three.8 8 8 counterpoint, is for a melodic theme, orreappear at half or twice the original speed, tech - A common device in musical composition, especially subject, to niques known astion, respectively. Figures 2 and 3 show the subjects rhythmic augmentation or dim inu of two fugues from the second volume of J. S.
Bach’s Well-Tempered Clavierject appears in diminution; and no. 2 in C minor, whose: no. 9 in E major, whose sub subject appears in augmentation. (The last note of the diminished or augmented subject may not be proportionally related to the original in order to allow a good continuation for the music that follows.) of other kinds too. A well-known construct in music theory is the Geometric relations have served as musical resources circle of fifths, which was originally designed to demonstrate the relationships between dif-ferent major and minor keys. As illustrated in figure 4, VII.
The Influence of Mathematics Figure 2 Fugue no. 9, subject and diminution. J. S. Bach, Well-Tempered Clavier, Book 2, Figure 3 Fugue no. 2, subject and augmentation. J. S. Bach, Well-Tempered Clavier, Book 2, the twelve notes are arranged around the circle as a suc-cession of perfect fifths. Any seven consecutive notes in this circle will be the notes of some major scale, which makes it easy to understand some of the patterns of the key signatures. For instance, the C major scale consists of all the notes from F to B (clockwise).
To change from C major to G major, one shifts the sequence by one, losing the note F but gaining F$*$. Continuing in this$way$, we see that C major is the key with no sharps or flats, G major has one sharp, D major has two sharps, A major has three sharps, etc. Similarly, moving coun-ter clockwise from C, F major has one flat, B+ major has two flats, E+major has three flats, etc.
From a mathematical point of view, we have transformed the chro-matic scale, which we identify with the additive group of integers mod 12, using the automorphism$x \to 7x$, and this makes some musical phenomena much more transparent. Reflective symmetry is another geometrical concept with a long history in musical composition. musicians will frequently describe melodic lines in spatial terms, referring to notes of higher frequencies as “up,” and notes of lower frequencies as “down.” This allows one to think of melodic lines as ascending or descending.
Reflection in a horizontal axis interchanges upand down. The musical counterpart to this is known as descending direction of each interval, and the result melodic in version: one reverses the ascending or is an inverted form of the melody. Figure 5 shows the subject of Fugue no. 23 in B major from the first volume of Bach’sof the subject in inverted form. A geometrical reflection Well-Tempered Clavier and a later appearance is clearly visible in the notation, but, more importantly,

VII.13. Mathematics and Music

FCGBDEAAEDF /G B

Figure 4 The circle of fifths.

tr

Figure 5 Fugue no. 23, subject and in version. J. S. Bach, Well-Tempered Clavier, Book 1, the in version can also be clearly heard in the sound of the music itself. Conventional Western musical notation implies a two-dimensional organization: the vertical dimension expresses the relative frequency of pitches from low to high, and the horizontal dimension expresses chrono-logical time from left to right. Another compositional device, much rarer than the devices of rhythmic augmentation and diminution or melodic in version, is thatof retrograde, where a melody is played backwards.
When the melody is played backwards and forwards simultaneously, the technique is known as a cancrizans canon. Perhaps the best-known examples of cancrizans occur in the music of J. S. Bach, such as in the first canon ofof the The Musical Offering Goldberg variations or the first and second canons. Figure 6 shows the opening and closing measures of the cancrizans from Bach’s Musical Offering.
The melody of the first few bars of the upper staff returns in reverse order at the end of the piece in the lower staff, and likewise, the melody of the first few bars of the lower staff returns in reverse order at the end of the upper staff. Joseph Haydn’sal rovescio, from the Sonata no. 4 for violin and piano, Menuetto is another well-known example of a similar technique,

939

in which the first half of the piece is played backwardsin the second half. We may regard the devices of melodic retrograde and in version as reflections in a two-dimensional musi-cal space. However, retrograde is much more esoteric, owing to the greater constraints involved in the manip-ulation of musical time. Examples such as those by Bach and Haydn mentioned above demonstrate great inge-nuity on the part of the composer, who must make the melodic retrogrades work convincingly with the under-lying harmonic progressions.
Certain common chord progressions, such as moving from the supertonic tothe dominant, do not work well in reverse, so a composer attempting to write a cancrizans canon is forced to avoid them. Similarly, many common melodic pat-terns do not sound good when reversed. These difficulties account for the rarity of retrograde techniques in tonal music (i.e., music based on major and minor keys). With the abandonment of tonality in the early twentieth century, the main constraints were removed, mak-ing composition with retrograde easier.
For instance, retrograde and in version played an important role in serial music, as we shall see. However, composers of such music replaced the traditional constraints of tonal music with others, such as avoiding major or minor tri-ads and bringing out other intervals deemed important for a particular piece. The atonal revolution in the early twentieth century, during which composers experimented with novel methods of harmonic organization, led to the explo-ration of new types of symmetry relations in music composition.
Scales based on repeating interval pat-terns (measured in semitones), such as the whole-tone scale (2–2–2–2–2–2) or the octatonic scale (1–2–1–2–12–1–2), appealed to composers for the symmetric struc-tures and novel harmonies they embodied. The octatonic scale, also known in jazz circles as the scale, had a particularly wide appeal among a variety diminished of composers of different nationalities, such as Igor Stravinsky, Olivier Messiaen, and Béla Bartók. The nov-elty of the whole-tone and octatonic scales is that they have nontrivial shared by the major or minor scales.
The whole-tone translational symmetry, a property not scale is unchanged if it is transposed by a tone, and the octatonic scale is unchanged if it is transposed bya minor third. There are thus only two distinct translates of the whole-tone scale and three of the octa-tonic scale. For this reason, neither scale has a clearly defined tonal center, which was a major reason for their attractiveness to early twentieth-century composers.

940

Figure 6 J. S. Bach, The Musical Offering, opening and closing measures of the cancrizans (canon 1). Reflective symmetry was used by twentieth-century composers as well, to help them with the formal aspects of compositional design. A fascinating example is the first movement of Bartók’ssion, and Celesta (1936), which extends the traditional Music for Strings, Pe rc us principles of the baroque fugue and incorporates asymmetric design. Figure 7 illustrates the structure of the fugue subject entries, starting from the initial entry on A.
In a traditional fugue, the subject is stated in tonic, followed by a statement in the dominant, and then again in the tonic (and continuing the alternat-ing pattern of tonic and dominant entries for fugues with more than three voices). In Bartók’s fugue, the first statement of the subject begins with the note A, and the next with E. Instead of returning to A for the third statement, however, the subsequent entries follow a pattern of alternating fifths in opposite directions from A: that is, the sequence A–E–B–F$*$, etc., alternates with the sequence A–D–G–C, etc.
This pattern is illustrated in figure 7. Each of the interlocked cycles completes a cir-cle of fifths, one clockwise, the other counter clockwise. Each letter in the illustration represents a statement of the fugue subject beginning on that note, and each ofthe interlocked cycles of fifths arrives on E+ (six semi- tones from the starting point, A) at its midpoint, so that all twelve notes occur once in the first half of the pattern and once again in the second half.
The midpointof the pattern corresponds to the dramatic climax of the work, after which the pattern of interlocked cycles of fifths resumes with subject entries in inverted form until the conclusion of the work with the return of the subject starting on A. sition, which he revealed in the early 1920 s, is based on permutations of all the twelve notes, rather than of Arnold Schoenberg’s twelve-tone method of composubsets of seven notes as one has in music in major or minor keys. In twelve-tone music (and atonal music in

VII. The Influence of Mathematics

Ebfc ae bf cg da a dg be cf cf be a

Figure 7 for Strings, Percussion, and Celesta Plan of fugal entries in Béla Bartók’s, first movement (after Music Morris (1994, p. 61), with permission). general), the twelve notes are supposed to have equal prominence: in particular, there is no single note with a special status like that of the tonic in a major or minor key. The basic ingredient of a piece of twelvetone music is aby some permutation of the twelve notes of the chro-tone row, which is a sequence given matic scale.
(These notes can, however, be stated inany octave.) Once the tone row has been chosen, it can be manipulated by means of four types of trans for-mation: transposition, in version, retrograde, and retrograde in version. Musical transposition corresponds tothe mathematical operation of translation:
the intervals between successive notes of a transposed row are the same as those between the corresponding notes of the original row, so the entire row is shifted up or down.6 In version corresponds to reflection, as we have already that a melody sounds “the same” when transposed, even though the pitches are different, because the successive intervals are the same. If6. Describing transposition as translation does justice to the fact one arranges the twelve notes in a circle, then one can also think ofthis translation as a rotation.

VII.13. Mathematics and Music

P4

I4

P10

I10

Figure 8 Row forms in Schoenberg’s

discussed: the intervals of the row are reflected abouta “horizontal” axis. Retrograde corresponds to reflection in time: the row is stated backwards. (However, ifit is combined with a transposition, as it may be, then it is better described as a glide reflection.) retrograde in version is a composition of two reflections, one vertical and one horizontal: it therefore corresponds to ahalf turn. to a row created by Schoenberg for his opus 25, published in 1923.
The forms of the row are Figure 8 illustrates the serial transformations applied Suite for Piano, labeled P (for prime—the original row and its transpo-sit i ons), R (for retrograde), I (for in version), and RI (for retrograde in version). The integers 4 and 10 in the row labels on the left and right refer to the starting notes of the P and I row forms by telling us how many semitones away from C they are. Thus, 4 refers to E (4 semitones above C) and 10 refers to B+ (10 semitones above C). The retrogrades of the P and I forms, R and RI, are labeled on the right-hand side of the figure.
It is easy to see the in version (reflection) of P4 in I4 about the first note E and the transposition of P4 by six semitones in P10, aswell as the in version of P10 about the first note, B+. understanding these abstract relationships and why they were so attractive to composers like Schoenberg. One may wonder what sort of insight we gain from In Schoenberg’sfig ure 8 are in fact the only ones used in all five Suite, the eight row forms shown in movements of the composition. This represents a high degree of selectivity, since there are 48 (= 12 . imes 4) avail-able row forms.
However, this self-imposed restriction is not on its own enough to account for the interestor attraction of this music. An additional aspect of the 941 R4 RI4 R10 RI10 Suite for Piano (1923).

technique is that the row itself, and the way its trans for-ma tions unfold in the course of a work, are chosen carefully to bring out certain relationships between notes. For example, all the row forms used in the Suite begin and end on the notes E and B+, and these notes are frequently articulated in the work so that they take on an anchoring function that fills the void created by the absence of a conventional tonal center.
Similarly, the notes in the third and fourth positions in each of the four row forms are always G and D$+$, in either order, and likewise these are articulated in various ways inthe movements of the Suite so that they can become recognizable. The two pairs of notes just mentioned, E–B$+$ and G–D+, are related to each other by sharing the same interval, six semitones (half an octave, also known as the tritone because it spans three whole tones).
In the hands of a master composer, a twelve-tone row isnot a random collection of notes, but a foundation for an extended composition carefully constructed to pro-duce interesting structural effects that one can learn to recognize and appreciate. musical parameters be sides pitch—such as rhythm, tempo, dynamics, and articulation—were explored by Permutations and serial transformations of other a new generation of postwar European composers, including Olivier Messiaen, Pierre Boulez, and Karlheinz Stockhausen.
Compared with the serialization of pitch, however, serialization of these parameters does not lend itself to such precise transformations, because it is less easy to organize them into discrete units than it is the twelve notes of musical space. It is important to recognize that Schoenberg and most composers whose music exhibits mathematical

942

conceptions such as those we have seen had little if any mathematical training.7 Nevertheless, the basic mathematical patterns and relations that we have discussed are so pervasive in so many aspects of so many different kinds of music that the importance of mathematics in music is undeniable. portion al relations such as the simple ones between note values reappear on a larger scale in relations We end this section with a few more examples. Pro between lengths of formal divisions in music of Mozart, Haydn, and others:
they often use basic building blocks of four-measure phrases and use them in pairs, and pairs of pairs, to form larger units. The techniques of melodic manipulation seen in Bach’s works, which are found in a new guise in Schoenberg’s twelve-tone techniques, can also be found in contrapuntal works of composers before Bach, such as Palestrina. And some composers, including Bach, Mozart, Beethoven, Debussy, Berg, and others, are said to have incorporated numerological elements into their composition, such as symbolic numbers or proportions based on Fibonacci sequences and the golden ratio.
4 Mathematics and Music Theory In the second half of the twentieth century, the ideas of Schoenberg and his followers were extended and developed in North American music theory. Milton Bab-bitt, a renowned American composer and theorist, is widely credited with introducing formal mathematics, specifically group theory, to the theoretical study of music.
He generalized Schoenberg’s twelve-tone system to any system where one has a finite set of basic musical elements (of which Schoenberg’s twelve-tone rows were just one example), with relations and transformations between them (see Babbitt 1960, 1992). There are forty-eight ways of transforming a row, and Babbitt noted that these transformations form a group, which is in fact the product of the dihedral group$D$ with the cyclic group this product is the symmetry group of a dodecagon, C2 of two elements.
(The D1212 in and theof transformations—P, I, R, and RI (see the previous C2 allows the time reversal.) The four sets section)—define a homomorphism from this group to cal training, which is reflected in their works. Iannis Xenakis, for exam-ple, was trained as an engineer, and had professional contact with7. Some composers, to be sure, have received extended mathemat i the architect Le Corbusier.
Xenakis found parallels between music and architecture through his study of Le Corbusier’s Modulor system and its approach to form and proportion based on the human figure. Xenakis’s compositions are characterized by their massive, physical sound and their complex algorithmic processes.

VII. The Influence of Mathematics

B 11 C0 1 C  = D

A  = B 10 2 D

A 9 3 D  = E

8 4

E

G  = A

G 7 F  = G6 5 F

Figure 9 Circular model of the twelve notes (pitch classes). the Klein group$C^{2} \times C^{2}$, by identifying transformations that are equivalent up to rotation. Identifying musical notes with the elements of the group musical operations by means of transformations on Z12 of integers mod 12, and modeling various this group, makes it much easier to analyze some kinds of music, such as the atonal music of Schoenberg, Berg, and Webern, that do not lend themselves easily to more traditional analysis of harmony (see Forte 1973; Morris1987; Straus 2005).
This identification is illustrated in figure 9. As we have already commented, multiplying by5 or 7 is an automorphism of Z , and gives the cycle of fifths shown in figure 4 (when one substitutes the mod-12 integers for the names of the notes). This mathemat-12 ical fact has many musical consequences.
One of themis that it is common to substitute fifths by semitones, and vice versa, in chromatic harmony and in jazz. A branch of music theory known as atonal set theory attempts to give a very general understanding of pitch relations by looking at all the 212= 4096 possible combinations of notes, and defining two such combinations to be equivalent if one can be derived from the other by two simple transformations, the idea being that equivalent combinations will have the same inter-vals. The transformations in question are transposition and in version.
A transposition up byn semitones (where we think of$T^{n}$. The notation Iis used for a reflection about the noten as an integer mod 12) is denoted C, so a general in version takes the form(In version in this context refers to reflection in musical Tn I for some n. space, and should not be confused with chord in version in tonal music.) In these terms, to use a familiar example, the major triad and the minor triad are related

VII.13. Mathematics and Music

to each other by in version since their successive inter-vals are reflections of each other (four then three semitones in the major triad and three then four semitones in the minor triad, counting from the lowest note). Con - sequently, all major and minor triads belong to the same equivalence class.
For example, the E-major triad\\\{4, 8, 11\\\} is related to the C-major triad \\\{0, 4,7\\\} by the transposition mod 12), and the G-minor triad T4 (because {4,8,11{7} ≡ \\\{,10,02+\\\}4 is related by}$, 4 + 4$,7 + 4, in version to the D-major triad$\\{7}$,10,2\\ ≡ {4 - 9, 4 - 6, 4 - 2{}2, mod 12)}. An equiva-, 6, 9} by T4 I (because lence class, such as the class of major and minor triads, will normally consist of twenty-four sets.
However, if it has internal symmetries, such as those of the dimin-ished seventh chord (with interval succession 3–3–3–3) or the whole-tone and octatonic scales mentioned ear-lier, then the number of sets in the class will be smaller, though it will always be a factor of 24. tain sonic attributes because they share the same num-ber and types of intervals.
But while it seems reason-Sets of notes in the same equivalence class share cer able enough to regard transposed chords as equivalent, since they really do have an obvious “sameness” in the way they sound, there has been some controversy over the notion of inversional equivalence. For example, is it reasonable to declare major and minor triads to be equivalent to each other when they clearly do not sound the same and have very different musical roles? Of course, we are free to define any equivalence relationwe like, so the real question is whether this one has any utility.
And in some contexts it does: with sets of notes that do not possess extensive associations with tonal music it is easier to recognize this form of equivalence than it is with major and minor triads. For example, the three notes C, F, and B share the same intervals (one semitone, one perfect fourth or fifth, and one tri-tone) as the three notes F$*$, G, and C*, and this does indeed give them a noticeable form of “sameness.” (Theset\\{11, 0, 5\\} is inversionally related to \\{1, 6, 7\\} by T I because$\\{11}$,0,5\\\\\\\\\\\\\\\\\\\\\} ≡ \\\\\\\\\\\\\\\\\\\\\{6 - 7,6 - 6, 6 -
1\\\\\\\\\\\\\\\\\\\\\}$, mod 12$.) has been inspired by group theory. The most influ-ential example is David Lewin’s There is other important work in music theory that Generalized Musical Intervals and transformations a formal theory that connects mathematical reasoning(1987), which develops and musical intuition. Lewin generalizes the concept of interval to mean any measurable distance, whether between pairs of pitches, durations, time points, or contextually defined events in a musical work. He develops a model called the generalized interval system (GIS),

943

which consists of a set of musical objects (e.g., pitches, rhythmic durations, time spans, or time points), a group (in the mathematical sense) of intervals (representing the distance, span, or motion between a pairof objects in the system), and a function that maps all possible pairs of objects in the system into the group of intervals. He also uses musical processes, through his notion of agraph theory [III.34] to model transformation network basic musical elements such as melodic lines or chordal. The vertices of such a network are roots.
These elements come with certain transforma-tions, such as transposition (or shifting by a generalized interval) or the serial transformations from twelvetone theory. Two vertices are joined by an edge if there is an allowable transformation that takes one to the other. The emphasis thus shifts from the basic ele-ments to the relations that connect them.
Transformation networks offer a dynamic way of looking at musical processes, giving visible form to abstract and often non chronological connections in the analysis of musical works. The level of generalization and abstraction makes Lewin’s treatise a challenge for the mathematically unsophisticated music theorist, but it does not need more than fairly simple undergraduate-level algebra, soit is accessible enough for the determined reader with some mathematical training.
It becomes clear to such a reader that the formality of the presentation is essential to a proper understanding of the transformation al approach to music theory and analysis. Despite this for-mality, Lewin continually maintains contact with music itself, and how his mathematical tools can be applied in different contexts. The result is that the reader is rewarded with insights that would be impossible with out the mathematical rigor.
Mathematicians, while likely to find the material relatively elementary, may find their attention “captivated by the way in which the author gives new and, some times, unexpected inter pre-tat i ons to classical mathematical ideas when applied to musical contexts” (Vuza 1988, p. 285). 5 Conclusion The playful Leibniz quotation with which this essay began underscores an enduring mathematical presence in music. Both disciplines rely in a fundamental way onconcepts of order and reason, as well as more dynamic concepts of pattern and transformation.
Music was once subsumed within mathematics, but it has now acquired its own identity as an art that has always