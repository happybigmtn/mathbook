# Quantum Groups

$272$

[IV.20](/part-04/computational-complexity) problems? The majority opinion is that they cannot, and indeed the statement that they cannot is becoming another of the many ‚Äúplausible hypotheses‚Äù of complexity theory, but it would be good to have stronger reasons for believing in this statement, such as a proof subject to already-known plausible hypotheses in classical computation. III . $75$ Quantum Groups

Shahn Majid

There are at least three different paths that lead to the objects known today as quantum groups. They could be summarized briefly as quantum geometry, quantum symmetry, and self-duality. Any one of them would be a great reason to invent quantum groups and each of them had a role in the development of the modern theory.
$1$ Quantum Geometry One of the great discoveries in physics in the last century was that classical mechanics should be replaced by quantum mechanics, in which the space of possible positions and moment a of a particle is replaced by the formulation of position and momentum as mutually noncommuting operators. This non commutativity under lies Heisenberg‚Äôs ‚Äúuncertainty principle,‚Äù but it also suggests the need for a more general notion of geometry in which coordinates need not commute. One approach to noncommutative geometry is discussed in operator algebras [IV.15](/part-04/operator-algebras).
 However, another approach is to note that geometry really grew out of examples such as spheres, tori, and so forth, which are lie groups [III.48](/part-03/lie-theory) or objects closely related to Lie groups. If one wants to ‚Äúquantize‚Äù geometry, one should first think about how to generalize basic examples like this: in other words, one should try to define ‚Äúquantum Lie groups‚Äù and associated ‚Äúquantum‚Äù homogeneous spaces. The first step is to consider geometrical structures not so much in terms of their points but in terms of corresponding algebras.
For example, the group S$L^{2}$ (C) Œ±Œ≤ is defined as the set of $2 \times 2$ matrices (Œ≥ Œ¥) of complex numbers such that $Œ±Œ¥ - Œ≤Œ≥ = 1$ . We can think of this as a subset of $C^{4}$ , and indeed not just a subset but a variety [III.95](/part-03/varieties). The natural class of functions associated with this variety is the set of polynomials in four variables (which are defined on $C^{4}$ ) restricted to the variety. However, if two polynomials take equal values on the III. Mathematical Concepts variety, then we identify them.
In other words, we take the algebra of polynomials in four variables a, b, c, and d and quotient [I.3](/part-01/fundamental-definitions) by the ideal [III.81](/part-03/rings-ideals-and-modules) generated by the polynomial $ad - bc - 1$ . (This construction is discussed in detail in arithmetic geometry [IV.5](/part-04/arithmetic-geometry).) Let us call the resulting algebra C [SL 2 ]. We can do the same for any subset X ‚äÇ C n that is defined by polynomial relations.
This gives us a precise one-to-one correspondence between subsets of this type and certain commutative algebras equipped with n generators. Let us write C [X] for the algebra that corresponds to X. As with many similar constructions (see , for example, the discussion of adjoint maps in duality [III.19](/part-03/duality)), a suitable map from X to Y gives rise to a map from C [Y] to C [X] .
More precisely, the map œÜ from X to Y has to be polynomial (in a suitable sense) and the resulting map from C [Y] to C [X] is an algebra homomorphism œÜ ‚àó that satisfies the formula œÜ ‚àó (p) (x) = p (œÜ x) for every x $\in$ X and p $\in$ C [Y]. Going back to our example, the set S$L^{2}$ (C) has a group structure  SL2(C). imes  SL2(C) \to  SL2(C) defined by the matrix product. The set $SL^{2}(C) \t\text{imes SL}^{2}(C)$ is a variety in $C^{8}$ and the matrix product depends in a polynomial way on the entries in the matrices, so we obtain an algebra homomorphism $\Delta$ :
$C[SL^{2} ]$ \to C$[SL 2 ]$ ‚äó C [SL 2 ], which is known as the coproduct. (The algebra C [SL 2] ‚äó C [SL 2] is isomorphic to C [SL 2 . imes SL 2 ] .) It turns out that $\Delta$ can be expressed by the formula a b a b a b $\Delta$ = ‚äó . c d c d c d This formula needs a word or two of explanation: the variables a, b, c, and d are the four generators of the algebra of polynomials in four variables (and hence of its quotient by $ad - bc - 1)$ , and the right-hand side is a shorthand way of saying that $\delta\text{a} = a$ ‚äó $a + b$ ‚äó  c, and so on.
Thus, $\Delta$ is defined on the generators by a sort of mixture of tensor products [III.89](/part-03/tensor-products) and matrix multiplication. One can then show that the associativity of matrix multiplication in S$L^{2}$ is equivalent to the assertion that $(\Delta$ ‚äó $id)\Delta = (id$ ‚äó $\Delta)\Delta$ . To understand what these expressions mean, bear in mind that $\Delta$ takes elements of C [SL 2] to elements of C [SL 2] ‚äó C [SL 2]. Thus, when we apply the map ( $\Delta$ ‚äó id) $\Delta$ , for example, we begin by applying $\Delta,$ and there by creating an element of C [SL 2] ‚äó C [SL 2].
This element will be a linear combination of elements of the form p ‚äó q, each of which will then be replaced by $\Delta p$ ‚äó q.

III . $75$ .

Quantum Groups

Similarly, one can express the rest of the group structure of S$L^{2}$ (C) equivalently in terms of the algebra C [SL 2 ]. There is a counit map : C [SL 2 ] \to k, which corresponds to the group identity, and an antipode map $S$ : $C[SL^{2} ]$ \to  C$[SL 2 ],$ which corresponds to the group in version. The group axioms appear as equivalent properties of these maps, making C [SL 2] into a ‚ÄúHopf algebra‚Äù or ‚Äúquantum group.‚Äù The formal definition is as follows. Definition. A Hopf algebra over a field k is a quadruple (H , $\Delta,$  , S), where (i) H is a unital algebra over k; ( $ii) \Delta$ :
$H \to H$ ‚äó  H,   : $H \to k$ are algebra homomorphisms such that $(\Delta$ ‚äó $id)\Delta = (id$ ‚äó $\Delta)\Delta$ and (‚äó $id)\Delta = (id$ ‚äó   $)\Delta = id;$ (iii) S : H $\to$ H is a linear map such that m (id ‚äó S) $\Delta = m(S$ ‚äó $id)\Delta = 1$   , where m is the product operation on H. There are two great things about this formulation. The first is that the notion of a Hopf algebra makes sense over any field. The second is that no where did we demand that H was commutative.
Of course, if H is derived from a group, then it certainly is commutative (since multiplying two polynomials is commutative), so if we can find a noncommutative Hopf algebra, then we have obtained a strict generalization of the notion of a group. The great discovery of the past two decades is that there are indeed many natural noncommutative examples.
For example, the quantum group C q [SL 2] is defined as the free associative noncommutative algebra on symbols a, b, c, and d modulo the relations $ba =$ qab, $bc = cb, ca =$ qac, $dc =$ qcd, $db =$ qbd, $da = ad + (q - q - 1)bc$ , $ad - q - 1bc = 1$ . This forms a Hopf algebra with $\Delta$ given by the same formula as it is for C [SL 2] and with suitable maps   and S. Here q is a nonzero element of C , and as q $\to 1$ one obtains C [SL 2]. This example generalizes to canonical examples C q [G] for all complex simple Lie groups G.
Much of group theory and Lie group theory can be generalized to quantum groups. For example, Haar integration is a linear map : H $\to$ k that is translation invariant in a certain sense that involves $\Delta.$ If it exists, it is unique up to a scalar multiple, and it does indeed exist in most cases of interest, including all finite dimensional Hopf algebras. Likewise, the notion of a complex of differential forms [III.16](/part-03/dierential-forms-and-integration) ( $\Omega$ , d) makes

$273$

sense over any algebra H as a proxy for a differential structure. Here, $\Omega = n\Omega^{n}$ is required to be an associative algebra generated by $\Omega^{0} = H$ and $\Omega^{1}$ , but one does not assume that it is graded-commutative as in the classical case. When H is a Hopf algebra one can ask that $\Omega$ is translation invariant, again in a certain sense that involves the coproduct $\Delta.$ In this case both $\Omega$ and its cohomology [IV.6](/part-04/algebraic-topology) as a complex are super (or graded) quantum groups.
The axioms of a (graded) Hopf algebra were originally introduced by Heinz Hopf in 1947 precisely to express the structure of the cohomology ring of a group, so this result brings us back full circle to the origins of the subject. For most quantum groups, including all the C q [G] , one has a natural minimal complex ( $\Omega$ , d). Thus, a ‚Äúquantum group‚Äù is not merely a Hopf algebra but has additional structure analogous to that of a Lie group. There are many other quantum groups that are not related to q-deformations. There are also applications of the theory to finite groups.
If G is a finite group, one has a corresponding algebra k (G) of all functions on G with pointwise product and a coproduct ( $\Delta$ f) (g , h) = f (gh) for f $\in$ k (G) and g, h $\in$ G. Here we identify k (G) ‚äó k (G) and k (G $\times$ G), which makes $\Delta f$ into a function of two variables, and one may check even more simply that this is a Hopf algebra.
There can never be an interesting classical differential structure on a finite set, but if we use the methods developed for quantum groups, then we have one or more translation-invariant complexes $(\Omega^{1}$ ,  d) on any finite group. Applying further parts of the theory of quantum group differential geometry, one finds, for example, that the alternating group $A^{4}$ is naturally Ricci-flat, while the symmetric group $S^{3}$ naturally has constant curvature [III.13](/part-03/curvature), much like $a_{3}$ - sphere .
$2$ Quantum Symmetry Symmetry in mathematics is usually expressed as the action of a group or Lie algebra of finite or infinitesimal transformations of some structure. If you have a collection of transformations that is closed under in version and composition, then you necessarily have an ordinary group. So how might one generalize this? The answer is that one begins by observing that a group G can act on several objects at the same time. If a group acts on two objects X and Y , then it also acts on their direct product X $\times$ Y , with g (x , y) = (gx , gy).
Here we are making implicit use of a diagonal or ‚Äúduplication‚Äù map $\Delta$ : $G \to G \times G,$ which duplicates a group

274

element so that one copy can act on the first object
and the other on the second object. In order to gener-
alize this it once again pays to replace the notion of
a group G by that of an algebra. This time we use the
group algebra k G, which is the set of all formal linear

combinations i . ambda i gi , where the gi are elements of G
and the . ambda i are scalars from the field k. The elements
of G (considered as particularly simple linear combina-
tions of this kind) form a basis of k G and we multiply
them as we would in G itself. One then extends this
definition to products of more general linear combi-
nations in the obvious way. We also extend Œî linearly
from Œîg = g ‚äó g on the basis elements to a map from
k G to k G ‚äó k G. Together with some associated maps
 and S, this makes k G into a Hopf algebra. Note that
this is a completely different use of the coproduct from
the one in the previous section, since the group prod-
uct has already gone into the algebra. One has a similar
story for the ‚Äúenveloping algebra‚Äù U(g) associated with
any Lie algebra g; this is generated by a basis of g with
certain relations and becomes a Hopf algebra with the
coproduct ŒîŒæ = Œæ ‚äó 1 + 1 ‚äó Œæ ‚Äúsharing out‚Äù an element
Œæ ‚àà g for the purposes of acting on a tensor product of
objects on which g acts.
   Extrapolating from these two examples, a general
‚Äúquantum symmetry‚Äù means an algebra H equipped
with further structure Œî that allows one to form a ten-
sor product V ‚äó W of any two representations V , W of
the algebra in an associative manner. An element h ‚àà H
acts as h(v ‚äó w) = (Œîh)(v ‚äó w), where one part of Œîh
acts on v ‚àà V and another part on w ‚àà W . This is
a second route to the Hopf algebra axioms we gave in
the previous section.
   Note that, in the examples just given, Œî has had a
symmetric output. As a consequence, if V and W are
representations of a group or Lie algebra, then V ‚äó W
and W ‚äó V are isomorphic via the obvious map that
takes v ‚äó w to w ‚äó v. In general, however, V ‚äó W and
W ‚äóV may be unrelated, so it is now the tensor product
that is being made noncommutative. In nice examples it
may be the case that V ‚äóW  W ‚äóV , but not necessarily
by the obvious map. Instead, there may be a nontriv-
ial isomorphism for every pair V , W , which may nev-
er the less obey some reasonable conditions. This hap-
pens for a large class of examples, denoted by Uq (g)

and associated with all complex simple Lie algebras.
For these examples, the isomorphism obeys the braid
or Yang‚ÄìBaxter relations among any three representa-
tions (see braid groups [III.4](/part-03/braid-groups)). As a result, these quan-   be Abelian, then k G  k(ƒú), where ƒú is the group of
tum groups lead to knot and 3-manifold invariants

III. Mathematical Concepts
[III.44](/part-03/knot-polynomials) (the Jones knot invariant comes from the exam-
ple Uq (sl2 ), where sl2 is the Lie algebra of the group
SL2 (C)). The parameter q can usefully be regarded here
as a formal variable, and these examples can be thought
of as some kind of deformation of the classical envelop-

ing algebras U(g). They arose originally in work of Drin-
feld and of Jimbo in the theory of quantum integrable
systems.
3   Self-duality
A third point of view is that Hopf algebras are the next
simplest category [III.8](/part-03/categories) after Abelian groups of struc-
tures that admit a fourier transform [III.27](/part-03/the-fourier-transform). It is not
immediately obvious, but the axioms (i)‚Äì(iii) in the def-
inition we gave earlier have a certain symmetry. One
can write out the requirement (i) of a unital algebra H
in terms of linear maps m : H ‚äó H ‚Üí k and Œ∑ : k ‚Üí H
(here Œ∑ specifies the identity element of H as the image
of 1 ‚àà k) that have to obey some straightforward com-
mutative diagrams. If you reverse all the arrows in
these diagrams, then you have the axioms displayed in
(ii), obtaining what could be called a ‚Äúcoalgebra.‚Äù The
requirement that the coalgebra structures Œî and  are
algebra maps is given by a collection of diagrams that
is invariant under arrow reversal. Finally, the axioms
in (iii), as commutative diagrams, are invariant under
arrow reversal in the above sense.
Thus, the axioms of a Hopf algebra have the spe-
cial property of being symmetric under arrow rever-
sal. A practical consequence is that if H is a finite-
dimensional Hopf algebra, then so is H ‚àó , with all
structure maps defined as the adjoints of those of
H (which necessarily reverses arrows). In the infinite-
dimensional case one needs a suitable topological dual,
or one can just speak of two Hopf algebras as dually
paired to each other. For instance, Cq [SL2 ] and Uq (sl2 )
above are dually paired, while if G is finite then (k G)‚àó =
k(G), the Hopf algebra of functions on G.
As an application, let H be finite dimensional with
basis {ea }, let H ‚àó have a dual basis {f a }, and let
denote a right-translation-invariant integral on H. The
Fourier transform F : H ‚Üí H ‚àó is defined as

F (h) =
a
and has many remarkable properties. A special case is
a Fourier transform F : k(G) ‚Üí k G for any finite group
G, which does not have to be Abelian. If G happens to
characters, and we recover the usual Fourier transform

III.76.   Quaternions, Octonions, and Normed Division Algebras

?

    Figure 1 Putting quantum groups in context. Self-dual

for finite Abelian groups. The point is that in the non-
Abelian case, k G is not commutative and hence not the
algebra of functions on any usual ‚ÄúFourier dual‚Äù space.
  This point of view is responsible for the second main
class of genuine quantum groups to have been dis-
covered, namely the ‚Äúbicrossproduct‚Äù ones of self-dual
form. They are simultaneously ‚Äúcoordinate‚Äù and ‚Äúsym-
metry‚Äù algebras, and are truly connected with quantum
mechanics. An example, which is written

is the so-called Poincar√© quantum group of a certain
noncommutative spacetime with coordinates x, y, z, t,
where t does not commute with the other variables.
This quantum group can also be interpreted as the
quantization of a particle moving in a curved geom-
etry with black-hole-like features. In essence, the self-
duality of quantum groups provides a paradigm for
‚Äútoy models‚Äù of the unification of gravity (as spacetime
geometry) and quantum theory.
   This is part of a wider picture indicated in figure 1.
A category of objects with a coherent notion of ‚Äútensor
product‚Äù is called a monoidal (or tensor ) category, and
we have seen that this is the case for representations
of quantum groups. There, one also has a ‚Äúforgetful
functor‚Äù to the category of vector spaces, which for-
gets the quantum group action. This embeds quantum
groups into the next most general self-dual category (in
a representation-theoretic sense), namely that of func-
tors between monoidal categories. Over on the right,
I have included Boolean algebras as primitive struc-

275
Quantales                           tures with (de Morgan) duality. However, the connec-
?
tion between duality here and the other dualities is
Quantum
field theory
Group                              Further Reading
duals
Majid, S. 2002. A Quantum Groups Primer. London Math-
ematical Society Lecture Notes, volume 292. Cambridge:
Monoidal
Cambridge University Press.
functors
III.76    Quaternions, Octonions, and
Groups
Riemannian
geometry
Uniform                            Mathematics took a leap forward in sophistication with
?                     the introduction of the complex numbers [I.3 ¬ß1.5](/part-01/fundamental-definitions). To
spaces
define these, one suspends one‚Äôs disbelief, introduces a
new number i, and declares that i2 = ‚àí1. A typical com-
categories are shown on the horizontal axis.
plex number is of the form a + ib, and the arithmetic
of complex numbers is easy to deduce from the nor-
mal rules of arithmetic for real numbers. For example,
to calculate the product of 1 + 2 i and 2 + i one simply
expands some brackets:
(1 + 2 i)(2 + i) = 2 + 5 i + 2 i2 = 5 i,
the last equality following from the fact that i2 = ‚àí1.
One of the great advantages of the complex numbers
is that, if complex roots are allowed, every polynomial
can be factorized into linear factors: this is the famous
fundamental theorem of algebra [V.13](/part-05/the-fundamental-theorem-of-algebra).
C[R3 > R]Œª  U (so(1, 3)),
Another way to define a complex number is to say
that it is a pair of real numbers. That is, instead of writ-
ing a + ib one writes simply (a, b). To add two complex
numbers is simple, and exactly what one does when
adding two vectors: (a, b)+ (c, d) = (a+c, b +d). How-
ever, it is less obvious how to multiply: the product of
(a, b) and (c, d) is (ac ‚àí bd, ad + bc), which seems
an odd definition unless one goes back to thinking of
(a, b) and (c, d) as a + ib and c + id.
Nevertheless, the second definition draws our atten-
tion to the fact that the complex numbers are formed
out of the two-dimensional vector space [I.3 ¬ß2.3](/part-01/fundamental-definitions) R2
with a carefully chosen definition of multiplication.
This immediately raises a question: could we do the
same for higher-dimensional spaces?
As it stands, this question is not wholly precise, since
we have not been clear about what ‚Äúthe same‚Äù means.
To make it precise, we must ask what properties this
multiplication should have. So let us return to R2 and
think about why it would be a bad idea to define the
product of (a, b) and (c, d) in a simple-minded way as

276

(ac, bd). Of course, part of the reason is that the prod-
uct of a + ib and c + id is not ac + ibd, but why should
we not also be interested in other ways of multiplying
vectors in R2 ?
   The trouble with this alternative definition is that
it allows zero divisors, that is, pairs of nonzero num-
bers that multiply together to give zero. For example,
it gives us (1, 0)(0, 1) = (0, 0). If we have zero divi-
sors, then we cannot have multiplicative inverses, since
if every nonzero number in a number system has a mul-
tiplicative inverse, and if xy = 0, then either x = 0 or
y = x ‚àí1 xy = x ‚àí1 0 = 0. And if we do not have multi-
plicative inverses, then we cannot define a useful notion
of division.
   Let us return then to the usual definition of the com-
plex numbers and try to think how we can go beyond it.
One way we might try to ‚Äúdo the same‚Äù as we did before
is to do to the complex numbers what we did to the real
numbers. That is, why not define a ‚Äúsuper-complex‚Äù
number to be an ordered pair (z, w) of complex num-
bers? Since we still want to have a vector space, we will
continue to define the sum of (z, w) and (u, v) to be
(z + u, w + v), but we need to think about the best way
of defining their product. An obvious guess is to use
precisely the expression that worked before, namely
(zu‚àíwv, zv +wu). But if we do that, then the product
of (1, i) and (1, ‚àíi) works out to be (1+i2 , i ‚àíi) = (0, 0),   tor space. (The letter ‚ÄúH‚Äù is in honor of William Rowan
so we have zero divisors.
   This example came from the following thought. The
modulus of a complex number z = a + ib, which mea-
sures the length of the vector (a, b), is the real number

|z| = a2 + b2 . This can also be written as . ar{z}z, where
. ar{z} is the complex conjugate a ‚àí ib of z. Now if a and
b are allowed to take complex values, then there is no
reason for a2 + b2 to be nonnegative, so we may not be
able to take its square root. More over, if a2 + b2 = 0
it does not follow that a = b = 0. The example above
came from taking a = 1 and b = i and multiplying the
number (1, i) by its ‚Äúconjugate‚Äù (1, ‚àíi).
   There is, nevertheless, a natural way to define the
modulus of a pair (z, w) that works even when z and w
are complex numbers. The number |z|2 + |w|2 is guar-
anteed to be nonnegative, so we can take its square
root. More over, if z = a + ib and w = c + id, then
we will obtain the number (a2 + b2 + c 2 + d2 )1/2 , which
is the length of the vector (a, b, c, d).
   This observation leads to another: the complex con-
jugate of a real number is the number itself, so, if we
want to ‚Äúuse the same formula‚Äù for the complex num-
bers as we used for the reals, we are free to introduce

III. Mathematical Concepts
complex conjugates into that formula. Before we try to
do that, let us think about what we might mean by the
‚Äúconjugate‚Äù of a pair (z, w). We expect (z, 0) to behave
like the complex number z, so its conjugate should be
(. ar{z}, 0). Similarly, if z and w are real, then the conjugate
of (z, w) should be (z, ‚àíw). This leaves us with two
reasonable possibilities for a general pair (z, w): either
(. ar{z}, ‚àí. ar{w}) or (. ar{z}, ‚àíw). Let us consider the second of these.
We would like the product of (z, w) and its conjugate,
which we are defining as (. ar{z}, ‚àíw), to be (|z|2 + |w|2 , 0).
We want to achieve this by introducing complex conju-
gates into the formula
(z, w)(u, v) = (zu ‚àí wv, zv + wu).
An obvious way of getting the result we want is to take
(z, w)(u, v) = (zu ‚àí . ar{w}v, . ar{z}v + wu),
and this modified formula, it turns out, defines an asso-
cia tive binary operation [I.2 ¬ß2.4](/part-01/language-and-grammar) on the set of pairs
(z, w). If you try the other definition of conjugate, you
will find that you end up with zero divisors. (A first
indication of trouble is that, under the other definition,
the pair (0, i) is its own conjugate.)
We have just defined the quaternions, a set H of
‚Äúnumbers‚Äù that form a four-dimensional real vector
space, or alternatively a two-dimensional complex vec-
Hamilton, their discoverer. See hamilton [VI.37](/part-06/william-rowan-hamilton-18051865) for
the story of how the discovery was made.) But why
should we have wished to do that? This question
becomes particularly pressing when we notice that the
‚àö
notion of multiplication that we have defined is not
commutative. For example, (0, 1)(i, 0) = (0, i), while
(i, 0)(0, 1) = (0, ‚àíi).
To answer it, let us take a step back and think about
the complex numbers again. The most obvious justifi-
cation for introducing those is that one can use them to
solve all polynomial equations, but that is by no means
the only justification. In particular, complex numbers
have an important geometrical interpretation, as rota-
tions and enlargements. This connection becomes par-
ticular ly clear if we choose yet another way of writing
‚àíb
the complex number a + ib, as the matrix ( a b a ). Multi-
plication by the complex number a + ib can be thought
of as a linear map [I.3 ¬ß4.2](/part-01/fundamental-definitions) on the plane R2 , and this
is the matrix of that linear map. For example, the com-
plex number i corresponds to the matrix ( 01 ‚àí10 ), which
is the matrix of a counter clockwise rotation through
1
2 œÄ about the origin, and this rotation is exactly what
multiplying by i does to the complex plane.

III . $76$ .

Quaternions, Octonions, and Normed Division Algebras If complex numbers can be thought of as linear maps from $R^{2}$ to $R^{2}$ , then quaternions should have an interpretation as linear maps from $C^{2}$ to $C^{2}$ . And indeed they do. Let us associate with the pair (z , w) the matrix z . ar{w}

$( - w$

. ar{z}). Now let us consider the product of two such matrices: $zu -$ . ar{w}v z. ar{v}  +  . ar{w} ≈´ z . ar{w} u . ar{v}

 = / $- w$ . ar{z}

$- v$ ≈´

$- z$ ÃÑ $v - wu$ . ar{z} ≈´ $- w$ . ar{v}

This product is precisely the matrix associated with the pair (zu  -  w . ar{v}, zv  +  w . ar{u}), which is the quaternionic product of (z , w) and (u , v) ! As an immediate corollary, we have a proof of a fact mentioned earlier: that quaternionic multiplication is associative. Why? Because matrix multiplication is associative. (And that is true because the composition of functions is associative: see [I.3](/part-01/fundamental-definitions).) Notice that the determinant [III.15](/part-03/determinants) of the matrix z . ar{w} $22$ .
ar{z}) is $|z| + |w|$ , so the modulus of the pair (z , w) $( - w$ (which is defined to be $|z|^{2} + |w|^{2})$ is just the determinant of the associated matrix. This proves that the modulus of the product of two quaternions is the product of their moduli (since the determinant of a product is the product of determinants). Notice also that the adjoint of the matrix (that is, the complex conjugate of the . ar{z} - . ar{w} transpose matrix) is (w z), which is the matrix associated with the conjugate pair (. ar{z}$, - w)$. Finally, notice that if $|z|^{2} + |w|^{2} = 1$ , then . ar{z} - .
ar{w} z . ar{w} 1 0$= ,$- w . ar{z} w z 0 1 which tells us that the matrix is unitary [III.50](/part-03/linear-operators-and-their-properties). Conversely, any unitary $2 \times 2$ matrix with determinant $1$ z . ar{w}. arz). There can easily be shown to have the form (  -  w fore, the unit quaternions (that is, the quaternions of modulus $1$ ) have a geometrical interpretation: they correspond to the ‚Äúrotations‚Äù of $C^{2}$ (that is, the unitary maps of determinant $1$ ), just as the unit complex numbers correspond to the rotations of $R^{2}$ .
The group of unitary transformations of $C^{2}$ of determinant $1$ is an important lie group [III.48](/part-03/lie-theory) called the special unitary group SU ( $2$ ). Another important Lie group is the group SO ( $3$ ), of rotations of $R^{3}$ . Surprisingly, the unit quaternions can be used to describe this group as well. To see this, it is convenient to present the quaternions in another, more conventional, way.
Quaternions, as they are usually introduced, are a system of numbers where we introduce not just one square root of $- 1$ but three, called i, j, and k (together with their negatives). Once one knows that $i^{2} = j^{2} = k^{2} = - 1$ , and also that $ij = k,jk = i,$ and $ki = j,$ one has $277$ all the information one needs to multiply two quaternions. For example, $ji =$ jjk $= - k$ . A typical quaternion takes the form $a + ib + jc + kd$ , which corresponds to the pair of complex numbers (a + ic, b + id) in our previous way of thinking about quaternions.
Now if we want, we can think of this quaternion as a pair (a , v), where a is a real number and v is the vector (b , c, d) in $R^{3}$ . The product of (a , v) and (b , w) then works out to be (ab - v ¬∑ w, aw + bv + v ‚àß w), where v ¬∑ $w$ and $v$ ‚àß w are the scalar and vector products of $v$ and $w$. If $q = (a$ ,  u) is a quaternion of modulus $1$ , then $a^{2} +$ u ${}^{2} = 1$ , so we can write q in the form (cos $\theta,v$ sin $\theta)$ with v a unit vector. This quaternion corresponds to a counter clockwise rotation R about an axis in direction v through an angle of $2\theta$ .
This angle is not what one might at first expect, and neither is the way the correspondence works. If w is another vector, we can represent it as the quaternion ( $0$ , w). We would now like a neat expression for the quaternion ( $0$ , Rw); it turns out that ( $0$ , Rw) = q ( $0$ , w) q ‚àó , where q ‚àó is the conjugate (cos $\theta, - v$ sin $\theta)$ of q, which is also its multiplicative inverse, as q has modulus $1$ . So to do the rotation R, you do not multiply by q but rather you conjugate by q.
(This is a different meaning of the word ‚Äúconjugate,‚Äù referring to multiplying on one side by q and on the other side by $q - 1$ .) Now if $q^{1}$ and $q^{2}$ are quaternions corresponding to rotations $R^{1}$ and $R^{2}$ , respectively, then $q^{2}q^{1}(0$ , $w)q^{1}$ ‚àó $q^{2}$ ‚àó $= q^{2}q^{1}(0$ , $w)(q^{2}q^{1})$ ‚àó , from which it follows that $q^{2}q^{1}$ corresponds to the rotation $R^{2}R^{1}$ . This tells us that quaternionic multiplication corresponds to composition of rotations. The unit quaternions form a group, as we have already seen--it is SU ( $2$ ).
It might appear that we have shown that SU ( $2$ ) is the same as the group SO ( $3$ ) of rotations of $R^{3}$ . However, we have not quite done this, because for each rotation of $R^{3}$ there are two unit quaternions that give rise to it. The reason is simple: a counter clockwise rotation through $\theta$ about a vector v is the same as a counter clockwise rotation through $- \theta$ about $- v$ . In other words, if $q$ is a unit quaternion, then q and - q give rise to the same rotation of $R^{3}$ . So SU ( $2$ ) is not isomorphic to SO ( $3$ ); rather, it is a double cover of SO ( $3$ ).
This fact has important ramifications in mathematics and physics. In particular, it lies behind the notion of the ‚Äúspin‚Äù of an elementary particle. Let us return to the question we were considering earlier: for which n is there a good way of multiplying vectors in R n ? We now know that we can do it for n $= 1$ ,

278

2, or 4. When n = 4 we had to sacrifice commutativity,
but we were amply rewarded for this, since quaternion
multiplication gives a very concise way of representing
the important groups SU(2) and SO(3). These groups
are not commutative, so it was essential to our suc-
cess that quaternion multiplication should also not be
commutative.
   One obvious thing we can do is continue the process
that led to the quaternions. That is, we can consider
pairs (q, r ) of quaternions, and multiply these pairs
together using the formula

Since the conjugate q‚àó of a quaternion q is the ana-
logue of the complex conjugate . ar{z} of a complex number
z, this is basically the same formula that we used for
multiplication of pairs of complex numbers‚Äîthat is,
for quaternions.
  However, we need to be careful: multiplication of
quaternions is not commutative, so there are in fact
many formulas we could write down that would be
‚Äúbasically the same‚Äù as the earlier one. Why choose the
above one, rather than, say, replacing q ‚àó t by tq‚àó ?
  It turns out that the formula suggested above leads
to zero divisors. For example, (j, i)(l, k) works out to be
(0, 0). However, the modified formula

which one can discover fairly quickly if one bears in
mind that one would like (q, r )(q‚àó , ‚àír ) to work out
as (|q|2 + |r |2 , 0), does produce a useful number sys-
tem. It is denoted O and its elements are called the
octonions (or some times the Cayley numbers). Unfor-
tunately, multiplication of octonions is not even asso-
cia tive, but it does have two very good properties:
every nonzero octonion has a multiplicative inverse,
and two nonzero octonions never multiply together to
give zero. (Because octonion multiplication is not asso-
cia tive, these two properties are no longer obviously
equivalent. However, any subalgebra of the octonions
generated by two elements is associative, and this is
enough to prove the equivalence.)
   So now we have number systems when n = 1, 2, 4, or
8. It turns out that these are the only dimensions with
good notions of multiplication. Of course, ‚Äúgood‚Äù has
a technical meaning here: matrix multiplication, which
is associative but gives zero divisors, is for many pur-
poses ‚Äúbetter‚Äù than octonion multiplication, which has
no zero divisors but is not associative. So let us finish
by seeing more precisely what it is that is special about     not change the norm, which gives A so many symme-
dimensions 1, 2, 4, and 8.

III. Mathematical Concepts
All the number systems constructed above have a
notion of size given by a norm [III.62](/part-03/normed-spaces-and-banach-spaces). For real and com-
plex numbers z, the norm of z is just its modulus. For
. qrt{a} quaternion or octonion x, it is defined to be x ‚àó x,
where x ‚àó is the conjugate of x (a definition that works
for real and complex numbers as well). If we write x
for the norm of x, then the norms constructed have
the property that xy = x y for every x and y.
This property is extremely useful: for example, it tells
us that the elements of norm 1 are closed under mul-
tiplication, a fact that we used many times when dis-
cuss ing the geometric importance of complex numbers
(q, r )(s, t) = (qs ‚àí r ‚àó t, q‚àó t + r s).
and quaternions.
The feature that distinguishes dimensions 1, 2, 4, and
8 from all other dimensions is that these are the only
dimensions for which one can define a norm  ¬∑  and a
notion of multiplication with the following properties.
(i) There is a multiplicative identity: that is, a num-
ber 1 such that 1 x = x1 = x for every x.
(ii) Multiplication is bilinear, meaning that x(y + z) =
xy +xz for every x, y, and z, and x(ay) = a(xy)
whenever a is a real number, and similarly for
multiplication on the right.
(iii) For any x and y, xy = x y (and therefore
there are no zero divisors).
A normed division algebra is a vector space Rn together
(q, r )(s, t) = (qs ‚àí tr ‚àó , q‚àó t + sr ),
with a norm and a method of multiplying vectors that
satisfy the above properties. So normed division alge-
bras exist only in dimensions 1, 2, 4, and 8. Further-
more, even in these dimensions, R, C, H, and O are the
only examples.
There are various ways to prove this fact, which is
known as Hurwitz‚Äôs theorem. Here is a very brief sketch
of one of them. The idea is to prove that if a normed
division algebra A contains one of the above examples,
then either it is that example, or it contains the next one
in the sequence. So either A is one of R, C, H, and O or
A contains the algebra produced by doing to O the pro-
cess we used to construct H from C and O from H, a pro-
cess known as the Cayley‚ÄìDickson construction. How-
ever, if one applies the Cayley‚ÄìDickson construction to
O, one obtains an algebra with zero divisors.
To see how such an argument might work, let us
imagine, for the sake of example, that A contains O
as a proper subalgebra. It turns out that the norm on
A must be a euclidean norm [III.37](/part-03/bayesian-analysis)‚Äîthat is, a norm
derived from an inner product. (Roughly speaking, this
is because multiplication by an element of norm 1 does
tries that the norm on A has to be the most symmetric