# Distributions

$184$

the expanded set ought to be $3$ d times as big. Therefore, $3^{d}$ should equal $2$ . This means that d should be log $2/$ log $3$ , which is roughly $0$ . $63$ . Once one knows this, the mystery of the Cantor set is lessened. As we shall see in a moment, a theory of fractional dimension can be developed with the useful property that a countable union of sets of dimension at most d has dimension at most d. Therefore, the fact that the Cantor set has dimension greater than $0$ implies that it cannot be countable (since single points have dimension $0$ ). On the other hand, because the dimension of the Cantor set is less than $1$ , it is much smaller than a one-dimensional set, so it is no surprise that its measure is zero. (This is a bit like saying that a surface has no volume, but now the two dimensions are $0$ . $63$ and $1$ instead of $2$ and $3$ .) The most useful theory of fractional dimension is one developed by hausdorff [VI.68](/part-06/felix-hausdor-18681942). One begins with a concept known as Hausdorff measure, which is a natural way of assessing the “d-dimensional volume” of a set, even if d is not an integer. Suppose you have a curve in $R^{3}$ and you want to work out its length by considering how easy it is to cover it with spheres. A first idea might be to say that the length was the smallest you could make the sum of the diameters of the spheres. But this does not work: you might be lucky and find that a long curve was tightly wrapped up, in which case you could cover it with a single sphere of small diameter. However, this would no longer be possible if your spheres were required to be small. Suppose, therefore, that we require all the diameters of the spheres to be at most δ. Let L (δ) be the smallest we can then get the sum of the diameters to be. The smaller δ is, the less flexibility we have, so the larger L (δ) will be. Therefore, L (δ) tends to a (possibly infinite) limit L as δ tends to $0$ , and we call L the length of the curve. Now suppose that we have a smooth surface in $R^{3}$ and want to deduce its area from information about covering it with spheres. This time, the area that you can cover with a very small sphere (so small that it meets only one portion of the surface and that portion is almost flat) will be roughly proportional to the square of the diameter of the sphere. But that is the only detail we need to change: let A (δ) be the smallest we can make the sum of the squares of the diameters of a set of spheres that cover the surface, if all those spheres have diameter at most δ. Then declare the area of the surface to be the limit of A (δ) as δ tends to $0$ . (Strictly speaking, we ought to multiply this limit by $\pi/4$ , but then we get a definition that does not generalize easily .)

III. Mathematical Concepts

We have just given a way of defining length and area, for shapes in $R^{3}$ . The only difference between the two was that for length we considered the sum of the diameters of small spheres, while for area we considered the sum of the squares of the diameters of small spheres. In general, we define the d-dimensional Hausdorff measure in a similar way, but considering the sum of the dth powers of the diameters. We can use the concept of Hausdorff measure to give a rigorous definition of fractional dimension. It is not hard to show that for any shape X there will be exactly one appropriate d, in the following sense: if c is less than d, then the c-dimensional Hausdorff measure of X is infinite, while if c is greater than d, then it is $0$ . (For instance, the c-dimensional Hausdorff measure of a smooth surface is $0$ if $c < 2$ and infinite if $c > 2$ .) This d is called the Hausdorff dimension of the set X. Hausdorff dimension is very useful for analyzing fractal sets, which are discussed further in dynamics [IV.14](/part-04/dynamics). It is important to realize that the Hausdorff dimension of a set need not equal its topological dimension. For example, the Cantor set has topological dimension zero and Hausdorff dimension log $2/$ log $3$ . $A$ larger example is a very wiggly curve known as the Koch snowflake. Because it is a curve (and a single point is enough to cut it into two) it has topological dimension $1$ . However, because it is very wiggly, it has infinite length, and its Hausdorff dimension is in fact log $4/$ log $3$ . III . $18$ Distributions

Terence Tao

A function is normally defined to be an object f : X $\to$ Y which assigns to each point x in a set X, known as the domain, a point f (x) in another set Y , known as the range (see the language and grammar of mathematics [I.2](/part-01/language-and-grammar) ) . Thus, the definition of functions is set-theoretic and the fundamental operation that one can perform on a function is evaluation: given an element x of X, one evaluates f at x to obtain the element f (x) of Y . However, there are some fields of mathematics where this may not be the best way of describing functions. In geometry, for instance, the fundamental property of a function is not necessarily how it acts on points, but rather how it pushes forward or pulls back objects that are more complicated than points (e . g . , other functions, bundles [IV.6](/part-04/algebraic-topology) and sections, schemes [IV.5](/part-04/arithmetic-geometry) and sheaves, etc .). Similarly, in analysis, a function need not

III . $18$ .

Distributions

necessarily be defined by what it does to points, but may instead be defined by what it does to objects of different kinds, such as sets or other functions; the former leads to the notion of a measure; the latter to that of a distribution. Of course, all these notions of function and function like objects are related. In analysis, it is helpful to think of the various notions of a function as forming a spectrum, with very “smooth” classes of functions at one end and very “rough” ones at the other. The smooth classes of functions are very restrictive in their membership: this means that they have good properties, and there are many operations that one can perform on them (such as, for example, differentiation), but it also means that one cannot necessarily ensure that the functions one is working with belong to this category. Conversely, the rough classes of functions are very general and inclusive: it is easy to ensure that one is working with them, but the price one pays is that the number of operations one can perform on these functions is often sharply reduced (see function spaces [III.29](/part-03/function-spaces)). Nevertheless, the various classes of functions can often be treated in a unified manner, because it is often possible to approximate rough functions arbitrarily well (in an appropriate topology [III.90](/part-03/topological-spaces)) by smooth ones. Then, given an operation that is naturally defined for smooth functions, there is a good chance that there will be exactly one natural way to extend it to an operation on rough functions: one takes a sequence of better and better smooth approximations to the rough functions, performs the operation on them, and passes to the limit. Distributions, or generalized functions, belong at the rough end of the spectrum, but before we say what they are, it will be helpful to begin by considering some smoother classes of functions, partly for comparison and partly because one obtains rough classes of functions from smooth ones by a process known as duality: a linear functional defined on a space E of functions is simply a linear map φ from E to the scalars R or C . Typically, E is a normed space, or at least comes with a topology, and the dual space is the space of continuous linear functionals. The class C ω [− 1 , 1 ] of analytic functions. These are in many ways the “nicest” functions of all, and include many familiar functions such as exp (x), sin (x), polynomials, and so on. However, we shall not discuss them further, because for many purposes they form too rigid a class to be useful. (For example, if an analytic func- $185$ tion is zero every where on an interval, then it is forced to be zero every where .) The class $C^{c}\i\text{nf ty}$ [− 1 , 1 ] of test functions. These are the smooth (that is, infinitely differentiable) functions f , defined on the interval [− 1 , 1 ] , that vanish on neighborhoods of $1$ and $- 1$ . (That is, one can find $δ > 0$ such that $f(x) = 0$ whenever $x > 1 - δ$ or $x < - 1 + δ$ .) They are more numerous than analytic functions and therefore more tractable for analysis. For instance, it is often useful to construct smooth “cutoff functions,” which are functions that vanish outside some small set but do not vanish inside it. Also, all the operations from calculus (differentiation , integration, composition, convolution, evaluation, etc .) are available for these functions. The class $C^{0}$ [− 1 , 1 ] of continuous functions. These functions are regular enough for the notion of evaluation, $x \to f(x)$ , to make sense for every $x \in$ [− 1 , 1 ] , and one can integrate such functions and perform algebraic operations such as multiplication and composition, but they are not regular enough that operations such as differentiation can be performed on them. Still, they are usually considered among the smoother examples of functions in analysis. The class $L^{2}$ [− 1 , 1 ] of square-integrable functions. These are measurable functions f : [− 1 , 1 ] \to  R$for$1 which the Lebesgue integral $- 1|f(x)|^{2}dx$ is finite. Usually one regards two such functions $f$ and $g$ as equal if the set of x such that f (x) = g (x) has measure zero. (Thus , from the set-theoretic point of view, the object in question is really an equivalence class [I.2](/part-01/language-and-grammar) of functions .) Since a singleton {x} has measure zero, we can change the value of f (x) without changing the function. Thus, the notion of evaluation does not make sense for a square-integrable function f (x) at any specific point x. However, two functions that differ on a set of measure zero have the same lebesgue integral [III.55](/part-03/measures), so integration does make sense. A key point about this class is that it is self-dual in the following sense. Any two functions in this class can be paired together by the inner product

$1$

f , g  $= - 1f(x)g(x)dx.$ Therefore, given a function $g\inL^{2}$ [− 1 , 1 ] , the map $f\to$ f , g defines a linear functional on $L^{2}$ [− 1 , 1 ] , which turns out to be continuous. Moreover, given any continuous linear functional φ on $L^{2}$ [− 1 , 1 ] , there is a unique function $g\inL^{2}$ [− 1 , 1 ] such that φ (f) $=$  f , g  for every f . This is a special case of one of the Riesz representation theorems.

$186$

The class $C^{0}$ [− 1 , 1 ] ∗ of finite Borel measures. Any finite Borel measure [III.55](/part-03/measures) \mu gives rise to a continuous linear functional on $C^{0}$ [− 1 , 1 ] defined by $f\to$  μ, $f = 1 - 1f(x)d\mu.$ Another of the Riesz representation theorems says that every continuous linear functional on $C^{0}$ [− 1 , 1 ] arises in this way, so one could in principle define a finite Borel measure to be a continuous linear functional on $C^{0}$ [− 1 , 1 ] . The class $C^{c}\i\text{nf ty}$ [− 1 , 1 ] ∗ of distributions. Just as measures can be viewed as continuous linear functionals on $C^{0}$ [− 1 , 1 ] , a distribution \mu is a continuous linear functional on $C^{c}\i\text{nf ty}$ [− 1 , 1 ] (with an appropriate topology). Thus, a distribution can be viewed as a “virtual function”: it cannot itself be directly evaluated, or even integrated over an open set, but it can still be paired with any test function $g\inC^{c}\i\text{nf ty}$ [− 1 , 1 ] , producing a number  μ, g . A famous example is the Dirac distribution δ 0 , defined as the functional which, when paired with any test function g, returns the evaluation g ( $0$ ) of g at zero:  δ 0 , g  $= g(0)$ . Similarly, we have the derivative of the Dirac distribution, $- δ^{0}$ , which, when paired with any test function g, returns the derivative g ( $0$ ) of g at zero: $-δ^{0}$ , g  $= g (0)$ . (The reason for the minus sign will be given later .) Since test functions have so many operations available to them, there are many ways to define continuous linear functionals, so the class of distributions is quite large. Despite this, and despite the indirect, virtual nature of distributions, one can still define many operations on them; we shall discuss this later. The class C ω [− 1 , 1 ] ∗ of hyperfunctions. There are classes of functions more general still than distributions. For instance, there are hyperfunctions, which roughly speaking one can think of as linear functionals that can be tested only against analytic functions $g \in Cω$ [− 1 , 1 ] rather than against test functions $g \inC\i\text{nf ty}$ [− 1 , 1 ] . However, as the class of analytic functions is so sparse, hyperfunctions tend not to be as useful in analysis as distributions. At first glance, the concept of a distribution has limited utility, since all a distribution \mu is empowered to do is to be tested against test functions g to produce inner products μ, g . However, using this inner product, one can often take operations that are initially defined only on test functions, and extend them to distributions by duality. A typical example is differentiation. Suppose one wants to know how to define the derivative \mu of a distribution, or in other words how to define μ , g III. Mathematical Concepts for any test function g and distribution μ. If \mu is itself a test function μ = f , then we can evaluate this using integration by parts (recalling that test functions vanish at $- 1$ and 1). We have $1$ f , $g = f$ (x) g (x) dx $- 11$ f (x) g (x) dx $= -$ f , g . $= -- 1$ Note that if g is a test function, then so is g . We can therefore generalize this formula to arbitrary distributions by defining μ , g $= -$ μ , g . This is the justification for the differentiation of the Dirac distribution: δ 0 , g $= -δ^{0}$ , $g= - g (0)$ . More formally, what we have done here is to compute the adjoint of the differentiation operation (as defined on the dense space of test functions). Then we have taken adjoints again to define the differentiation operation for general distributions. This procedure is well-defined and also works for many other concepts; for instance, one can add two distributions, multiply a distribution by a smooth function, convolve two distributions, and compose distributions on both the left and the right with suitably smooth functions. One can even take Fourier transforms of distributions. For instance, the Fourier transform of the Dirac delta δ 0 is the constant function $1$ , and vice versa (this is essentially the Fourier inversion formula), while the distribution $n\inZδ^{0}(x - n)$ is its own Fourier transform (this is essentially the Poisson summation formula). Thus, the space of distributions is quite a good space to work in, in that it contains a large class of functions (e . g . , all measures and integrable functions), and is also closed under a large number of common operations in analysis. Because the test functions are dense in the space of distributions, the operations as defined on distributions are usually compatible with those on test functions. For instance, if $f$ and $g$ are test functions and $f = g$ in the sense of distributions, then $f = g$ will also be true in the classical sense. This often allows one to manipulate distributions as if they were test functions without fear of confusion or inaccuracy. The main operations one has to be careful about are evaluation and pointwise multiplication of distributions, both of which are usually not well-defined (e . g . , the square of the Dirac delta distribution is not well-defined as a distribution). Another way to view distributions is as the weak limit of test functions. A sequence of functions f n is said to converge weakly to a distribution \mu if f n , g $\to$  μ, g
