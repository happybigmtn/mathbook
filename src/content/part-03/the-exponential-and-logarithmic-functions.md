# The Exponential and Logarithmic Functions

III . $25$ .

The Exponential and Logarithmic Functions the adjacency matrix A. Therefore, its largest eigenvalue is $1$ , and if $\lambda$ (G) is small then all other eigenvalues are small. Suppose that this is the case, and let p be any probability distribution [III.71](/part-03/probability-distributions) on the vertices of G. Then we can write p as a linear combination i u i , where $u^{i}$ is an eigenvector of T with eigenvalue $d - 1\lambda^{i}$ . If T is applied k times, then the new distribution will be

$- 1 - 1$

k

k

i (d $\lambda$ i) u i . If $\lambda$ (G) is small, then (d $\lambda$ i) tends rapidly to zero, except that it equals $1$ when $i = 1$ . In other words, after a short time, the “nonconstant part” of p goes to zero and we are left with the uniform distribution. Thus, random walks on expanders mix rapidly. This property is at the heart of some of the applications of expanders. For example, suppose that V is a large set, f is a function from V to the interval [0 , 1] , and we wish to estimate quickly and accurately the average of f . A natural idea is to choose a random sample $v^{1}$ , $v^{2}$ , . . . , v k of points in V and calculate the average $k - 1 ki = 1f(v^{i})$ . If k is large and the $v^{i}$ are chosen independently, then it is not too hard to prove that this sample average will almost certainly be close to the true average: the probability that they differ by more than

$2$

is at most e  -    k .

This idea is very simple, but actually implementing it requires a source of randomness. In theoretical computer science, randomness is regarded as a resource, and it is desirable to use less of it if one can. The above procedure needed about log (|V|) bits of randomness for each v i , so k log ( |$V$| ) bits in all. Can we do better? Ajtai, Komlós, and Szemerédi showed that the answer is yes: big time! What one does is associate V with the vertices of an explicit expander. Then, instead of choosing $v^{1}$ , $v^{2}$ , . . . , v k independently, one chooses them to be the vertices of a random walk in this expanding graph, starting at a random point $v^{1}$ of V . The randomness needed for this is far smaller: log ( |$V$| ) bits for $v^{1}$ and log (d) bits for each further v i , making log $(|V|) + k$ log  (  d  )  bits in all. Since V is very large and d is a fixed constant, this is a big saving: we essentially pay only for the first sample point. But is this sample any good? Clearly there is a heavy dependence between the v i . However, it can be shown that nothing is lost in accuracy: again, the probability that the estimate differs from the true mean by

$2$

more than   is at most e  -    k . Thus, there are no costs attached to the big saving in randomness. This is just one of a huge number of applications of expanders, which include both practical applications

$199$

and applications in pure mathematics. For instance, they were used by Gromov to give counterexamples to certain variants of the famous baum-connes conjecture [IV.15](/part-04/operator-algebras). And certain bipartite graphs called “lossless expanders” have been used to produce linear codes with efficient decodings. (See reliable transmission of information [VII.6](/part-07/reliable-transmission-of-information) for a description of what this means .) III . $25$ The Exponential and Logarithmic Functions $1$ Exponentiation The following is a very well-known mathematical sequence: $2$ , $4$ , $8$ , $16$ , $32$ , $64$ , $128$ , $256$ , $512$ , 1024 , . . . . Each term in this sequence is twice the term before, so, for instance, $128$ , the seventh term in the sequence, is equal to 2 \times 2 \times 2 \times 2 \times 2 \times 2 \times 2 . Since repeated multiplications of this kind occur through out mathematics, it is useful to have a less cumbersome notation for them, so 2 \times 2 \times 2 \times 2 \times 2 \times 2 \times 2 is normally written as $2^{7}$ , which we read as “ $2$ to the power $7$ ” or just “ $2$ to the $7$ . ” More generally, if a is any real number and m is any positive integer, then $a^{m}$ stands for $a \t\text{imes a} \times$ · · · $\t\text{imes a}$, where there are m as in the product. This product is called “a to the m,” and numbers of the form a m are called the powers of a. The process of raising a number to a power is known as exponentiation. (The number m is called the exponent .) A fundamental fact about exponentiation is the following identity: $a^{m} + n = a^{m}$ · $a^{n}$ This says that exponentiation “turns addition into multiplication.” It is easy to see why this identity must be true if one looks at a small example and temporarily reverts to the old, cumbersome notation. For instance, 27 = 2 \times 2 \times 2 \times 2 \times 2 \times 2 \times 2= (2 \times 2 \times 2) \times (2 \times 2 \times 2 \times 2)= 23 \times 24 . Suppose now that we are asked to evaluate $2^{3}/^{2}$ . At first sight, the question seems misconceived: an essential part of the definition of $2$ m that has just been given was that m was a positive integer. The idea of multiplying one-and-a-half $2s$ together does not make sense. However, mathematicians like to generalize, and even if we cannot immediately make sense of $2$ m except when

$200$

m is a positive integer, there is nothing to stop us inventing a meaning for it for a wider class of numbers. The more natural we make our generalization, the more interesting and useful it is likely to be. And the way we make it natural is to ensure that at all costs we keep the property of “turning addition into multiplication.” This, it turns out, leaves us with only one sensible choice for what $2^{3}/^{2}$ should be. If the fundamental property is to be preserved, then we must have $2^{3}/^{2}$ · $2^{3}/^{2} = 2^{3}/^{2} + {}^{3}/^{2} = 2^{3} = 8$ . √ Therefore, $2^{3}/^{2}$ has to be ± $8$ . It turns out to be convenient to take $2^{3}/^{2}$ to be positive, so we define $2^{3}/^{2}$ to √ be $8$ . A similar argument shows that $2^{0}$ should be defined to be $1$ : if we wish to keep the fundamental property, then $2 = 2^{1} = 2^{1} + {}^{0} = 2^{1}$ · $2^{0} = 2$ · $2^{0}$ . Dividing both sides by $2$ gives the answer $2^{0} = 1$ . What we are doing with these kinds of arguments is solving a functional equation, that is, an equation where the unknown is a function. So that we can see this more clearly, let us write f (t) for $2$ t . The information we are given is the fundamental property f (t  +  u)  =  f (t) f (u) together with one value, f ( $1 ) = 2$ , to get us started. From this we wish to deduce as much as we can about f . It is a nice exercise to show that the two conditions we have placed on f determine the value of f at every rational number, at least if f is assumed to be positive. For instance, to show that f ( $0$ ) should be $1$ , we note that $f(0)f(1) = f(1)$ , and we have already shown √ that $f(3/2)$ must be $8$ . The rest of the proof is in a similar spirit to these arguments, and the conclusion is that f (p  /  q) must be the qth root of $2$ p . More generally, the only sensible definition of a p  /  q is the qth root of a p . We have now extracted everything we can from the functional equation, but we have made sense of a t only if t is a rational number. Can we give a sensible definition when t is irrational? For example, what would be √ the most natural definition of $2^{2}$ ? Since the functional √ equation alone does not determine what $2^{2}$ should be, the way to answer a question like this is to look for some natural additional property that f might have that would, together with the functional equation, specify f uniquely. It turns out that there are two obvious choices, both of which work. The first is that f should be an increasing function: that is, if s is less than t, then

III. Mathematical Concepts

f (s) is less than f (t) . Alternatively, one can assume that f is continuous [I.3](/part-01/fundamental-definitions). Let us see how the √ first property can in principle be used to work out $2^{2}$ . The idea is not to calculate it directly but to obtain better and better estimates. For √ instance, √ since $1$ . $4 < 2 < 1$ . $5$ the order property tells us that $2^{2}$ should lie between $2^{7}/^{5}$ √ and $2^{3}/^{2}$ , and in gen√ eral that if $p/q < 2 < r/s$ then $2^{2}$ should lie between $2^{p}/^{q}$ and $2^{r}/^{s}$ . It can be shown that if two rational numbers $p/q$ and $r/s$ are very close to each other, then $2^{p}/^{q}$ and $2$ r / s are also close. It follows that as we choose fractions p / $q$ and $r$ / s that are closer and closer together, so the resulting numbers $2^{p}/^{q}$ and $2^{r}/^{s}$ converge to some √ $2$ limit, and this limit we call $2$ . $2$ The Exponential Function One of the hallmarks of a truly important concept in mathematics is that it can be defined in many different but equivalent ways. The exponential function exp (x) very definitely has this property. Perhaps the most basic way to think of it, though for most purposes not the best, is that exp (x) = e x , where e is a number whose decimal expansion begins $2$ . 7182818 . Why do we focus on this number? One property that singles it out is that if we differentiate the function exp (x) = e x , then we obtain e x again--and e is the only number for which that is true. Indeed, this leads to a second way of defining the exponential function: it is the only solution of the differential equation f (x) = f (x) that satisfies the initial condition $f(0) = 1$ . A third way to define exp (x), and one that is often chosen in textbooks, is as the limit of a power series: $x3x2 + +$ ··· , exp $(x) = 1 + x + 2$ ! $3$ ! known as the Taylor series of exp (x). It is not immediately obvious that the right-hand side of this definition gives us some number raised to the power x, which is why we are using the notation exp (x) rather than e x . However, with a bit of work one can verify that it yields the basic properties exp (x + y) = exp (x) exp (y), exp $(0) = 1$ , and $(d/dx)$ exp $(x) =$ exp ( x ). There is yet another way to define the exponential function, and this one comes much closer to telling us what it really means. Suppose you wish to invest some money for ten years and are given the following choice: either you can add $100$ % to your investment (that is, double it) at the end of the ten years, or each year you can take whatever you have and increase it by $10$ % . Which would you prefer?

III . $25$ .

The Exponential and Logarithmic Functions The second is the better investment because in the second case the interest is compounded: for instance, if you start with 100 , then after a year you will have 110 and after two years you will have 121 . The increase of 11 in the second year breaks down as $10$ % interest on the original 100 plus $a$ further dollar, which is $10$ % interest on the interest earned in the first year. Under the second scheme, the amount of money you end up with is 100 times (1 . $1)^{10}$ , since each year it multiplies by $1$ . $1$ . The approximate value of (1 . $1)^{10}$ is $2$ . 5937 , so you will get almost 260$\text{instead of} 200$ . What if you compounded your interest monthly?

$1$

Instead of multiplying your investment by $1^{10}$ ten

$1$

times, you would multiply it by 1120 120 times. By the end of ten years your 100 would have been multiplied 1 120$by$(1 +  120), which is approximately $2$ . $707$ . If you compounded it daily, you could increase this to approximately $2$ . $718$ , which is suspiciously close to e. In fact, e can be defined as the limit, as n tends to infinity, of $1$ the number $(1 + n)n$ . It is not instantly obvious that this expression really does tend to a limit. For any fixed power m, the limit $1$ of $(1 + n)m$ as $n$ tends to infinity is $1$ , while for any fixed n, the limit as m tends to infinity is $\i\text{nf ty}$. When it $1 n$ comes to $(1 + n)$ , the increase in the power just com$1$ pensates for the decrease in the number $1 + n$ and we get a limit between $2$ and $3$ . If x is any real number, then x n $(1 + n)$ also converges to a limit, and this we define to be exp (x) . Here is a sketch of an argument that shows that if we define exp (x) in this way, then we obtain the main property that we need if our definition is to be a good one, namely exp (x) exp (y) = exp (x + y). Let us take x n n y $1 + 1 +$ , n n which equals n y xy x + $+ 21 +$ . n n n Now the ratio of $1 + x/n + y/n + xy/$ n2 to $1 + x/n + y/n$ is smaller than $1 + xy/$ n2, and $(1 + xy/$ n2)n can be shown to converge to $1$ (as here the increase in n is not enough to compensate for the rapid decrease in xy / $n_{2}$ ). Therefore, for large n the number we have is very close to $x +$yn $1 +$ . n Letting n tend to infinity, we deduce the result. 2013 Extending the Definition to Complex Numbers If we think of exp (x) as e x , then the idea of generalizing the definition to complex numbers seems hopeless: our intuition tells us nothing, the functional equation does not help, and we cannot use continuity or order relations to determine it for us. However, both the power series and the compound-interest definitions can be generalized easily. If z is a complex number, then the most usual definition of exp (z) is $z3z2 + +$ ··· . $1 + z + 2$ ! $3$ ! Setting $z = i\theta,$ for $a$ real number $\theta,$ and splitting the resulting expression into its real and imaginary parts, we obtain $\theta4\theta3\theta5\theta2 + +$ ··· $+ i \theta - + -$ ··· , $1 - 2$ ! $4$ ! $3$ ! $5$ ! which, using the power-series expansions for cos ( $\theta$ ) and sin $(\theta)$ , tells us that exp $(i\theta) =$ cos $(\theta) + i$ sin $(\theta)$ , the formula for the point with argument $\theta$ on the unit circle in the complex plane. In particular, if we take $\theta = \pi$ , we obtain the famous formula $ei\pi = - 1($ since cos $(\pi) = - 1$ and sin $(\pi) = 0)$ . This formula is so striking that one feels that it ought to hold for a good reason, rather than being a mere fact that one notices after carrying out some formal algebraic manipulations. And indeed there is a good reason. To see it, let us return to the compound-interest idea and define exp (z) to be the limit of ( $1 +$ z / n) n as n tends to infinity. Let us concentrate just on the case where $z = i\pi$ : why should $(1 + i\pi/n)n$ be close to $- 1$ when n is very large? To answer this, let us think geometrically. What is the effect on a complex number of multiplying it by $1 + i\pi/n$ ? On the Argand diagram this number is very close to $1$ and vertically above it. Because the vertical line through $1$ is tangent to the circle, this means that the number is very close indeed to a number that lies on the circle and has argument $\pi/n ($ since the argument of a number on the circle is the length of the circular arc from $1$ to that number, and in this case the circular arc is almost straight). Therefore, multiplication by $1 + i\pi/n$ is very well approximated by rotation through an angle of $\pi/$ n . Doing this n times results in a rotation by $\pi$ , which is the same as multiplication by $- 1$ . The same argument can be used to justify the formula exp (i $\theta ) =$ cos $(\theta) + i$ sin $(\theta)$ . Continuing in this vein, let us see why the derivative of the exponential function is the exponential function.
